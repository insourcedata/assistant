{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base OAI Libraries & Environment Setup\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import html2text\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading up Langchain v0.1.0 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#New Libraries to be imported\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text, model=None):\n",
    "    if model == 'gpt-4':\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_youtube(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Convert to text\n",
    "        transcript = ' '.join([t['text'] for t in transcript])\n",
    "\n",
    "        # Create document\n",
    "        document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "\n",
    "        return document\n",
    "    except:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_search():\n",
    "    \n",
    "    yt_ids = []\n",
    "\n",
    "    while True:\n",
    "        yt_input = input(\"Enter a video_id (format 'KQjZ68mToWo') or 'Done': \")\n",
    "        if yt_input.lower() == 'done':\n",
    "            break\n",
    "        if len(yt_input) == 11 and yt_input.isalnum():\n",
    "            yt_ids.append(yt_input)\n",
    "        else:\n",
    "            print(\"Invalid format. Please enter a valid video_id.\")\n",
    "\n",
    "    return yt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid format. Please enter a valid video_id.\n",
      "Invalid format. Please enter a valid video_id.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LuhJEEJQgUM', 'KQjZ68mToWo', 'hvAPnpSfSGo', 'ro312jDqAh0']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_loader(yt_ids):\n",
    "    yt_docs = []\n",
    "    \n",
    "    for id in yt_ids:\n",
    "        loader = YoutubeLoader.from_youtube_url(f\"https://www.youtube.com/watch?v={id}\",\n",
    "        add_video_info=True,\n",
    "        language=[\"en\", \"id\"],\n",
    "        translation=\"en\",\n",
    "        )\n",
    "    \n",
    "    return yt_docs.extend(RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50).split_documents(loader.load()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Setup - gpt-3.5-turbo-1106 is a chat model\n",
    "client = OpenAI()\n",
    "model=\"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mmBo8nlu2j0', 'hvAPnpSfSGo', 'KQjZ68mToWo']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_ids = []\n",
    "\n",
    "while True:\n",
    "    yt_input = input(\"Enter a video_id (format 'KQjZ68mToWo') or 'Done': \")\n",
    "    if yt_input.lower() == 'done':\n",
    "        break\n",
    "    if len(yt_input) == 11 and yt_input.isalnum():\n",
    "        yt_ids.append(yt_input)\n",
    "    else:\n",
    "        print(\"Invalid format. Please enter a valid video_id.\")\n",
    "\n",
    "yt_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_docs = []\n",
    "\n",
    "for id in yt_ids:\n",
    "    loader = YoutubeLoader.from_youtube_url(f\"https://www.youtube.com/watch?v={id}\",\n",
    "    add_video_info=True,\n",
    "    language=[\"en\", \"id\"],\n",
    "    translation=\"en\",\n",
    "    )\n",
    "    \n",
    "    yt_docs.extend(RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50).split_documents(loader.load()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"all right let's get started this is the  opening eye prompt engineering page  before we can start building with it  though we need to set things some things  up so I've created a f a fresh uh  virtual environment what I'm going to do  is I'm going to set up a l  serve uh uh template a l or application  with this the reason that I'm going to  be using lay serve is it'll make it easy  to deploy this once I finish creating  so the first thing that I'm going to do  is bootstrap a link serve project I'm  going to do that with the Lang chain CLI  I'm installing it here after it finishes  installing I'm going to create a new app  I will call  it open AI  prompter I am not going to add a package  because I'm going to be creating my  own I can then go inside it and if I  open it up the main thing that I'm  interested in is this Lang serve server  right here which wraps around fast API  and will make it really easy to deploy  my Lang chain  chain the last thing I'm going to do for  setup is set up laying Smith so laying  Smith is in uh private beta if you don't  have access to it and you're seeing this  and you want to follow along shoot me a  DM on LinkedIn or Twitter and I will uh  get you access to it um what it will  make it really easy to do is debug as we  go along and this will involve a lot of  prompter engineering and so we'll see  how that becomes very very helpful um  someone in a previous video commented  that I should go over how to set this up  more so you can go to your projects  let's even create a new project um let's  call this uh open AI  prompter um that's all we can do let's  then go to setup and then we can just  export some API Keys  here um so you can just copy this  you can paste it here you then need to  add in your API key you can do that let  me let me move my face over here and  let's move this over here as  well you can do that by going to API  keys and all right good it's not showing  mine so you can create an API key and  then put it there um I've already did  this ahead of time um so mine's all set  up but if you needed to do that this is  how you do that all right so let's go  back to this prompt engineering guide  the first thing we're going to do we're  just going to copy it all because we are  going to be using this in our prompt to  help us write better  prompts let's paste it into a  file and then I'm going to I'm going to  create my chain in a Jupiter notebook  and then I'm going to put it into uh and  then I'm going to put it into this app  and so the reason that I'm doing this  there's uh there's there's one real  reason and one fake reason the one real  reason is that a lot this will it' be a  pretty iterative process I'm going to do  some prompt engineering some of that  I'll do in Lang Smith but some of that  I'll I'll do in uh The Notebook as well  and we'll see why um and so having a  notebook like environment is really  really helpful because it's an iterative  environment and I can really easy  iterate the other reason I want to do  this is I want to show how it's really  easy to create a chain and then just  export it from a notebook most of the  time it's it's pretty simple so let's  save  this and now I've got this jup notebook  let's load um what I just put in  there all right so let's do  that let's import uh we can print out  the head I guess just  to see it let's import some stuff from L  chain that we're going to want so from L  chain  core. prompts  import template from L core do output  parsers string output parser from L  chain  Community do chat models import chat  open AI so I'm importing three important  things this is going pretty simple chain  that I'm writing we'll see maybe we'll  get more complex but it should just be  pretty simple because I'm just going to  use a model that has a long context  window and can work with all these  instructions so I'm importing a prompt  template this will help me uh structure  the inputs to the model I'm importing an  output parser that'll basically just  convert it from a message format which  is how the newer models respond the chat  messes respond with uh message but I I  really want to set it's string so I'm  getting that and then I'm importing chat  opening app which is the L chain wrapper  around the open AI models which are the  ones that I'll be  using now is the fun part where I'm  going to write my template for creating  or my template that's going to help me  create a chain that can take in an  objective and write a good  prompt so let's add this  there  um and okay so so this is actually an  interesting point that I'll get to later  but basically what I'm doing right here  is I'm going to have these instructions  as a variable that I'll pass into the  prompt and I'll I'll go into that more  later but for now let's just assume  that's what we're going to do and then  let's add just some delimiters here and  then let's say  based on the above  instructions help me write a  good  prompt I want a  prompt  that okay so what I actually want to  write is not actually a prompt but I  want to write a  prompt  template because I think most people  write prompt templates because they want  to write prompt templates that then they  can use in their code or at least most  linkchain users want to write prompt  templates help me write a good prompt  template  this template should be a python s  string  it can take in any number of  variables depending on my  objective return your answer in the  following  format this is my  objective this seems like a decent first  pass I don't know we'll see um and then  we're going to create the chain okay so  as I mentioned we have this thing here  um which I'm going to pass in the  instructions to let's see what would  happen if we didn't do that and if we  just did  like this which you could also easily do  so if I did this I would have prompt  equals prompt template from template  template  chain um equals prompt  Shadow  AI string up the  parser chain. invoke and now I'm going  to pass in objective what should my  objective  be  um  answer B answer a question based on a  based on context  provided and only on that context  so this is a pretty typical rag prompt  or retrieval augmented generation prompt  you want the the language model to  respond to a user question based on the  context  provided um so let's do this and I get  ke because basically what's happening is  in these instructions there's other um  there's other curly brackets which  aren't variables that I want to format  they're just part of the text and so  this often happens when you're working  with code or something like that and  we've got a a lot of questions about how  to deal with it so my preferred way to  deal with it is to do what I'd done  earlier treat the the text that has  these unwanted curly brackets as input  variables and then okay so so one way to  do this is you may be thinking okay so  that means that I now have to provide  text as text here and this is a little  bit annoying because now you have two  input variables it's not the end of the  world but it's a little bit annoying and  if if this chain is part of a larger  chain then then it starts to get more  and more complicated more and more  annoying um  and ah I  had that should be  taex this is  erroring because the model's not long  enough I could do some fancy rag um it's  just like barely over the context window  and I'm I'm going to want to use this  model anyways because this task is kind  of hard so I want a good model to do it  um so this will just get me around that  and then let me also set temperature  equals to zero while I'm at it so I  could do it this way I could pass in the  text here um and this would work let's  see what answer it gives um for the next  time I'll use streaming um so we can  start seeing what's going on earlier um  all right it's writing long thing while  while it's  working okay so let's um this is a  little  unreadable um because it's not super  readable what I'm going to do  is all right so this is Lang Smith this  is me debugging I want to see the output  um this is a all right so this is better  output um I get uh oh interesting  okay so all right so it's giving me some  instructions  on how to use  it  um I actually don't know if this is  correct because of these curly brackets  but anyways  um what we can do is uh uh sorry back to  this what we can do there's a lot I want  to show off but first things we can do  is partial The Prompt so we can now  do this and that means we no longer have  to pass this in so we're basically just  partialing the prompt we're passing in  some input variables ahead of time  before we even construct the train and  then because we know those are always  going to be fixed and they're and they  shouldn't be variables so we're going to  pass them in and then we can do this I'm  going to now change this to streaming  for Chunk in or more accurately for  token in chain.  stream print  token um all right I need to  add something like  this all right  so there we  go it's not  exactly um what I really want I really  want like just one I really just want  this  thing um templ should be a  python see if this  helps  so it's not helping a ton I can go back  in here and I can I can start doing some  prompt engineering here if I want so one  thing that I can do it's not really that  helpful here because this is a really  simple um chain but if I wanted to I  could open up the playground and it's  also not helpful here because my prompt  is really really long and this isn't the  grass oh I can okay that's slightly  better so now I have this um and um  let's let's just do  like bunch of new that thing based on  the above instructions help me write a  good prompt template this prompt  template should  be a string that can be formatted as if  a python fstring I can take in any  number of variables depending on my  objective  um I can now  run I can now run  this  okay okay that's pretty  good\", metadata={'source': 'mmBo8nlu2j0', 'title': 'Auto-Prompt Builder (with Hosted LangServe)', 'description': 'Unknown', 'view_count': 5430, 'thumbnail_url': 'https://i.ytimg.com/vi/mmBo8nlu2j0/hq720.jpg', 'publish_date': '2024-01-05 00:00:00', 'length': 1386, 'author': 'LangChain'}),\n",
       " Document(page_content=\"can now run  this  okay okay that's pretty  good  all right so that's pretty good I'm I'm  I'm relatively happy with  this so um let's  uh take  this from the playground let's bring it  back in the  notebook  24 being a bit slow okay so it's  generally doing pretty  good I'm relatively happy with this I  might do a little bit more prompt  engineering by the time this gets out  but I'm relatively happy with this for  now so what I'm going to do is I'm going  to move it into Lang serve now I'm going  to deploy it with Lang  serve so I am going  to create a new file in here I am going  to basically copy this chain  over that's done  I'm going to go into the  server um so I'm going to edit this add  the chain you want to add um from app.  chain  import  chain  chain  path  prompter cool um I'm going to go back  here and  let's try it out and see what happens L  chain  serve doesn't find the file that is  right so L chain  notebooks open AI prom what did I name  it prompting oh right let specify  that let's try serving it  again and I'm going to poetry add open  AI because I'm going to deploy this  project later um and this will add it to  the Pi Project file so it's going to  save  that um let's try it  again  cool so now if I go  to Local Host  8,000 prompter playground  I get  this let's try it out I take  this add it  here all right so I get back some  streaming um I get back some nice  stuff okay so there's definitely some  prompt engineering that I want to do um  it's doing this um Let me let me pause  the video a bit do some prompt  engineering and then resume it in a  second all right so I'm back with a  little bit of a better prompt um and I  will uh uh I I'll include links to all  the code so I won't read out here  basically now if I go here and I type in  an  objective this gets me um this gets me a  pretty solid prompt that I'd use for for  retrievable augmented  generation so so that's basically how we  get to something that's deployed with  Ling serf um this is still all local and  so for the final part of this I'll walk  through how to do this um on laying  Smith so that you can share it with  other  people as you  see so I have my my personal tenant my  personal tenant doesn't have access to  uh laying serve deployments yet on Lang  Smith so this is uh this is an alpha  feature that we're still testing um it's  it's it's only available to a few people  um if you are interested in being an  alpha tester please let us know um or  please let me know and uh we are not  letting a ton of people off because we  want we do want to make some  improvements um but as we let more and  more people off um you know we'll we'll  will uh let me know and and you can be  one of  them in order to show you briefly what  it looks  like I am uh going to switch to the Lang  chain account I'm going to go to  deployments um and you'll see once it  loads that I have a bunch of deployments  here these deployments are all connected  to GitHub um so one thing that I'm going  to do after this is I'm going to pause  set up a GitHub um uh repo  with this code so that I can easily  deploy  it um so let me pause and do that and  then I'll come back and walk through a  new deployment and see what  happens all right so we're back so I'm  going to create a new deployment I'm  going to click here I'm going to  choose this repo that I created open AI  Auto prompter name it uh open AI  prompting  helper um I'm going to add some  environment variables um namely I'm  going to add open AI  API key um I'm going to make it a secret  G to pause while I put this  in all right it's in um I automatically  get a tracing project for this so that's  a big benefit of deploying on Lang Smith  is it automatically connects to  everything else in Lang Smith um right  now that's tracing we'll make that  testing um and and prompts and other  things in the short  term um I can now submit  this and it will spin up a  deployment um and so here is this uh uh  deployment as you can see it's uh it'll  take a little bit so I'm going to pause  and come back when it's finished  deploying all right we're back our  deployment succeeded we can now open the  playground for this  deployment so we can go  here task um what what was the task we  had way back in the  day so let's just use the same task  here we can see it  streaming  awesome so we get this result um one  thing we can do is we can give thumbs up  or thumbs down I like this one so I'm  going to click thumbs up if I go back  here I can see it's only one Trace count  but I can see the traces for this so  here this is the most recent Trace I can  click into it this is a tracing from  this is the same tracing that's in uh uh  langing Smith as you've been using it so  that's one of the big benefits as it  comes with all this stuff it comes with  feedback automatically hooked up so we  have a score here um and  yeah that's uh basically it for the  video now hopefully this was pretty  helpful in terms of getting  um a pretty simple chain it's you know  it's just a a prompt a model and an Alpa  parer up and running using lell uh  getting around some uh prompt uh  annoyances with all the formatting of  the curly brackets shwing how to set up  langing Smith showing how to set up a  laying serve project and then showing uh  the new laying serve deployment feature  as well hopefully you guys enjoyed this  let me know in the comments have a good  one\", metadata={'source': 'mmBo8nlu2j0', 'title': 'Auto-Prompt Builder (with Hosted LangServe)', 'description': 'Unknown', 'view_count': 5430, 'thumbnail_url': 'https://i.ytimg.com/vi/mmBo8nlu2j0/hq720.jpg', 'publish_date': '2024-01-05 00:00:00', 'length': 1386, 'author': 'LangChain'}),\n",
       " Document(page_content=\"hey everyone this is Harrison from Lang  chain here I wanted to do a video on  Lang graph which is a new library we  released um pretty recently and how you  can use l graph to create some  multi-agent workflows so for those of  you who aren't caught up on L graph we  have a whole series of videos going  through it and it's basically a way to  dynamically  create agent-like workflows as graphs so  what do I mean by that by an agent-like  workflow I I generally mean running a  language model in a loop in a variety of  ways um and there's different structure  and how you might want to run that  language model in a loop and so L graph  is useful for defining that structure  and specifically it allows for the  definition of Cycles because this is a  loop after all um another way to think  about this um is thinking about it as a  way to define kind of like State  machines um and and so a state machine  machine and a labeled directed graph are  pretty similar and that you have  different nodes and if if you think of  this as a state machine then a node  would be a state and if you think of it  um as as uh uh kind of like in this  multi-agent workflow then that node  state is an agent um and then you have  these edges between them and so in a  graph that's an edge in a state machine  that is uh you know transition  probabilities and when you think of  these multi-agent workflows then those  are basically how do these agents  communicate to each other so we'll think  about building multi-agent workflows  using L graph the nodes are the agents  the edges are how they communicate um if  you haven't already checked out L graph  again highly recommend that you check  out the this the video that we have at a  high level the syntax looks like this  it's very similar to network X you  define a graph that tracks some State  over time you then add nodes into that  graph you then Define edges between  those nodes and then you can use it um  like you would uh any other um L chain  chain so today I want to talk about  three different variants of multi-agent  workflows that we're going to use laying  graph to create so the first one is what  we're calling multi-agent  collaboration and so this is largely  defined as you have multiple agents  working on the same state of messages so  when you think about multiple agents  collaborating they can either kind of  like share State between them or they  can be kind of like independent and work  on their own and then maybe pass like  final responses to the other and so in a  multi-agent collaboration they share the  state and so specifically this this  example that we've set up it has two  different agents and these are basically  prompts plus llms so there's a prompts  plus an llm for a researcher and there's  a prompt plus an llm for a chart  generator they each have a few different  tools that they can call but basically  the way that it works is we first call  the researcher uh node or the researcher  agent we get a message  and then from there there's three  different ways that it can go one if the  researcher says to call a function then  we call that tool and then we go back to  the researcher two if the researcher  says a message that that says final  answer so we in the prompt we say like  if you're done say final answer so if  the researcher says final answer then it  returns to the user and then three if it  just sends a normal message there's no  tool calls and there's no final answer  then we let the chart generator take a  look at the state that's accumulated  um and kind of uh respond after that so  we're going to we're going to walk  through what this looks like um we're  first going to set up everything we've  need I've already done this um  importantly we're going to be using Lang  Smith here to kind of like track the the  multi-agents and see what's going on for  for a really good debugging experience  um Lang Smith right now is in private  beta if you don't have access shoot me a  DM on LinkedIn or Twitter and we can get  you access so the first thing we're  going to do do is we're going to Define  this helper function that creates an  agent so basically we're going to create  uh these two kind of like agents up here  um and these agents are parameterized by  an llm the tools they have and a system  message and so then we're taking kind of  like uh we're basically creating this  mini chain this very simple mini chain  that is a prompt which has uh the system  message in it um and then it's a it's a  call to the llm and it has access to  some tool so let's define this helper  function we're then going to define the  tools that we want the agents to be able  to call so we'll have one Search tool  we'll use tavil search for that and then  we'll have one python tool that can run  python  code um now we can start to create our  graph so first we're going to define the  state of what we want to track so we  want to track the messages and so again  like each agent will add a message to  this messages property and then we also  want to track who the most recent sender  is and the reason reason we need this  info is because after we call this tool  we're going to check who the sender was  and we're going to return to them so  this is just some some State that's  useful to track over  time um so let's define this state um  we're now going to Define some agent  nodes so here we've created a little  helper function to create an agent node  and so specifically this agent node is  going to uh take in state agent name  it's going to call the agent on the  state and it's then going to look at the  uh uh result we need to do a little bit  of kind of like converting here so the  agent is going to respond with an AI  message but when we add that to the  messages States that's accumulated over  time we want to represent that as a  human message actually um because we  basically want the next AI that sees  this to kind of like work with that and  so we're going to if it's a function  message then we're going to represent it  as a function message because it will  just say a function message otherwise  we're going to cast it to a human  message  and we're going to give it a specific  name we're going to give it the name of  the agent so that it knows basically is  this message that it sees again because  we're keeping track of This Global State  and so in the global State when it sees  a human message is it from this  researcher or is it from this charge  generator um and so we're going to do  that and we're going to create two nodes  we're going to create this research  agent node um by basically partialing  out the agent node function um and then  we're going to create this chart  generator  node  we're now going to define the tool node  this is the node that just runs the  tools it basically looks at the most  recent message it loads the tool  arguments it kind of it will call it it  will call the tool executor the tool  executor is just a simple wrapper around  tools that makes it easy to call them  and then it will convert the response  into a function message and append that  um to the messages  property and now we need the edge logic  so this is the this is the uh big router  logic here um which basically defines uh  some of the logic of where to go  so we look at the messages we look at  the last message if there's a function  call in the last message then we go to  the call tool or then we return call  tool and we'll see what that leads to  later on if there's final answer in the  last message then we return end um  otherwise we return continue and so  let's see how we use this in the graph  so first we create this graph let me run  this m first first we create this graph  with the agent State then we add the  three nodes that we have the research  node the chart node and the tool node um  we then add this conditional Edge so the  researcher after we call the researcher  um it uses the router logic if if the  router returns continue then we go to  chart generator so if the researcher  returns basically something that doesn't  have a tool and something that isn't  final answer then we return continue in  which case we go to the chart generator  if the router returns call tool then we  call the tool and if it returns end then  we end similar with the chart generator  um and then we add this conditional Edge  after call tool so after we call the  tool we basically look at who the sender  is and if the sender is the researcher  then we go to the researcher if the  sender is the charge generator then we  go to the chart generator and we set the  entry point to be the researcher  node now we can invoke it um we'll use  the stream method so that we can see  things being printed out um we'll pass  in this human message asking it to to  fetch the UK's GDP over the past 5 years  then draw a line graph of it um and then  after we code it finish it um and we'll  run this it'll take a little bit so I'll  probably pause the video here or I'll  kick off the I'll kick it off pause it  come back when it's finished and then we  can see what it looks like in Lang Smith  as  well all right so it's finished so it  was streaming out so we can take a look  at the whole stream of things in here if  I can figure out how to expand it so we  can see the researcher um you know the  first thing it does is it queries UK GDP  data for the past 5 years and then calls  a tool um and then it kind of keeps on  going um at some point there's this  chart so you know it PL plots it over  the past uh three years it looks like so  it didn't quite get all the all the data  it needs so you can see uh yeah you can  see it says that the data for years 2018  2019 is missing um and so it plots that  and then this is just the final response  so that's why it shows up so big if we  want to see this in a little bit nicer  view because this is a lot to digest we  can go to Lang Smith um and so Lang  Smith uh at the top of the\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 3937, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"Smith um and so Lang  Smith uh at the top of the notebook I  sent I set it so that it would log to  this specific project the multi-agent  collaboration I can see that I've got  one run can see some basic stats on it  like it took over a minute it's got some  tokens on it um if I click into it I can  now see exactly what was going on under  the hood so first it called the  researcher and we can click into here  and we can see the exact call that it  made to open AI um and so there was this  system prompt and then uh this was the  user's first message and then output  this  it then called toil um which is the  Search tool we can see the results there  it then called the researcher again we  can see that you know there's now more  in it because we start to build up this  message Bank we can see it calls toil  again okay so now the researcher and it  doesn't call toil okay interesting let's  see what's going on here so at the  bottom it basically says okay so now  it's uh it's responding what the values  are um and there's no tool call and  there's no uh uh thing saying that it's  finished so that means that it goes to  the next one which is the chart  generator so it calls the chart  generator under the hood that calls open  AI um we can see that down here we get a  function call and this is running python  code so it prints this out calls the  python reppel  um uh we can see that it runs this um  the chart generator then goes um and the  chart generator says uh you know here's  the line graph see the chart above the  researcher then goes and it says final  answer so now that there's final answer  it's finished and that's basically what  happened um so that's the multi-agent  collaboration bit the the kind of like  specific thing about this that's  interesting is it has This Global state  of messages that each llm sees and a  penss to so it keeps on adding to this  over time and so on one hand this is  good because it allows each agent to see  kind of like exactly the other steps  that the other one is doing and that's  why we called it collaboration it's very  collaborative the other version of this  is that you have agents that have their  own independent scratch pads um and so  we'll take a look at the next  multi-agent example that we have which  is this agent  supervisor so here we have the  supervisor which basically routes  between different independent agents so  these agents will be Lang chain agents  um so they'll basically be an llm that's  run with the agent executor in a loop  the llm decides what to do um the agent  executive then calls the tool goes back  to the llm calls another tool goes back  to the llm and then then finishes and uh  then there's basically the supervisor  the supervisor has its own list of  messages and essentially what it will do  is it will call agent One agent one will  go do its work but it will return only  the final answer only that final answer  is then added the supervisor only sees  his final answer and then based on that  it can call another agent or it can call  another agent another way to think about  this is that this is essentially um you  know the supervisor an agent and it's  got a bunch of tools and these tools  themselves are agents so it's a slightly  different framing than before it's not  as collaborative they're not working on  kind of like the same  sections so let's get this started um  again we're going to be uh I'm going to  let me restart my kernel we're going to  be using Lang Smith um so again uh hit  me up if you don't have access to that  um first thing we're going to do is  we're going to create some tools we'll  use the same two tools as before the  tavil tool and the python  tool we'll next Define some helpers  again to create these agents um as the  individual nodes so we have this create  agent function similar to last time the  difference is now that it's actually  doing a little bit more work under the  hood so it's not just a prompt plus a  language model it's actually creating an  open AI tool agent which is an agent  class that we have in Lang chain from  there it's creating an agent executor  which is also in Lang chain and we're  returning that that's the final  agent so let's run that then what we're  doing is we're also creating similar to  last time this agent node thing which  does this human message converion just  like we did last time is taking the  agent result um which is an AI message  and converting it into human message  with a name so we know kind of like  where it's coming  from now we create the agent supervisor  so this is going to be the the you know  system that's responsible for looking at  the different agents that it has and  deciding which one to to pass it on to  so  here you can see that we're defining a  bunch of stuff we're defining system  prompt your supervisor task with  managing conversation a between the  following workers and it's basically  using this function definition to select  the next Ro to send things to um and uh  it's or it's selecting finish finish is  another option we're creating this  prompt template we're creating this llm  and we're creating this supervisor chain  um which basically has a prompt um it's  got an llm with some functions that it  can call specifically we're forcing it  to call this route function where it  selects the next best role and then  we're pressing the the response in Json  um and we're using that determine where  to go to  next now that we've got that we can  create the graph so we've got this agent  State um which is messages and then a  next field um we've got this research  agent so we create the agent we then  create the node for it so we partial out  agent node we then do the same for the  code agent um and then we start creating  the graph so we create the graph with  the agent State and we add the three  nodes we now connect the edges in the  graph okay so for all the members um and  again if we look at where members is  defined members is defined up here so  it's the researcher encoder it's two  it's the two basically sub nodes or sub  agents um so for those after we call  those agents we go back to the  supervisor um then we also have this  conditional Edge so from the supervisor  um after the supervisor um is run we  look at what's in next and we look in  the conditional map and the conditional  map is basically a mapping between the  the members so uh and and finish so so  you know if if it says researcher then  we go to research node if it says coder  then we go to coder node if it says  finish then we finish we set the entry  point to the supervisor and then we run  the  graph now we can do the same thing we  can invoke the team um we can uh see  what happens um and see how it  performs all right so it runs it um you  can see that it calls the coder so here  we're asking it to print out hello or to  say hello world and print it to the  terminal so it calls the coder and then  it finishes here we can write a brief  research report on pasas and it will do  that while that's running um we can  maybe look in Lang Smith um to see the  previous ones so here we have the coding  one um we can see that we first called  the supervisor agent it's got this call  to open AI um again it's got the system  prompt and then the human um and then it  decides what to do next and it's calling  it's using this function call we can see  it has this function definition of Route  here based on that it goes to the coder  so now there's the coder agent the coder  agent does several things under the hood  it calls open AI first um it does a  function call it then calls the python  reppel um and then it calls open AI  again and so when we go back to the  supervisor if we look at what the  supervisor see says it just sees the  response from the coder so there's a  human message with the name coder and it  gets this response it doesn't see any of  these calls though or any of the calls  to the python reppel and then we get a  response and it says to finish so this  is an example of where you know there's  many calls but they're kind of hidden  within one agent  State we can also see that it finished  writing the research report on pasas so  if we go back here we can see that it's  finished running and if we take a look  it now calls the researcher agent under  the hood and we can see here if you know  this is interesting because this took a  pretty long time 45 seconds we can  quickly drill in and see that the the  longest call was this call to open AI um  inside the researcher agent where  basically it uh uh  has it has all these uh uh information  from the internet and it writes this  long research  report so that's pretty much it for the  agent supervisor now I want to talk  about the third and final type of agent  that we have an example of which is the  hierarchical agent team  so this is very similar to the previous  concept it's just changing what's going  on in the uh  uh it's just changing what's going on in  the sub things so now each agent node is  itself a different kind of like  supervisor agent setup and so what this  means is that it's going to be a lot  more kind of like a l graph um and a a a  little bit less of L  chain we can get started by setting up  the environment similar to last time we  now create the tools so there's two  different sets of tools that we need to  create one's for the research team it's  the Searcher and web scraper the others  for this document authoring team and  it's these three tools here so we can do  that we can load to villy we can create  our scrape web pages tool and then we  can start creating these uh other tools  as well um and these are for the writer  team so we create one that creates an  outline read documents write documents  edit documents um and then a python  reppel tool as well we we're going to  create a few helper utilities that make  it easier to basically construct this  pretty complicated graph so we have one\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 3937, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"this  pretty complicated graph so we have one  that creates an agent we then have an  agent node um and we then have this team  supervisor idea and so these are kind of  like taking a lot of the concepts that  we had in the last notebook and  extending them now we can Define these  agent teams they're hierarchical teams  so we have this research team This  research team has a few different  attributes on it the main things here  are this messages um we create uh we  initi in llm we create the search agent  and then the search node the research  agent and then the research node and now  we have the supervisor agent as  well we now put this all together um I'm  not going to go over all the edges  because it's very similar to some of the  notebooks that we did before but the end  result is that we have this uh uh graph  that we create in order for this graph  to be used in as a subgraph in another  component we're going to add this enter  chain function which basically just is  just a converter to make it easier to  use we can use this directly um so we  can try out and again this is just one  of the sub teams but we can try asking  it a question um and we can see that it  does a search it has a search to so  it'll get a response from there and it  will give us an  answer we can see that we get back a  response here and if we look at that in  lsmith we can see that it's doing a  pretty similar thing to before um it's  got the supervisor agent it's calling  search and then it responds with  supervisor we're now going to put  together another agent or another graph  of Agents so it's this document writing  team um so here we are defining a bunch  of stuff we're defining the state that  we're tracking it's largely these  messages um there's then uh some logic  that's run before each worker agent  begins um so that's more aware of the  current state of everything that exists  this is basically with populating  relevant context we then create a bunch  of agents and a bunch of uh uh agent  nodes um um and then we create the dock  writing supervisor um now that that's  done we can create the graph so we're  adding a bunch of nodes we're adding a  bunch of edges we've got some  conditional routing edges um we set the  entry point um and now that we have this  we can do something simple like write an  outline for a poem and write the poem to  disk we can see that this is finished  and if we look at this in Lang Smith we  can see that it also looks fairly  similar as to what we were doing before  we've got the supervisor we do a bunch  of note taking and then the supervisor  finishes it doesn't see any of the  internal things it just sees the final  response and we can now put this  together so we now have a layer on top  of this we've got this team supervisor  um and now we have this top level graph  State it's just a list of messages we  have some helper functions we start  creating the graph we add the research  team as its own node it's got a little  bit of uh it's got a little bit of  wrappers around the previous chain to  like get the previous message and then p  and then at the begin beginning pass it  that into the chain and then call join  graph to join it with the rest of the  stuff same uh uh with the paper writing  team we then add the supervisor we then  add some nodes we compile it all right  um and so now we can uh do a more  complex thing where we call asket to  write you know this brief research  report on the North American sturgeon  include a chart so let's kick this off  and I'll come back when this is  done all right so it's done took a while  if we go to Lang Smith we can see that  we have this most recent Trace took 400  seconds if we click inside it we can see  that there is a lot going on so a lot of  calls um down the hood it looks like  there's this big call to the paper  writing team at one point that took a  while there was also this big call to  the research team to start that took a  while I can click into any of these I  can see like what exactly is going on at  any point in time what exactly was  happening um so this is pretty helpful  for understanding what's going on um  yeah wow that's a lot of tokens um  anyways yeah that's pretty much it for  the multi-agent stuff hope this was  helpful I think uh you know the the  benefits of thinking about things as a  multi-agent way are really that it I  think it helps compar compartmentalize  what you expect different functions to  be happening inside the system so you  know each agent basically has its own  prompt its own tools even its own llm at  times and so you can really focus it on  just doing specific things as opposed to  having one General autonomous agent with  lots of tools and so even if you you  know even if you have all those same  tools and you just organize it a bit  more hierarchically that can often be  helpful for for doing kind of like  complex tasks as well as just a good  mental model to how to think about  things so hopefully this has been a good  introduction to that these are again  probably only a few of the ways that you  can create multi-agent workflows with L  graph really excited to see what other  ones  emerge\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 3937, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"hi today I want to show you ways to  improve AI applications with retrieval  augmented generation I'll show you  different Advanced Techniques that you  can use in combination with the length  chain to get better results when  retrieving data from a vector store we  will take a look at multiquery  retrievers contextual compression  Ensemble retrievers self carrying  retrievers which is my personal favorite  and time weighted Vector store  retrievers you first have a look at the  typical workflow in an AI application  you first have the indexing step you  load raw data into memory split the data  embed the chunks into embeddings and  store the chunks and the embeddings into  a vector store then you've got the  retrieval step you ask a question embed  the question and retrieve the most  similar documents you pass the documents  to a prompt and send everything to an  llm which will create a final answer the  techniques I show you are ways to  improve different steps of that workflow  you can code along with me and you will  find the link to the repository with the  iPod notebook in the description okay  I'm here in vs code and before we start  with the techniques I show you a little  experiment which shows why the chunk  Size Matters and how you can use  different chunk sizes to improve the  results with a parent child retriever so  first we have to install our  dependencies I created an requirement.  txt so we can just run pip install minus  r requirements.txt and install  everything in a virtual environment or  if you want in the global environment  there's length chain there is chrom  ADP and Tik Tok and everything you need  for an llm application okay let's start  with setting up the openi API key we  also have a pretty print doc function  which allows us to yeah print the  documents which we retrieve from the  retriever a little bit nicer and then we  also create an instance of the OPI  embeddings class so let's run that code  in the next step we want to load our raw  data so I provided a docs. txt and a  restaurant do. txt so we will use Ed the  loader class which is a text loader from  Leng chain to load that into memory so  the importance here is the difference  between dogs. txt and restaurant. txt so  the dogs. TT is a fictional Q&A um  format of a dog school and you can ask  different questions about the dog school  how what's the name where is it located  and what kind of trainings does that  school offer and in the restaurant. txt  we can see some questions about the food  of the restaurant and if it's a vegan or  vegetarian friendly restaurant and so on  so quite different topics and what we're  going to do is we now load the data and  do nothing with it so we won't split it  so we have one large chunk for each of  the files okay now let's embed some  questions how is the weather what is the  name of the dog school and what food do  you offer so the first question is  unrelated to both both files the second  is related to the dogs. txt and the  third one is related to the restaurant.  txt and then we also embed all of our  documents so we've got two documents now  we can also print that so the length of  our data  Vector so now we've got three embeddings  for the um questions and two for the  files so let's run that as we can see  we've got two vectors now okay now let's  use that vectors to make a little plot  so we uh create a plot of the coine  similarity which is a metric for  calculating the similarity of different  vectors and plot that with mat plot lip  so let's wait a little bit okay now  let's have a look at the range on the  left we can see the queries related to  the dogs file so we can see the qu  related docks is here on top and the  others are quite low so that's actually  fine but for the other file we can see  that the car related to restaurants it  still ranks on top but the similarity  between the first and the second so  related to dogs and the restaurant is  yeah it's small the difference is very  small so if we would even cover  different topics in the restaurants file  the results could be even worse and  that's why we want to create chunks to  cover different topics of a single file  with the embeddings when you create an  embedding over multiple topics the  result of that embedding does not make  any sense so it's quite hard to  differentiate the queries against that  embedding so let's run the code again  but now we use a text  splitter so we first have to change that  to data because we will split the raw  data and we will create a chunk let's  say of 120 with a chunk overlap of 10 so  let's run that  again and  now we will create more and different  vectors so this will not be two so we've  got 35 vectors now and now let's create  that plot  again so we can see that the difference  between the highest ranking vector and  the lowest ranking Vector is higher than  before so smaller chunk sizes and their  vectors are therefore more selective and  better suited to getting the right  documents from the vector store but on  the other hand very small chunks can  provide only two little context for the  llm and this is where the parent child  retriever comes into play you send a  query against a small selective chunk  and then send the larger document with  more context to the llm okay now we have  to use the parent document retriever  from length chain to make that work as  you can see we've got two Splitters here  a child splitter which will create the  smaller chunks of our data so we will  use a Chun size of 120 with a chunk  overlap of 20 and then our parent  splitter this is the splitter for our  larger documents here we can see we've  got a much bigger chunk size of 400 and  also a chunk overlap of 20 then we will  have to create an instance of some kind  of vector store we will use chroma here  and then we also have to provide an  inmemory store this is where our larger  chunks are stored and then we use the  parent document retriever to create our  retriever we have to provide the vector  store and we also have to provide the  document store this is where the larger  documents are stored and then we also  have to provide the splitter the child  splitter and the parent splitter so now  let's run that code and now we can add  our documents this the documents we  created before so 35 documents and we  add that to our retriever so we can just  run the add documents function to  retrieve the documents we can use the  similarity uh search method of the  vector store so this will now return the  documents uh and we can also use the  retriever so this is the preferred way  because it's the standardized interface  for retrieving documents this works for  every retriever so as you can see we  retrieve the same documents here okay  that was the parent child retriever  which uses different Chun sizes now  let's have a look at the multiquery  retriever which uses a different  approach so why might we even want to  use that multi quarri so when you ask a  question question very small differences  in the way you ask can already have a  large impact on the retriever process so  the multi-query retriever creates  variation of that question and uses  different questions to query against the  database for the multiquery retriever we  first have to create an instance of some  kind of um llm class so we use the chat  OPI class then we use the from llm  method of the multiquery retriever class  to create an instance of that retriever  so we have to pass the retriever we want  to have so this is the chroma Vector  store and we use the S retriever method  on that to get our Retriever and we also  pass the llm which we created here to  create our retriever instance so we just  have to run the get relevant documents  on that Retriever and as you can see so  as you can see we retrieve four  documents so what's actually happening  under the hood is that this retriever  will create multiple different queries  from our original question so what is  the name of the dog school it will  create variations of that and run each  of that query against the database at  the end we will create a large list of  documents and the duplicated documents  which will be retrieve will be dropped  so at the end we will end with a list of  unique  documents okay that was easy to use but  I will actually show you what happens  under the hood we can Implement  something like this on our own and the  key part of that is actually a prompt  template and here we pass in a single  input variable which is Quest question  and our prompt is that you are Ani  language model assistant your task is to  generate five different versions of the  given user question to retrieve relevant  documents from a vector store and this  is the key part so we now will create an  llm chain so we use the llm chain class  llm is not defined so we first have  to run that code  again and now let's run that again  and now we can invoke  the question here what is the name of  the dog school and the result is a list  of five different variations of that so  here we can see first can you tell me  the name of the dog training center  second what is the official name of the  dog school so this is what actually  happens under the hood so each of that  questions which will be generated by the  llm will be now run against the Vector  store and we will retrieve the documents  and merge the documents into a single  list of unique documents okay that was  the multiquery Retriever and now let's  have a look at contexture compression  first I we store that question inside a  variable because we will use it multiple  times and yeah we don't have to write it  again every time so what is actually  contextual compression for contextual  compression you need a basic Retriever  and a document compressor and then you  run a query against your base retriever  that documents from that Retriever and  then you forward them to a compressor so  the document compressor is an llm\", metadata={'source': 'KQjZ68mToWo', 'title': 'LangChain - Advanced RAG Techniques for better Retrieval Performance', 'description': 'Unknown', 'view_count': 4630, 'thumbnail_url': 'https://i.ytimg.com/vi/KQjZ68mToWo/hq720.jpg', 'publish_date': '2023-12-18 00:00:00', 'length': 1497, 'author': 'Coding Crashcourses'}),\n",
       " Document(page_content=\"compressor so  the document compressor is an llm that  takes a list of documents and shortens  them by reducing the content of the  documents or is actually allowed to omit  the documents all together so instead of  passing all of the documents to the llm  to create a final answer we make another  step by reducing the content or the  amount of documents you pass to the llm  so in code we first have to create a  vector store and then add some documents  to that Vector store so let's run that  code and then at the end we also run the  S retriever method on the vector store  to create our retriever object so again  we can just run get relevant documents  against that Retriever and here we can  see we get four documents back okay  these are the documents we normally  would pass uh in some kind of prompt  template and pass that to the llm when  we run a compressor we reduce that kind  of documents so first we have to create  the compressor and to do that we can use  the LM chain extractor class from Leng  chain so this is stored in Len chain.  retrievers do doent compressors so we  create an instance of that by providing  in the chat llm class and then we also  have to use the class contextual  compression retriever so we provide the  base compressor object here and we also  provide our base retriever which is the  chroma retriever here which we created  here on top and then we just run the get  relevant documents and inside here this  will make use of the llm chain extractor  and now we should see a reduced version  of that documents here so let's run that  so if we now take a look at the  documents we retrieved we can see the  dog school is called Canine Academy  Canine Academy and doc uh 3 and four  only contain the name so the original  question was what is the name of the doc  school so the llm which takes all of the  documents now extracts the relevant  information so we don't send everything  to the llm but only the a relevant  information from that question so this  makes it easier for the LM to create a  better final answer so this is very  powerful but it's also quite expensive  because you make an LM call for every  document and an alternative which is  cheaper is to use an embeddings filter  an embeddings filter won't send all of  the documents to the llm but will send  documents to an embedding model  embeddings model embedding models are  cheaper and faster so we can now  calculate the cosine similarity of each  document and we can set a similarity  threshold so if the threshold is below  the threshold we want to set we can now  drop the documents but the documents  will not be changed so this only filters  documents so we have to be careful by  setting that threshold I can show you  what happens if we set it quite High  maybe we will now get an empty list no  currently this works but we only get two  documents if we set it to high let's say  0.99 we should retrieve an empty  list if we set it to low on the other  hand here we can see nothing is returned  if we set it to low this won't have an  impact on the documents and we will just  get back all of our four documents so be  careful when you use an embedding filter  it's cheap but it's also not as powerful  as using the llm for yeah creating  smaller documents what is cool is that  we can actually make use of multiple  filters in some kind of Pipeline and as  you can see here we use the embeddings  Redundant filter to filter out redundant  documents and then we use another  embeddings filter to filter documents by  a threshold so we retrieve documents  pass it to the first filter then to the  second filter and then at the end we  only get the yeah best documents for our  llm so the documents compressor takes in  a Transformers argument which is a list  and this is the sequence what we going  to do first we're going to use the  splitter then we use the Redundant  filter and then we use the relevant  documents filter which filters by a  similarity threshold and we can just  pass it here as base compressor and then  we only have to pass the Retriever and  this now makes it easy to use multiple  compressor steps inside the retrieval  step of our um Vector store so this as  you can see this takes a little bit  longer because multiple steps are  required to get our documents as you can  see we just retrieved the four documents  so in this case um it doesn't really  matter and yeah be very careful when  using filters you have to know what why  you want to use them otherwise you will  make your application slower and more  expensive okay now let's have a look at  Ensemble retrievers what is an ensemble  retriever an ensemble retriever makes  use of multiple algorithms to retrieve  the best documents from a vector store  so we can use our highly dimensional  vectors with the embedding model but we  can also use uh another algorithm which  is the  bm25 algorithm and this is an algorithm  which is good for retrieving keywords  and we can use a combination so we can  use keyword search and also Vector  search with highly dimensional vectors  and get the best of Two Worlds so we can  use the bm25 Retriever and we can pass  documents here and then we can create  another Retriever and use both  retrievers here inside The Ensemble  retriever so we pass in a list of  retrievers so we pass in the bm25  retriever and also our chroma retriever  we can also set a weights argument here  and these weights have to add up to one  so if we set the first to 0.5 and the  second to 0.5 we can say that both of  the retrievers are equally important for  retrieving the documents so now let's  create an instance okay I can see I  first have to create or install another  package so pip install Rank  bm25 and now we should be able to run  that  code okay now the documents are stored  in the vector  store and we create an instance of that  retriever so we can again just use the a  r interface get relevant documents and  pass in the question here and yeah so  this makes no use of two different  algorithms to retrieve the best  documents from the vector store okay now  let's have a look at the self quing  retriever the self quing retriever is  able to use the underlying metadata of a  document so this is the document class  from length chain and as you can see  we've got the page content this is  normally what we query against in a  vector store but the metadata is most of  the time not used but we can use it with  a self query retriever so the interface  of Leng chain allows to provide these  kind of additional information in the  metadata and the self quaring retriever  uses an llm to filter by that metadata  so when you ask I want some kind of a  dog school or special training for my  dog and it should be in the premium  class you can just ask for that and the  llm will filter the relevant not the  relevant documents by their metadata so  this can be quite powerful if used  correctly so let's create a list of  documents now and as you can see in the  metadata we've got our type we've got  feature and we've got a price this is  what we want to filter so we have to  provide that information in each of our  documents otherwise that this can lead  to an error so let's now run the code  and here you can see we also create a  vector store and provide the documents  we created here okay now we've got our  Vector store but we don't have our  retriever yet so we can use the self  query retriever class from linkchain  that is stored in Len chain. retrievers  do self query and their dot base so this  is used to create an instance of that  retriever so to create a retriever we  first have to pass in the llm the vector  store and then also a Content  description so this is just a normal  string description of Doc training  service for example but the metadata  field info is more important so we have  to provide the information what we  actually store in our metadata so when  we scroll up here we can see that we've  got type feature and price  so we have to provide that information  for the self creting Retriever and we  have to create a list and inside the  list we have to create multiple  attribute info instances we have to  provide the name which is for example  type so as you can see here is the the  key of the dictionary that is type then  we have to provide a description so what  is the type about so for example the  type of Doc training service and then  also the type which which is just a  string so this is every time just a  string the price could be for example  also an integer or a float but in this  case yeah it's also a string so every  metadata attribute info is a string and  here we can see the name is again here's  feature and there's price so we've got  three um attribute info instances in  that class and we have to provide that  in the Constructor here for the self  cing retriever so now let's run the code  and I get an import error and I have to  install lar so pip install lar so now  let's run that and here for the  retriever we can use the invoke method  and just ask what premium pric trainings  do you offer and here we can see these  are the documents which we retrieve so  we can see this is the page content and  if we scroll to the right we can see  that we only extract documents which  have got this price in the premium  category so this works very nice to  retrieve the information based on its  metadata so I think that's a very very  powerful way to make the retrieval  process better okay so now let's have a  look at a last Vector store or retrieval  technique and this is the time weighted  Vector store retriever this is not  available for every Vector store I only  was able to make that work with Device  Vector store so we first have to import  some classes and the most important  class here is the time weighted Vector  store retriever so what we have to do  here is we have to create an instance of  that index flat instance or class and we  have to provide the embedding\", metadata={'source': 'KQjZ68mToWo', 'title': 'LangChain - Advanced RAG Techniques for better Retrieval Performance', 'description': 'Unknown', 'view_count': 4630, 'thumbnail_url': 'https://i.ytimg.com/vi/KQjZ68mToWo/hq720.jpg', 'publish_date': '2023-12-18 00:00:00', 'length': 1497, 'author': 'Coding Crashcourses'}),\n",
       " Document(page_content=\"or class and we  have to provide the embedding size the  embedding size for OPI embeddings is  1,536 so we have to provide that number  and then we have to use the F class  which is just the normal Vector store  provide the embeddings and provide also  the the index which we created here and  also we have to provide an inmemory doc  store and the inmemory Dock Store starts  with an empty dictionary so this is what  we have to do to make that work and then  we pass in that Vector store as Vector  store argument to the time weighted  Vector store Retriever and we also now  have to set a Decay rate so what is the  Decay rate so the time weighted Vector  store retriever makes use of time stamps  so you have to provide some time stamps  in the metadata and you can use for  example last access add in the metadata  and this will now invalidate metadata  that is not used you can also use last  created ad so last created ad is  responsible for let's say when you add  documents to the Retriever and let's say  for example you've got some news  articles and you update your vector  store every day then you can invalidate  your um news from yesterday just by  providing here a high de carate so let's  first start with a low DEC rate and I'll  show you the difference so let's start  with a DEC rate near zero and now we  will create um a metadata object here  last access at and then set it to  yesterday so now let's add these two  documents to the Retriever and now let's  run get relevant documents and then  hello world so this should retrieve that  document  here  so as you can see now we receive the  document back and the page content here  is hello world so this works but we um  set the Decay rate to a very low number  if you set it almost to one then the  Decay rate is quite high and it will  invalidate this document very fast so  this is yeah almost instant as you can  see now we add the documents again and  now we can see the time stamp here is  yesterday and if we run that we can see  that yesterday is too old to retrieve  that document so we only retrieve here  hello Fu even if the question was hello  world so normally we would expect to  retrieve that document but yeah that was  invalidated because our Decay rate is so  high so we only retrieve the hello Fu  document even it's not as similar as the  hello world  document  yeah so I think the time weighted Vector  store retriever can be quite powerful if  you work with news articles for example  and you don't want users to retrieve old  documents but be very careful and  experiment a little bit with that decay  rate you won't or don't want to set it  to high but you also don't want to set  it too low okay so that's it that were  some kind of advanced retrieval  techniques I hope you like the video and  if yes please subscribe to my channel  and like the video thank you very much  see you bye-bye\", metadata={'source': 'KQjZ68mToWo', 'title': 'LangChain - Advanced RAG Techniques for better Retrieval Performance', 'description': 'Unknown', 'view_count': 4630, 'thumbnail_url': 'https://i.ytimg.com/vi/KQjZ68mToWo/hq720.jpg', 'publish_date': '2023-12-18 00:00:00', 'length': 1497, 'author': 'Coding Crashcourses'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ongoing Problem: Cant get Youtube Chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://docs.smith.langchain.com/](https://docs.smith.langchain.com/',\n",
       " 'https://python.langchain.com/docs/modules/model_io/prompts/quick_start',\n",
       " 'https://python.langchain.com/docs/integrations/vectorstores/chroma']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_addresses = []\n",
    "\n",
    "while True: \n",
    "    web_address = input(\"Enter a web address (format 'https://www.example.com') or 'done': \") \n",
    "    if web_address.lower() == 'done': \n",
    "        break\n",
    "    if web_address.startswith('http://') or web_address.startswith('https://'): \n",
    "        web_addresses.append(web_address) \n",
    "    else: print(\"Invalid format. Please enter a valid web address.\")\n",
    "\n",
    "web_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main content  🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat  🦜️🔗  LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  Providers  AnthropicAWSGoogleHugging FaceMicrosoftOpenAI  More  Components  LLMs  Chat models  Document loaders  Document transformers  Text embedding models  Vector stores  Activeloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAstra DBAtlasAwaDBAzure Cosmos DBAzure AI SearchBagelDBBaidu Cloud ElasticSearch VectorSearchBigQuery Vector SearchChromaClarifaiClickHouseDashVectorDatabricks Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissFaiss (Async)Google Vertex AI Vector SearchSAP HANA Cloud Vector EngineHippoHologresJaguar Vector DatabaseKDB.AILanceDBLanternLLMRailsMarqoMeilisearchMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVecto.rsPGVectorPineconeQdrantRedisRocksetScaNNSemaDBSingleStoreDBscikit-learnSQLite-VSSStarRocksSupabase (Postgres)SurrealDBTairTencent Cloud VectorDBTigrisTileDBTimescale Vector (Postgres)TypesenseUSearchValdVearchVectaraVespaviking DBWeaviateXataYellowbrickZepZilliz  Retrievers  Tools  Agents and toolkits  Memory  Callbacks  Chat loaders  Adapters  Stores  ComponentsVector storesChroma  On this page  ChromaBasic Example\\u200bBasic Example (including saving to disk)\\u200bPassing a Chroma Client into Langchain\\u200bBasic Example (using the Docker Container)\\u200bUpdate and Delete\\u200bUse OpenAI Embeddings\\u200bOther Information\\u200bSimilarity search with score\\u200bRetriever options\\u200bMMR\\u200bFiltering on metadata\\u200b\"),\n",
       " Document(page_content='Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.  Install Chroma with:  pip install chromadb  Chroma runs in various modes. See below for examples of each integrated with LangChain. - in-memory - in a python script or jupyter notebook - in-memory with persistance - in a script or notebook and save/load to disk - in a docker container - as a server running your local machine or in the cloud  Like any other database, you can: - .add - .get - .update - .upsert - .delete - .peek - and .query runs the similarity search.  View full docs at docs. To access these methods directly, you can do ._collection.method()', metadata={'Header 1': 'Chroma'}),\n",
       " Document(page_content='In this basic example, we take the most recent State of the Union Address, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.  # importfrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.embeddings.sentence_transformer import ( SentenceTransformerEmbeddings,)from langchain_community.vectorstores import Chroma# load the document and split it into chunksloader = TextLoader(\"../../modules/state_of_the_union.txt\")documents = loader.load()# split it into chunkstext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)# create the open-source embedding functionembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")# load it into Chromadb = Chroma.from_documents(docs, embedding_function)# query itquery = \"What did the president say about Ketanji Brown Jackson\"docs = db.similarity_search(query)# print resultsprint(docs[0].page_content)  /Users/jeff/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm  Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'Header 1': 'Chroma', 'Header 2': 'Basic Example\\u200b'}),\n",
       " Document(page_content='Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.  Caution: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other’s work. As a best practice, only have one client per path running at any given time.  # save to diskdb2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")docs = db2.similarity_search(query)# load from diskdb3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)docs = db3.similarity_search(query)print(docs[0].page_content)  Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'Header 1': 'Chroma', 'Header 2': 'Basic Example (including saving to disk)\\u200b'}),\n",
       " Document(page_content='You can also create a Chroma Client and pass it to LangChain. This is particularly useful if you want easier access to the underlying database.  You can also specify the collection name that you want LangChain to use.  import chromadbpersistent_client = chromadb.PersistentClient()collection = persistent_client.get_or_create_collection(\"collection_name\")collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])langchain_chroma = Chroma( client=persistent_client, collection_name=\"collection_name\", embedding_function=embedding_function,)print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")  Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Insert of existing embedding ID: 1Add of existing embedding ID: 2Insert of existing embedding ID: 2Add of existing embedding ID: 3Insert of existing embedding ID: 3  There are 3 in the collection', metadata={'Header 1': 'Chroma', 'Header 2': 'Passing a Chroma Client into Langchain\\u200b'}),\n",
       " Document(page_content='You can also run the Chroma Server in a Docker container separately, create a Client to connect to it, and then pass that to LangChain.  Chroma has the ability to handle multiple Collections of documents, but the LangChain interface expects one, so we need to specify the collection name. The default collection name used by LangChain is “langchain”.  Here is how to clone, build, and run the Docker Image:  git clone git@github.com:chroma-core/chroma.git  Edit the docker-compose.yml file and add ALLOW_RESET=TRUE under environment  ... command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config log_config.yml environment: - IS_PERSISTENT=TRUE - ALLOW_RESET=TRUE ports: - 8000:8000 ...  Then run docker-compose up -d --build  # create the chroma clientimport uuidimport chromadbfrom chromadb.config import Settingsclient = chromadb.HttpClient(settings=Settings(allow_reset=True))client.reset() # resets the databasecollection = client.create_collection(\"my_collection\")for doc in docs: collection.add( ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content )# tell LangChain to use our client and collection namedb4 = Chroma( client=client, collection_name=\"my_collection\", embedding_function=embedding_function,)query = \"What did the president say about Ketanji Brown Jackson\"docs = db4.similarity_search(query)print(docs[0].page_content)  Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'Header 1': 'Chroma', 'Header 2': 'Basic Example (using the Docker Container)\\u200b'}),\n",
       " Document(page_content='While building toward a real application, you want to go beyond adding data, and also update and delete data.  Chroma has users provide ids to simplify the bookkeeping here. ids can be the name of the file, or a combined has like filename_paragraphNumber, etc.  Chroma supports all these operations - though some of them are still being integrated all the way through the LangChain interface. Additional workflow improvements will be added soon.  Here is a basic example showing how to do various operations:  # create simple idsids = [str(i) for i in range(1, len(docs) + 1)]# add dataexample_db = Chroma.from_documents(docs, embedding_function, ids=ids)docs = example_db.similarity_search(query)print(docs[0].metadata)# update the metadata for a documentdocs[0].metadata = { \"source\": \"../../modules/state_of_the_union.txt\", \"new_value\": \"hello world\",}example_db.update_document(ids[0], docs[0])print(example_db._collection.get(ids=[ids[0]]))# delete the last documentprint(\"count before\", example_db._collection.count())example_db._collection.delete(ids=[ids[-1]])print(\"count after\", example_db._collection.count())  {\\'source\\': \\'../../../state_of_the_union.txt\\'}{\\'ids\\': [\\'1\\'], \\'embeddings\\': None, \\'metadatas\\': [{\\'new_value\\': \\'hello world\\', \\'source\\': \\'../../../state_of_the_union.txt\\'}], \\'documents\\': [\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\']}count before 46count after 45', metadata={'Header 1': 'Chroma', 'Header 2': 'Update and Delete\\u200b'}),\n",
       " Document(page_content='Many people like to use OpenAIEmbeddings, here is how to set that up.  # get a token: https://platform.openai.com/account/api-keysfrom getpass import getpassfrom langchain_openai import OpenAIEmbeddingsOPENAI_API_KEY = getpass()  import osos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY  embeddings = OpenAIEmbeddings()new_client = chromadb.EphemeralClient()openai_lc_client = Chroma.from_documents( docs, embeddings, client=new_client, collection_name=\"openai_collection\")query = \"What did the president say about Ketanji Brown Jackson\"docs = openai_lc_client.similarity_search(query)print(docs[0].page_content)  Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'Header 1': 'Chroma', 'Header 2': 'Use OpenAI Embeddings\\u200b'}),\n",
       " Document(page_content=\"The returned distance score is cosine distance. Therefore, a lower score is better.  docs = db.similarity_search_with_score(query)  docs[0]  (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}), 1.1972057819366455)\", metadata={'Header 1': 'Chroma', 'Header 2': 'Other Information\\u200b', 'Header 3': 'Similarity search with score\\u200b'}),\n",
       " Document(page_content='This section goes over different options for how to use Chroma as a retriever.', metadata={'Header 1': 'Chroma', 'Header 2': 'Other Information\\u200b', 'Header 3': 'Retriever options\\u200b'}),\n",
       " Document(page_content='In addition to using similarity search in the retriever object, you can also use mmr.  retriever = db.as_retriever(search_type=\"mmr\")  retriever.get_relevant_documents(query)[0]  Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'})', metadata={'Header 1': 'Chroma', 'Header 2': 'Other Information\\u200b', 'Header 3': 'Retriever options\\u200b', 'Header 4': 'MMR\\u200b'}),\n",
       " Document(page_content='It can be helpful to narrow down the collection before working with it.  For example, collections can be filtered on metadata using the get method.  # filter collection for updated sourceexample_db.get(where={\"source\": \"some_other_source\"})  {\\'ids\\': [], \\'embeddings\\': None, \\'metadatas\\': [], \\'documents\\': []}', metadata={'Header 1': 'Chroma', 'Header 2': 'Other Information\\u200b', 'Header 3': 'Filtering on metadata\\u200b'}),\n",
       " Document(page_content='Previous  BigQuery Vector Search  Next  Clarifai  Basic ExampleBasic Example (including saving to disk)Passing a Chroma Client into LangchainBasic Example (using the Docker Container)Update and DeleteUse OpenAI EmbeddingsOther Information  Similarity search with scoreRetriever optionsFiltering on metadata  Community  DiscordTwitter  GitHub  PythonJS/TS  More  HomepageBlog  Copyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_docs = []\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "    (\"h5\", \"Header 5\"),\n",
    "    (\"h6\", \"Header 6\"),\n",
    "]\n",
    "\n",
    "for address in web_addresses:\n",
    "    html_text = str(BeautifulSoup(requests.get(address).content, \"html.parser\"))\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on).split_text(html_text)\n",
    "\n",
    "for doc in html_splitter:\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \"\")\n",
    "    \n",
    "web_docs.extend(html_splitter)\n",
    "\n",
    "web_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loader & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "docs.extend(yt_docs)\n",
    "docs.extend(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "  docs,\n",
    "  embedding=OpenAIEmbeddings(),\n",
    "  persist_directory='./data'\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieverdb = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main content  🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat  🦜️🔗  LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  Use cases  Interacting with APIsChatbotsExtractionSummarizationTaggingWeb scrapingCode understandingSynthetic data generation  Q&A with RAG  SQL  Tool use  Graph querying  Use casesSummarization  On this page  SummarizationUse case\\u200bOverview\\u200bQuickstart\\u200bOption 1. Stuff\\u200bGo deeper\\u200bOption 2. Map-Reduce\\u200bGo deeper\\u200bOption 3. Refine\\u200bSplitting and summarizing in a single chain\\u200b\"),\n",
       " Document(page_content=\"Skip to main content  🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat  🦜️🔗  LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  LangServeLangGraph  Get started  IntroductionInstallationQuickstartSecurity  LangChain Expression Language  Get startedWhy use LCELInterfaceLangChain Expression Language (LCEL)  How to  Cookbook  Modules  Chains  Model I/O  Model I/OQuickstartConcepts  Prompts  Quick StartCompositionExample selectorsFew-shot prompt templatesFew-shot examples for chat modelsTypes of `MessagePromptTemplate`Partial prompt templatesPipeline  Example Selector Types  Chat Models  LLMs  Output Parsers  Retrieval  Agents  More  LangSmith  ModulesModel I/OPromptsQuick Start  On this page  Quick StartPromptTemplate\\u200bChatPromptTemplate\\u200bLCEL\\u200b\"),\n",
       " Document(page_content=\"Skip to main content  🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat  🦜️🔗  LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  Providers  AnthropicAWSGoogleHugging FaceMicrosoftOpenAI  More  Components  LLMs  Chat models  Document loaders  Document transformers  Text embedding models  Vector stores  Activeloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAstra DBAtlasAwaDBAzure Cosmos DBAzure AI SearchBagelDBBaidu Cloud ElasticSearch VectorSearchBigQuery Vector SearchChromaClarifaiClickHouseDashVectorDatabricks Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissFaiss (Async)Google Vertex AI Vector SearchSAP HANA Cloud Vector EngineHippoHologresJaguar Vector DatabaseKDB.AILanceDBLanternLLMRailsMarqoMeilisearchMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVecto.rsPGVectorPineconeQdrantRedisRocksetScaNNSemaDBSingleStoreDBscikit-learnSQLite-VSSStarRocksSupabase (Postgres)SurrealDBTairTencent Cloud VectorDBTigrisTileDBTimescale Vector (Postgres)TypesenseUSearchValdVearchVectaraVespaviking DBWeaviateXataYellowbrickZepZilliz  Retrievers  Tools  Agents and toolkits  Memory  Callbacks  Chat loaders  Adapters  Stores  ComponentsVector storesChroma  On this page  ChromaBasic Example\\u200bBasic Example (including saving to disk)\\u200bPassing a Chroma Client into Langchain\\u200bBasic Example (using the Docker Container)\\u200bUpdate and Delete\\u200bUse OpenAI Embeddings\\u200bOther Information\\u200bSimilarity search with score\\u200bRetriever options\\u200bMMR\\u200bFiltering on metadata\\u200b\")]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the vectordb\n",
    "query = \"What is langsmith and its use cases\"\n",
    "docs = retrieverdb.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Example Prompt\n",
    "style = \"\"\"\n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    ## CODE EXAMPLES\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        Follow the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        Follow the guidelines provided in the style (between the dotted lines) for reference on how to summarize the note.\n",
    "\n",
    "        ----------------------------------------------\n",
    "        {style}\n",
    "        ----------------------------------------------\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Prompt Engineering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", user_message)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"input\") | retrieverdb,\n",
    "        \"input\": itemgetter(\"modelinput\"),\n",
    "        \"style\": itemgetter(\"style\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('input'))\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f0a78b6e320>, search_kwargs={'k': 3}),\n",
       "  input: RunnableLambda(itemgetter('input')),\n",
       "  style: RunnableLambda(itemgetter('style'))\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'input', 'style'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style'], template='\\n        You are a world class note taking assistant.\\n        You summarize notes into concise manner summarizing the most relevant information.\\n        Follow the information in given in the text, do not make things up - unless you are asked for examples.\\n        Follow the guidelines provided in the style (between the dotted lines) for reference on how to summarize the note.\\n\\n        ----------------------------------------------\\n        {style}\\n        ----------------------------------------------\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Use the following document to summarize into a note under the specified title\\n        the note must follow a specified styleand  specified style:\\n        \\n        <document>\\n        {context}\\n        </document>\\n        \\n        title: {input}'))])\n",
       "| ChatOpenAI(verbose=True, client=<openai.resources.chat.completions.Completions object at 0x7f0a621253f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f0a62157610>, model_name='gpt-3.5-turbo-1106', openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": title, \"style\": style})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# TITLE: \n",
       "Prompt Engineering\n",
       "\n",
       "## ABSTRACT: \n",
       "- Prompt templates are predefined recipes for generating prompts for language models. \n",
       "- LangChain provides tooling to create and work with prompt templates, aiming to create model agnostic templates to facilitate reuse across different language models. \n",
       "- Users can easily customize the prompt and try different LLMs (e.g., Claude) via the llm parameter.\n",
       "\n",
       "## KEY POINTS:\n",
       "- Prompt Templates:\n",
       "    - Predefined recipes for generating prompts for language models.\n",
       "    - Includes instructions, few-shot examples, specific context, and questions for a given task.\n",
       "- LangChain Tooling:\n",
       "    - Provides tooling to create and work with prompt templates.\n",
       "    - Aims to create model agnostic templates for easy reuse across different language models.\n",
       "- Prompt Customization:\n",
       "    - Users can easily customize the prompt and try different LLMs via the llm parameter.\n",
       "\n",
       "## CONTEXT\n",
       "- Prompt templates are predefined recipes for generating prompts for language models, including instructions, few-shot examples, specific context, and questions appropriate for a given task.\n",
       "- LangChain provides tooling to create and work with prompt templates, striving to create model agnostic templates for easy reuse across different language models.\n",
       "- Users can easily customize the prompt and try different LLMs (e.g., Claude) via the llm parameter.\n",
       "- The document discusses setting up a LangChain environment for prompt engineering, involving the creation of an app using LangChain CLI and LangServe, as well as setting up LangSmith for debugging purposes.\n",
       "- It also touches upon the process of creating a chain in a Jupyter notebook to demonstrate how it can be exported from a notebook, emphasizing the iterative environment and ease of exporting a chain.\n",
       "- The document provides code examples and mentions importing prompt templates, output parsers, and chat models from LangChain, highlighting the simplicity and effectiveness of the prompt engineering process.\n",
       "\n",
       "## REFLECTIONS\n",
       "- How does LangChain ensure model agnostic templates for prompt engineering?\n",
       "- What are the benefits of using Jupyter notebook for creating and exporting prompt chains?\n",
       "- Can you provide an example of customizing a prompt for a specific language model?\n",
       "- How does LangSmith aid in the debugging process for prompt engineering?\n",
       "- The document emphasizes the importance of model agnostic prompt templates and the ease of customization, showcasing the practical application of prompt engineering in language model tasks.\n",
       "\n",
       "    ----------------------------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Query Experiments - No Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the splitting of the transcripts by chunks according to tokens\n",
    "It seems to be better to split by tokens - where we are getting exact as opposed to the Characters (via RecursiveCharacterTextSplitter) because we are achieving more accurate results based on input limits - which is our primary concern. By defining encoding name and model_name, we can get exact num_tokens as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "# chunks = splitter.split_documents([document])\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=15000, chunk_overlap=100, encoding_name=\"cl100k_base\", model_name=model)\n",
    "\n",
    "texts = text_splitter.split_text(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(texts[0], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_i = \"\"\"\n",
    "    You are a note taking assistant for a courses. \n",
    "    \n",
    "    Given the Document, write a note based on the following format and instructions defined in point format below:\n",
    "    \n",
    "    # TITLE: \n",
    "    - One line catchy title to capture the essence of the document.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_outputs = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    user_message = f\"\"\"Document: {texts[i]}\"\"\"\n",
    "    prompt = [\n",
    "            {'role': 'system', 'content': system_message_i},\n",
    "            {'role': 'user', 'content': user_message}\n",
    "        ]\n",
    "\n",
    "    response = get_completion(prompt, model=model)\n",
    "    \n",
    "    list_of_outputs.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_c = \"\"\"\n",
    "    You are a note taking assistant for a courses who summarizes several notes. Each note is separated by \"***\" sign\n",
    "    \n",
    "    Given these combined notes, compile a new note with the same headers but combining the points under each header. \n",
    "    Make sure there is no duplication of points.    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note = \"***\"\n",
    "\n",
    "for i in range(len(list_of_outputs)):\n",
    "    combined_note += list_of_outputs[i] + \"***\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*** on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.***the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result***some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\nIn addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\nRemember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!***torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.***# TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.***model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check***we've replicated the same model using nn.sequential which is a simpler way to define a neural network in PyTorch. We've used nn.sequential to define the layers of our model and set the in and out features accordingly. This allows us to create a neural network with the same architecture as before, but with less code.\\n\\nIn the next video, we'll move on to defining a loss function and an optimizer for our model. We'll also explore how to train and evaluate our model using the training and test data sets we created earlier.***hyperparameters that we've changed so far and then we're going to change the number of epochs so we're going to go from 100 to a thousand so we're going to train for longer so let's write some code to do that so we're going to create a new model so we're going to call this circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1***of combining linear and non-linear functions remains the same. So let's write some code to replicate these non-linear activation functions. We'll start by creating a simple non-linear function using Python. We'll create a function that applies the rectified linear unit (ReLU) function to an input. The ReLU function returns the maximum of 0 or the input value. Here's how we can write this function:\\n\\n```python\\ndef relu(x):\\n    return max(0, x)\\n```\\n\\nThis simple function demonstrates the essence of the ReLU activation function. It takes an input and returns the maximum of 0 or the input value. This is a basic representation of a non-linear activation function.\\n\\nNext, let's replicate the sigmoid function. The sigmoid function is another common non-linear activation function used in neural networks. It takes an input and returns a value between 0 and 1, which is often interpreted as a probability. Here's how we can write this function:\\n\\n```python\\nimport math\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\n```\\n\\nThese simple functions demonstrate the basic principles of non-linear activation functions. They take an input and apply a non-linear transformation to produce an output.\\n\\nFinally, let's replicate the ReLU and sigmoid functions using PyTorch. PyTorch provides built-in non-linear activation functions that we can use in our neural network models. Here's how we can use the ReLU and sigmoid functions in PyTorch:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\n# Using ReLU in PyTorch\\nrelu = nn.ReLU()\\n\\n# Using sigmoid in PyTorch\\nsigmoid = nn.Sigmoid()\\n```\\n\\nThese PyTorch functions provide the same non-linear transformations as the custom functions we wrote earlier. They can be easily integrated into our neural network models to introduce non-linearity.\\n\\nBy replicating these non-linear activation functions, we gain a deeper understanding of how they work and how they contribute to the power of neural networks in capturing complex patterns in data.\\n\\nIn conclusion, non-linear activation functions play a crucial role in enabling neural networks to model complex relationships in data. By replicating and understanding these functions, we can better appreciate their impact on the performance of neural network models.***solutions in the solutions folder but i'd highly recommend trying to do it on your own first and then checking the solutions if you get stuck and then if you'd like some extra curriculum i'd recommend checking out the torch metrics module for 10 minutes and then also the article beyond accuracy precision and recall by Will Koisrin for when to use precision and recall and then finally the classification report in scikit-learn so that's a fair bit of stuff to practice and i'd highly recommend going through it and then once you've done that you can come back and we can go through the solutions together and see how you went but i think that's a fair bit of stuff to practice and i'll leave that with you and i'll see you in the next video.***number 0 and we have 32 samples of 28 by 28 pixels and then we have a tensor of 32 labels so we have 32 labels for each of our 32 samples and then we can visualize a single image from the batch so we can go plt.imshow and we can pass in train features batch at index 0 so we're going to visualize the first image from the batch and then we can go plt.title and we can pass in train labels batch at index 0 so we're going to visualize the label of the first image from the batch and then we can go plt.show and we can see what this looks like so we've got a single image from the batch and we've got the label of that image and then we can do the same for the test data loader and visualize a single image from the test data loader and its label and that's how we can visualize a batch of images and their labels from the data loader. So we've covered a lot in this video. We've prepared our data into mini batches using the data loader and we've visualized a batch of images from the data loader. In the next video, we'll start building our convolutional neural network model to classify these images. I'll see you there!***'re right. We can create a function for our training loop and testing loop to make our code more modular and reusable. This will allow us to easily train and evaluate different models without having to rewrite the same code multiple times. Let's start by creating a function for the training loop.\\n\\n```python\\ndef train_model(model, train_loader, criterion, optimizer, device):\\n    model.train()  # Set the model to training mode\\n    train_loss = 0.0  # Initialize the training loss\\n\\n    for inputs, labels in train_loader:  # Loop through the training batches\\n        inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n        optimizer.zero_grad()  # Zero the gradients\\n        outputs = model(inputs)  # Forward pass\\n        loss = criterion(outputs, labels)  # Calculate the loss\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update the model parameters\\n\\n        train_loss += loss.item() * inputs.size(0)  # Accumulate the training loss\\n\\n    train_loss = train_loss / len(train_loader.dataset)  # Calculate the average training loss\\n\\n    return train_loss\\n```\\n\\nIn this function, we set the model to training mode, loop through the training batches, move the inputs and labels to the device, perform the forward pass, calculate the loss, perform backpropagation, update the model parameters, and accumulate the training loss. Finally, we calculate the average training loss and return it.\\n\\nNext, let's create a function for the testing loop.\\n\\n```python\\ndef test_model(model, test_loader, criterion, device):\\n    model.eval()  # Set the model to evaluation mode\\n    test_loss = 0.0  # Initialize the test loss\\n    correct = 0  # Initialize the number of correct predictions\\n\\n    with torch.no_grad():  # Disable gradient calculation\\n        for inputs, labels in test_loader:  # Loop through the testing batches\\n            inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n            outputs = model(inputs)  # Forward pass\\n            test_loss += criterion(outputs, labels).item() * inputs.size(0)  # Accumulate the test loss\\n            _, predicted = outputs.max(1)  # Get the predicted labels\\n            correct += predicted.eq(labels).sum().item()  # Count the number of correct predictions\\n\\n    test_loss = test_loss / len(test_loader.dataset)  # Calculate the average test loss\\n    test_accuracy = correct / len(test_loader.dataset)  # Calculate the test accuracy\\n\\n    return test_loss, test_accuracy\\n```\\n\\nIn this function, we set the model to evaluation mode, loop through the testing batches, move the inputs and labels to the device, perform the forward pass, accumulate the test loss, count the number of correct predictions, and calculate the average test loss and test accuracy. Finally, we return the test loss and test accuracy.\\n\\nThese functions will allow us to train and evaluate our models in a more organized and reusable manner. Now, we can use these functions to train and evaluate our model v1. Let's give it a go in the next video.***and so on and so forth so you can play around with these values and see how the output shape changes as you pass data through a max pool layer and a convolutional layer and you can also change the kernel size and see how that affects the output shape as well. This is a great way to understand how these layers manipulate the shape of the data and how they can be used to extract features from the input.\\n\\nIn the next video, we'll continue to explore the other layers of the convolutional neural network and see how they affect the data. Keep experimenting and have fun with it!***Now that we have created the directory to save our model, we can proceed with saving the model itself. We will use the `torch.save` function to save the model to a file. Let's continue with the code.\\n\\n```python\\n# Save the model\\ntorch.save(model2.state_dict(), model_path / 'fashion_mnist_model.pth')\\n```\\n\\nIn this code, we are using the `torch.save` function to save the state dictionary of our model to a file called 'fashion_mnist_model.pth' in the 'models' directory that we created earlier.\\n\\nNow that we have saved our model, we can proceed with loading it back into memory. Let's continue with the code.\\n\\n```python\\n# Load the model\\nmodel = YourModelClass(*args, **kwargs)  # Instantiate your model\\nmodel.load_state_dict(torch.load(model_path / 'fashion_mnist_model.pth'))\\nmodel.eval()  # Set the model to evaluation mode\\n```\\n\\nIn this code, we are instantiating a new instance of our model class and then loading the state dictionary from the saved file back into the model. We then set the model to evaluation mode using `model.eval()`.\\n\\nBy following these steps, we have successfully saved our best performing model to a file and then loaded it back into memory. This allows us to use the model for making predictions or further training in the future.\\n\\nIf you have any questions or need further assistance, feel free to ask!***the transformed image so we're going to create a new variable called transformed and we're going to pass in our data transform and then we're going to pass in f so we're going to transform our original image and then we're going to plot it so we're going to go ax 1 dot m show and we're going to pass in the transformed image and then we're going to set the title so set title and we're going to set this to be the transformed image and then we're going to turn off the axes for the second plot so we're going to go axis and we're going to set that to false and then we're going to show the plot so plt dot show and then we're going to return the random image paths so that's our function there so let's see if this works so let's test it out so we're going to go visualize transformed images and we're going to pass in our image path list and we're going to pass in our data transform and we're going to pass in the number of images to transform so let's say we want to transform three images at a time and then we're going to set the random seed so we're going to set the seed to 42 and let's see what happens so we've got a pizza image here and then we've got the transformed image so we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image so we've got a way to visualize what our transformed images look like and this is a very important step when working with any kind of data set is to visualize what your transformed data looks like so that you can see if it's going to be suitable for your model and so that's how we can visualize our transformed images and in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video***# TITLE: \\n- Custom Data Loading: Replicating Image Folder Functionality\\n\\n## ABSTRACT: \\n- The document outlines the process of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It demonstrates the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set.\\n\\n## KEY POINTS:\\n- Subclassing torch.utils.data.Dataset to create a custom data set.\\n- Initializing the custom data set with a target directory and a transform.\\n- Creating attributes such as paths, transform, classes, and class to idx.\\n- Writing a function to load images and overriding the len and get item methods.\\n- Creating a function to display random images from the custom data set.\\n\\n## CONTEXT \\n- The document provides a detailed walkthrough of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. \\n- The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader.\\n\\n## REFLECTIONS\\n- How can the custom data set be further extended to handle additional data formats or preprocessing steps?\\n- What are the potential challenges of creating a custom data set, and how can they be addressed?\\n- The importance of visualizing data to ensure the correctness of the custom data loading process.\\n\\nRecap: The document provides a comprehensive guide to creating a custom data set in PyTorch, replicating the functionality of the original image folder data loader class. It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader. The importance of visualizing data to ensure the correctness of the custom data loading process is emphasized.***we don't use random seeds in practice but for educational purposes, we are using them to exemplify how we can get similar numbers on our page. However, ideally, regardless of the random seed used, our model's performance should be similar. \\n\\nWe are going to train for five epochs and create an instance of the `tiny vgg` model. The input shape for the model is three, as we are dealing with color images. The `hidden_units` is set to 10, in line with the CNN explainer website, and the `output_shape` is the number of classes in our training data set. \\n\\nFor the loss function, we are using the `nn.CrossEntropyLoss` as we are dealing with multi-class classification. As for the optimizer, we are trying the `Adam` optimizer with a learning rate of 0.001. The default learning rate for Adam is 1e-3.\\n\\nNow, let's move on to training and evaluating the model using the functions we have created in the previous sections.***we can make a prediction on our custom image. Let's see if we can get our model to predict on this image in the next video. I'll see you there!***# TITLE: \\n- Predicting on Custom Data with PyTorch: Overcoming Challenges and Building a Function\\n\\n## ABSTRACT: \\n- The document covers the process of making predictions on custom images using PyTorch, highlighting the challenges and steps involved in preparing the custom data for model prediction. It emphasizes the critical points of ensuring the data type, shape, and device compatibility with the model. The document also introduces the concept of functionizing the prediction process for ease of use and scalability.\\n\\n## KEY POINTS:\\n- Preparing Custom Data:\\n    - Ensuring the custom image is in the same data type, shape, and device as the model.\\n    - Adding a batch dimension to the image to align with the model's expectations.\\n- PyTorch Built-in Functions:\\n    - Exploring PyTorch's built-in functions for handling various data types, including images, text, and audio.\\n    - Custom data set classes can be written by subclassing torch.utils.data.dataset for specific requirements.\\n- Balancing Overfitting and Underfitting:\\n    - Understanding the balance between overfitting and underfitting in machine learning models.\\n- Predicting on Custom Data:\\n    - Highlighting the importance of addressing wrong data types, shapes, and devices when making predictions on custom data.\\n- Exercises and Extra Curriculum:\\n    - Encouraging practice through exercises and providing resources for further learning and exploration.\\n\\n## CONTEXT\\n- The document provides a detailed walkthrough of the process of preparing and making predictions on custom images using PyTorch. It covers the challenges faced, such as ensuring data compatibility with the model, and introduces the concept of functionizing the prediction process for scalability. The document emphasizes the importance of aligning the custom data with the model's expectations in terms of data type, shape, and device. It also encourages further learning through exercises and extra curriculum resources.\\n\\n- The process involves loading the custom image, transforming it to match the model's requirements, ensuring it is on the correct device, and adding a batch dimension. The document also discusses the balance between overfitting and underfitting in machine learning models and provides insights into PyTorch's built-in functions for handling diverse data types.\\n\\n- The critical takeaway is the significance of addressing wrong data types, shapes, and devices when making predictions on custom data. The document also encourages practical application through exercises and provides additional resources for further learning.\\n\\n## REFLECTIONS\\n- How can the challenges faced in preparing custom data for model prediction be mitigated effectively?\\n- What are the potential implications of incorrect data types, shapes, and devices when making predictions on custom data?\\n- The importance of functionizing the prediction process for scalability and ease of use is evident. How can this approach be further optimized for real-world applications?\\n\\n- The document provides a comprehensive understanding of the complexities involved in preparing and making predictions on custom data using PyTorch. It highlights the critical considerations and challenges, emphasizing the need for meticulous data preparation and alignment with the model's expectations. The concept of functionizing the prediction process adds a layer of scalability and practicality to the workflow, paving the way for further exploration and application in real-world scenarios.***\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(combined_note, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"Document: {combined_note}\"\"\"\n",
    "prompt = [\n",
    "        {'role': 'system', 'content': system_message_c},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "response = get_completion(prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Combined Note:\\n\\n## on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.\\n\\n## the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result\\n\\n## some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\n## In addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\n## Remember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!\\n\\n## torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.\\n\\n## # TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.\\n\\n## model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith Setup [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langsmith does not work - put in placeholder - need to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv('LANGSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__47bc0afe60cd4471a05f4f1578aac790\"\n",
    "!export LANGCHAIN_PROJECT=\"pt-large-date-95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Embeddings & Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with FAISS\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vector = FAISS.from_documents(docs, embeddings)\n",
    "# retriever = vector.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can provide tools and frameworks for implementing automated testing of software applications. This can help in quickly and efficiently running tests and identifying any issues or bugs in the code.\\n\\n2. Test Case Management: Langsmith can offer solutions for managing and organizing test cases, making it easier for QA teams to create, execute, and track test cases for different scenarios.\\n\\n3. Performance Testing: Langsmith can assist in conducting performance testing to evaluate the speed, responsiveness, and stability of software applications under various conditions.\\n\\n4. Test Data Management: Langsmith can provide tools for managing test data, ensuring that the right data is available for testing different aspects of the software.\\n\\n5. Continuous Integration and Continuous Testing: Langsmith can support the implementation of continuous integration and continuous testing processes, enabling developers to quickly identify and fix issues in the code.\\n\\nOverall, Langsmith can help in streamlining the testing process, improving the efficiency and effectiveness of testing efforts, and ensuring the quality and reliability of software applications.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame the LLM object\n",
    "llm = ChatOpenAI(model=model, verbose=True)\n",
    "llm.invoke(\"how can langsmith help with testing?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        Follow the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        Follow the guidelines provided in the style for reference on how to summarize the note.\n",
    "\n",
    "        <style>\n",
    "        {style}\n",
    "        </style>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"user\", \n",
    "        \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Prompt Engineering\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response answer: <style>\n",
      "        \n",
      "    # TITLE: \n",
      "    - Prompt Engineering\n",
      "    \n",
      "    ## ABSTRACT: \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models, including instructions, few-shot examples, and specific context and questions. LangChain provides tooling for creating and working with prompt templates, aiming to create model agnostic templates for easy reuse across different language models.\n",
      "    \n",
      "    ## KEY POINTS:\n",
      "    - Prompt templates: Predefined recipes for generating prompts for language models.\n",
      "    - LangChain tooling: Provides tools for creating and working with prompt templates.\n",
      "    - Model agnostic templates: Designed for easy reuse across different language models.\n",
      "    - Prompt structure: Language models expect the prompt to be either a string or a list of chat messages.\n",
      "    \n",
      "    ## CONTEXT \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models. These templates include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. The goal is to create model agnostic templates to make it easy to reuse existing templates across different language models. Language models expect the prompt to either be a string or a list of chat messages.\n",
      "    - The document discusses the process of prompt engineering and the use of LangChain CLI to set up a virtual environment and create an application called \"open AI prompter.\" The author mentions the use of LangChain CLI to bootstrap a Lang serve project and the importance of using LangSmith for debugging during prompt engineering. The author also highlights the iterative nature of prompt engineering and the ease of creating and exporting a chain from a notebook. The document further details the deployment of the prompter using Lang serve and the process of sharing it with others using LangSmith.\n",
      "    \n",
      "    ## REFLECTIONS\n",
      "    - How can prompt engineering contribute to the efficiency of language models?\n",
      "    - What are the advantages of using model agnostic prompt templates?\n",
      "    - How does the use of LangChain CLI simplify the process of creating prompt templates?\n",
      "    \n",
      "    * The document provides a comprehensive overview of prompt engineering and the tools provided by LangChain for creating and working with prompt templates. It emphasizes the importance of model agnostic templates and the iterative nature of prompt engineering. Additionally, it highlights the deployment process using Lang serve and the sharing capabilities of LangSmith. Overall, it offers valuable insights into the practical application of prompt engineering in language model development.\n",
      "    \n",
      "        </style>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response answer: {response['answer']}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f2aac22c790>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n        You are a world class note taking assistant.\\n        You summarize notes into concise manner summarizing the most relevant information.\\n        Follow the information in given in the text, do not make things up - unless you are asked for examples.\\n        Follow the guidelines provided in the style for reference on how to summarize the note.\\n\\n        <style>\\n        \\n    # TITLE: \\n    - As per user provided Title.\\n    \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate 3 to 5 questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n    \\n        </style>\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Use the following document to summarize into a note under the specified title\\n        the note must follow a specified styleand  specified style:\\n        \\n        <document>\\n        {context}\\n        </document>\\n        \\n        title: {input}'))])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f2aac22cca0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f2aac22f070>, openai_api_key='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', openai_proxy='')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(retriever, document_chain) \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChatPromptTemplate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m]),\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "chain = create_retrieval_chain(retriever, document_chain) | output_parser\n",
    "chain.invoke({\"input\": \"ChatPromptTemplate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperSeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing by providing a comprehensive and efficient testing framework. Here are some ways in which Langsmith can help with testing:\\n\\n1. Test Automation: Langsmith enables the automation of tests by providing a testing framework that allows developers to write tests in a clear and concise manner. This framework supports various testing methodologies, such as unit testing, integration testing, and end-to-end testing.\\n\\n2. Test Coverage: Langsmith helps in measuring the coverage of tests by providing tools and features to track which parts of the codebase have been tested. This ensures that all critical areas of the application are thoroughly tested, reducing the chances of undiscovered bugs.\\n\\n3. Test Orchestration: Langsmith allows developers to easily manage and run tests across different environments and configurations. It provides a way to define test suites, group related tests, and execute them in parallel or sequentially, improving the testing process's efficiency.\\n\\n4. Test Reporting: Langsmith generates detailed and comprehensive test reports, highlighting the test results, including pass/fail status, execution time, and any errors or exceptions encountered. These reports help in identifying and debugging issues quickly.\\n\\n5. Mocking and Stubbing: Langsmith provides mechanisms for creating mock objects or stubs, allowing developers to isolate components for testing. This is particularly useful when testing code that relies on external dependencies, as it enables developers to simulate their behavior, leading to more reliable and independent tests.\\n\\n6. Performance Testing: Langsmith offers tools and utilities for performance testing, enabling developers to evaluate the system's performance under different load conditions. This helps identify potential bottlenecks and optimize the application for better scalability and responsiveness.\\n\\n7. Continuous Integration/Continuous Delivery (CI/CD) Integration: Langsmith seamlessly integrates with CI/CD pipelines, allowing tests to be automatically triggered on code changes or deployments. This ensures that tests are run consistently and continuously, providing immediate feedback on the code's quality.\\n\\nOverall, Langsmith enhances the testing process by providing a robust framework, tools, and utilities that facilitate test automation, coverage analysis, test orchestration, reporting, mocking, and performance testing. These capabilities help ensure the reliability, stability, and quality of the software being developed.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can greatly assist with testing in several ways:\\n\\n1. Test Case Generation: Langsmith can automatically generate test cases based on the specifications and requirements of your software. It uses natural language processing techniques to understand the desired behavior of the system and generate test cases that cover various scenarios and edge cases. This can save significant time and effort in test case creation.\\n\\n2. Test Data Generation: Langsmith can also generate realistic and diverse test data for your software. It understands the data requirements and constraints of your system and creates test data that covers different data types, ranges, and combinations. This can help in ensuring thorough testing and identifying potential issues related to data handling.\\n\\n3. Test Automation: Langsmith can generate test scripts or code snippets in various programming languages to automate the execution of test cases. It can integrate with popular testing frameworks and tools to streamline the automation process. This can greatly accelerate the testing process and improve test coverage.\\n\\n4. Test Documentation: Langsmith can assist in creating comprehensive test documentation. It can generate test plans, test scripts, test reports, and other relevant documentation based on the identified test cases. This ensures that the testing process is well-documented and easily understandable for all stakeholders.\\n\\n5. Test Oracles: Langsmith can help in defining oracles for automated testing. An oracle is a mechanism to determine whether the actual behavior of the system matches the expected behavior. Langsmith can understand the expected outcomes of various functionalities and generate oracles to validate the test results automatically.\\n\\nOverall, Langsmith can enhance the efficiency and effectiveness of testing by automating test case generation, test data generation, test script creation, and test documentation. It can also improve the quality of testing by ensuring comprehensive coverage and accurate oracles.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f8e84927340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8e6bf61d80>, model_name='gpt-3.5-turbo-1106', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \\n\\ntext: ```{text}```\\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"You are a study note taking assistant for courses.\n",
    "\n",
    "Given the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \n",
    "\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_style = \"\"\"\n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_text = document.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ntext: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\\n\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studynote_messages = prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)\n",
    "studynote_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_response = chat(studynote_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ABSTRACT:\n",
       "The document discusses the Lang chain expression language, which allows for writing minimalist code to build chains within line chain. It emphasizes the advanced features of parallel execution, async, and streaming using the expression language. The document also explores the syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## KEY POINTS:\n",
       "- Lang chain expression language for building minimalist code to create chains within line chain.\n",
       "- Advanced features include parallel execution, async, and streaming.\n",
       "- Syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## CONTEXT:\n",
       "- The Lang chain expression language allows for writing minimalist code to build chains within line chain, making it easier to use advanced features like parallel execution, async, and streaming.\n",
       "- The pipe operator is used to string components together, making the code simpler and more flexible.\n",
       "- The runnable lambdas are used to wrap functions and create custom operations within the expression language.\n",
       "- The document provides examples of using the expression language to retrieve information in parallel and modify the output using runnable lambdas.\n",
       "- The expression language has pros such as minimalist code and advanced features, but also cons like increased abstraction and non-standard syntax.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the Lang chain expression language compare to other methods of building chains within line chain?\n",
       "- What are the potential use cases for the runnable lambdas within the expression language?\n",
       "- The expression language offers a minimalist approach and advanced features, but also introduces increased abstraction and non-standard syntax."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(studynote_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_schema = ResponseSchema(name=\"python_code\",\n",
    "                            description=\"Parse any code within the text and write the code in string with backticks. \\\n",
    "                                If there was no code found, then output as -1.\")\n",
    "webpage_schema = ResponseSchema(name=\"webpage_link\",\n",
    "                                    description=\"Was there any webpage links recommended. \\\n",
    "                                    If this information is not found, output -1.\")\n",
    "reading_schema = ResponseSchema(name=\"further_reading\",\n",
    "                                    description=\"Extract any recommendations on further research or reading on the subject. \\\n",
    "                                    Output them as a comma separated Python list. If none is recommended, output -1.\")\n",
    "\n",
    "response_schemas = [python_schema, \n",
    "                    webpage_schema,\n",
    "                    reading_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\\n\\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\\n\\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\\n}\\n```'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
      "\n",
      "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
      "\n",
      "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "\n",
      "text: today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\n",
      "\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\n",
      "\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studynote_text = document.page_content\n",
    "\n",
    "template_string2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
    "\n",
    "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
    "\n",
    "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "dict_prompt = ChatPromptTemplate.from_template(template=template_string2)\n",
    "\n",
    "dict_messages = dict_prompt.format_messages(text=studynote_text, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "print(dict_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_response = chat(messages)\n",
    "\n",
    "dict_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_code': -1,\n",
       " 'webpage_link': -1,\n",
       " 'further_reading': 'Lang chain expression language, Line chain products, Line chain abstraction'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "#extract dict key gift\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# Vector Data Memory\n",
    "# Entity Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": studynote_messages}, {\"output\": studynote_response})\n",
    "memory.save_context({\"input\": dict_messages}, {\"output\": dict_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\\nAI: ```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\n",
      "AI: ```json\n",
      "{\n",
      "\t\"python_code\": -1,\n",
      "\t\"webpage_link\": -1,\n",
      "\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\n",
      "}\n",
      "```\n",
      "Human: Hi, what is langchain about?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lang chain is a powerful expression language that allows for running retrievers in parallel and modifying the output using runnable lambdas. It has a specific syntax and is worth learning and experimenting with. It is used for line chain products and line chain abstraction.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is langchain about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)  \n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lang Chain Expression Language company'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = studynote_messages\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 1: input= text and output= summary\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Come up with a summary for the following text:\"\n",
    "    \"\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= search queries\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate some additional web search queries based on the summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"search_queries\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= topic\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a one line topic based summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, \n",
    "                     output_key=\"topic\"\n",
    "                    )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"search_queries\",\"topic\"],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\",\n",
       " 'summary': 'The text discusses the Lang chain expression language, which allows for minimalist code to build chains within Lang chain. It explores the advanced features of parallel execution, async, and streaming using the expression language. The text explains the syntax and shows examples of how to use the expression language with a practical use case. It also covers the pros and cons of using the expression language and provides a detailed explanation of how it works. Overall, the text provides a comprehensive overview of the Lang chain expression language and its potential benefits and drawbacks.',\n",
       " 'search_queries': '1. How to use Lang chain expression language for parallel execution?\\n2. What are the practical use cases for Lang chain expression language?\\n3. Pros and cons of using Lang chain expression language for async execution\\n4. What are the advanced features of Lang chain expression language?\\n5. Examples of minimalist code using Lang chain expression language\\n6. How does streaming work in Lang chain expression language?\\n7. Comparison of Lang chain expression language with other expression languages\\n8. Is Lang chain expression language suitable for large-scale applications?\\n9. Tips for optimizing code using Lang chain expression language\\n10. How to get started with Lang chain expression language programming?',\n",
       " 'topic': 'An in-depth exploration of the Lang chain expression language and its advanced features, syntax, practical use cases, and pros and cons.'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = document.page_content\n",
    "overall_chain(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over a larger document - Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working right now - need to revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/home/dpvj/SecondBrain/templates/Meeting Note Template.md\")\n",
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/pydantic/_migration.py:276: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wnat are the key points of the document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- [ ]  Follow-up meetings scheduled\\n- [ ]  Next steps and responsibilities assigned\\n\\n## Decisions\\n_List any decisions made during the meeting_\\n\\n## Next Steps\\n_Outline the next steps and responsibilities_\\n\\n## Follow-up\\n_Summarize any follow-up actions required_\\n\\n## Additional Notes\\n_Any additional notes or information related to the meeting_\\n\\n## Meeting Adjourned\\n_Time the meeting was adjourned_\\n\\n## Next Meeting\\n_Date, time, and agenda for the next meeting_'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.call_as_llm(f\"{qdocs} Question: Please list all the headers of the documents\") \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_format = studynote_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetingnote_format = \"\"\"\n",
    "## Agenda\n",
    "_Summarize the agenda of the meeting_\n",
    "_Ensure key stakeholders are participating & Leading_\n",
    "\n",
    "## Goals\n",
    "_What do we want to achieve from this meeting_\n",
    "_Align with why the meeting was called in the first place_\n",
    "\n",
    "## Discussion notes\n",
    "_Write the notes that are key to the goals & objectives, note who has said it_\n",
    "\n",
    "## Action items\n",
    "_Summarize the action items_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"studynote\", \n",
    "        \"description\": \"Good for taking notes for a course\", \n",
    "        \"prompt_template\": studynote_template,\n",
    "        \"prompt_style\": studynote_format\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meetingnote\",  # Changed from \"math\" to \"meetingnote\"\n",
    "        \"description\": \"Good for taking notes for a meeting\", \n",
    "        \"prompt_template\": meetingnote_template,\n",
    "        \"prompt_style\": meetingnote_format\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\nYou are expected to take notees for a course.\\n\\nGiven the course delimeted by tripple backticks, extract information into a study note following the format: \\n\\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ncourse: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\")]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains['studynote'].prompt.format_messages(format=studynote_style, text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:97\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:511\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    512\u001b[0m         _output_key\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    517\u001b[0m         _output_key\n\u001b[1;32m    518\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:92\u001b[0m, in \u001b[0;36mMultiRouteChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     91\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m---> 92\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouter_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mdestination) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mnext_inputs), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m route\u001b[38;5;241m.\u001b[39mdestination:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:40\u001b[0m, in \u001b[0;36mRouterChain.route\u001b[0;34m(self, inputs, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Route:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Route inputs to a destination chain.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        a Route object\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Route(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:61\u001b[0m, in \u001b[0;36mLLMRouterChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     57\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     58\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m     60\u001b[0m     Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:114\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m raised following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Web Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrap_text(url: str):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>begin this chain now I'm going to</td>\n",
       "      <td>612.959</td>\n",
       "      <td>6.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>implement it with this we will see that</td>\n",
       "      <td>615.880</td>\n",
       "      <td>6.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>line chain actually uses I think they</td>\n",
       "      <td>619.720</td>\n",
       "      <td>3.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>use</td>\n",
       "      <td>622.160</td>\n",
       "      <td>3.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>invoke so rather than call they would</td>\n",
       "      <td>623.360</td>\n",
       "      <td>5.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>what we we had earlier so yeah we have</td>\n",
       "      <td>1212.159</td>\n",
       "      <td>5.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>these two functions let's take those</td>\n",
       "      <td>1214.400</td>\n",
       "      <td>4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>okay we can see runable it's what we</td>\n",
       "      <td>1217.320</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>were doing before so that we could use</td>\n",
       "      <td>1219.240</td>\n",
       "      <td>5.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>this let's do it again</td>\n",
       "      <td>1221.280</td>\n",
       "      <td>5.879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text     start  duration\n",
       "226        begin this chain now I'm going to   612.959     6.761\n",
       "227  implement it with this we will see that   615.880     6.280\n",
       "228    line chain actually uses I think they   619.720     3.640\n",
       "229                                      use   622.160     3.480\n",
       "230    invoke so rather than call they would   623.360     5.919\n",
       "..                                       ...       ...       ...\n",
       "449   what we we had earlier so yeah we have  1212.159     5.161\n",
       "450     these two functions let's take those  1214.400     4.840\n",
       "451     okay we can see runable it's what we  1217.320     3.960\n",
       "452   were doing before so that we could use  1219.240     5.319\n",
       "453                   this let's do it again  1221.280     5.879\n",
       "\n",
       "[228 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = YouTubeTranscriptApi.get_transcripts([video_id], languages=['en'])\n",
    "ls = list(transcripts[0].values())[0]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(ls)\n",
    "\n",
    "filtered_df = df[(df['start'] >= 611) & (df['start'] <= 1224)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\n",
      "\n",
      "Key Links:\n",
      "Code from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
      "LangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\n",
      "GPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "soup = BeautifulSoup(requests.get('https://www.youtube.com/watch?v=DjuXACWYkkU').content)\n",
    "pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'AIzaSyDYyXnayylCG2L1ToqrZykiVA--QxZ7-3Y'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=video_id\n",
    ")\n",
    "response = request.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#videoListResponse',\n",
       " 'etag': 'VB1pOZ9dsyaJCmlUHZOx-KOEwbk',\n",
       " 'items': [{'kind': 'youtube#video',\n",
       "   'etag': 'mFwyljJthlBCHJcUCN2cbl3du5U',\n",
       "   'id': 'DjuXACWYkkU',\n",
       "   'snippet': {'publishedAt': '2023-11-16T14:35:01Z',\n",
       "    'channelId': 'UCC-lyoTfSrcJzA1ab3APAgw',\n",
       "    'title': 'Building a Research Assistant from Scratch',\n",
       "    'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher',\n",
       "    'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/default.jpg',\n",
       "      'width': 120,\n",
       "      'height': 90},\n",
       "     'medium': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/mqdefault.jpg',\n",
       "      'width': 320,\n",
       "      'height': 180},\n",
       "     'high': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/hqdefault.jpg',\n",
       "      'width': 480,\n",
       "      'height': 360},\n",
       "     'standard': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/sddefault.jpg',\n",
       "      'width': 640,\n",
       "      'height': 480},\n",
       "     'maxres': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/maxresdefault.jpg',\n",
       "      'width': 1280,\n",
       "      'height': 720}},\n",
       "    'channelTitle': 'LangChain',\n",
       "    'categoryId': '22',\n",
       "    'liveBroadcastContent': 'none',\n",
       "    'localized': {'title': 'Building a Research Assistant from Scratch',\n",
       "     'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher'}},\n",
       "   'contentDetails': {'duration': 'PT43M40S',\n",
       "    'dimension': '2d',\n",
       "    'definition': 'hd',\n",
       "    'caption': 'false',\n",
       "    'licensedContent': False,\n",
       "    'contentRating': {},\n",
       "    'projection': 'rectangular'},\n",
       "   'statistics': {'viewCount': '9045',\n",
       "    'likeCount': '307',\n",
       "    'favoriteCount': '0',\n",
       "    'commentCount': '27'}}],\n",
       " 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = response['items'][0]['snippet']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_date = response['items'][0]['snippet']['publishedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = response['items'][0]['statistics']['viewCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a note taking assistant for a courses. \\n    Given the following document, write key points.\\n    If the document is not relevant, write \"not relevant\".\\n    '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8TClnBZvafYSdsb9WiFpkNfbp1GOt', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1701971291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
