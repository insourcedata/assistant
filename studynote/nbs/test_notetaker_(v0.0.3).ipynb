{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base OAI Libraries & Environment Setup\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain Libraries\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "# For the Youtube Trancript Download\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Used - for Future Reference\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import html2text\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text, model=None):\n",
    "    if model == 'gpt-4':\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_youtube(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Convert to text\n",
    "        transcript = ' '.join([t['text'] for t in transcript])\n",
    "\n",
    "        # Create document\n",
    "        document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "\n",
    "        return document\n",
    "    except:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Setup - gpt-3.5-turbo-1106 is a chat model\n",
    "client = OpenAI()\n",
    "model=\"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3271"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id = 'mmBo8nlu2j0' # Auto-Prompt Builder (with Hosted LangServe) smaller number of tokens 23 mins\n",
    "transcript=read_youtube(video_id).page_content\n",
    "get_num_tokens(transcript, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the splitting of the transcripts by chunks according to tokens\n",
    "It seems to be better to split by tokens - where we are getting exact as opposed to the Characters (via RecursiveCharacterTextSplitter) because we are achieving more accurate results based on input limits - which is our primary concern. By defining encoding name and model_name, we can get exact num_tokens as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "# chunks = splitter.split_documents([document])\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=15000, chunk_overlap=100, encoding_name=\"cl100k_base\", model_name=model)\n",
    "\n",
    "texts = text_splitter.split_text(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(texts[0], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_i = \"\"\"\n",
    "    You are a note taking assistant for a courses. \n",
    "    \n",
    "    Given the Document, write a note based on the following format and instructions defined in point format below:\n",
    "    \n",
    "    # TITLE: \n",
    "    - One line catchy title to capture the essence of the document.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_outputs = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    user_message = f\"\"\"Document: {texts[i]}\"\"\"\n",
    "    prompt = [\n",
    "            {'role': 'system', 'content': system_message_i},\n",
    "            {'role': 'user', 'content': user_message}\n",
    "        ]\n",
    "\n",
    "    response = get_completion(prompt, model=model)\n",
    "    \n",
    "    list_of_outputs.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_c = \"\"\"\n",
    "    You are a note taking assistant for a courses who summarizes several notes. Each note is separated by \"***\" sign\n",
    "    \n",
    "    Given these combined notes, compile a new note with the same headers but combining the points under each header. \n",
    "    Make sure there is no duplication of points.    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note = \"***\"\n",
    "\n",
    "for i in range(len(list_of_outputs)):\n",
    "    combined_note += list_of_outputs[i] + \"***\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*** on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.***the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result***some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\nIn addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\nRemember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!***torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.***# TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.***model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check***we've replicated the same model using nn.sequential which is a simpler way to define a neural network in PyTorch. We've used nn.sequential to define the layers of our model and set the in and out features accordingly. This allows us to create a neural network with the same architecture as before, but with less code.\\n\\nIn the next video, we'll move on to defining a loss function and an optimizer for our model. We'll also explore how to train and evaluate our model using the training and test data sets we created earlier.***hyperparameters that we've changed so far and then we're going to change the number of epochs so we're going to go from 100 to a thousand so we're going to train for longer so let's write some code to do that so we're going to create a new model so we're going to call this circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1***of combining linear and non-linear functions remains the same. So let's write some code to replicate these non-linear activation functions. We'll start by creating a simple non-linear function using Python. We'll create a function that applies the rectified linear unit (ReLU) function to an input. The ReLU function returns the maximum of 0 or the input value. Here's how we can write this function:\\n\\n```python\\ndef relu(x):\\n    return max(0, x)\\n```\\n\\nThis simple function demonstrates the essence of the ReLU activation function. It takes an input and returns the maximum of 0 or the input value. This is a basic representation of a non-linear activation function.\\n\\nNext, let's replicate the sigmoid function. The sigmoid function is another common non-linear activation function used in neural networks. It takes an input and returns a value between 0 and 1, which is often interpreted as a probability. Here's how we can write this function:\\n\\n```python\\nimport math\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\n```\\n\\nThese simple functions demonstrate the basic principles of non-linear activation functions. They take an input and apply a non-linear transformation to produce an output.\\n\\nFinally, let's replicate the ReLU and sigmoid functions using PyTorch. PyTorch provides built-in non-linear activation functions that we can use in our neural network models. Here's how we can use the ReLU and sigmoid functions in PyTorch:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\n# Using ReLU in PyTorch\\nrelu = nn.ReLU()\\n\\n# Using sigmoid in PyTorch\\nsigmoid = nn.Sigmoid()\\n```\\n\\nThese PyTorch functions provide the same non-linear transformations as the custom functions we wrote earlier. They can be easily integrated into our neural network models to introduce non-linearity.\\n\\nBy replicating these non-linear activation functions, we gain a deeper understanding of how they work and how they contribute to the power of neural networks in capturing complex patterns in data.\\n\\nIn conclusion, non-linear activation functions play a crucial role in enabling neural networks to model complex relationships in data. By replicating and understanding these functions, we can better appreciate their impact on the performance of neural network models.***solutions in the solutions folder but i'd highly recommend trying to do it on your own first and then checking the solutions if you get stuck and then if you'd like some extra curriculum i'd recommend checking out the torch metrics module for 10 minutes and then also the article beyond accuracy precision and recall by Will Koisrin for when to use precision and recall and then finally the classification report in scikit-learn so that's a fair bit of stuff to practice and i'd highly recommend going through it and then once you've done that you can come back and we can go through the solutions together and see how you went but i think that's a fair bit of stuff to practice and i'll leave that with you and i'll see you in the next video.***number 0 and we have 32 samples of 28 by 28 pixels and then we have a tensor of 32 labels so we have 32 labels for each of our 32 samples and then we can visualize a single image from the batch so we can go plt.imshow and we can pass in train features batch at index 0 so we're going to visualize the first image from the batch and then we can go plt.title and we can pass in train labels batch at index 0 so we're going to visualize the label of the first image from the batch and then we can go plt.show and we can see what this looks like so we've got a single image from the batch and we've got the label of that image and then we can do the same for the test data loader and visualize a single image from the test data loader and its label and that's how we can visualize a batch of images and their labels from the data loader. So we've covered a lot in this video. We've prepared our data into mini batches using the data loader and we've visualized a batch of images from the data loader. In the next video, we'll start building our convolutional neural network model to classify these images. I'll see you there!***'re right. We can create a function for our training loop and testing loop to make our code more modular and reusable. This will allow us to easily train and evaluate different models without having to rewrite the same code multiple times. Let's start by creating a function for the training loop.\\n\\n```python\\ndef train_model(model, train_loader, criterion, optimizer, device):\\n    model.train()  # Set the model to training mode\\n    train_loss = 0.0  # Initialize the training loss\\n\\n    for inputs, labels in train_loader:  # Loop through the training batches\\n        inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n        optimizer.zero_grad()  # Zero the gradients\\n        outputs = model(inputs)  # Forward pass\\n        loss = criterion(outputs, labels)  # Calculate the loss\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update the model parameters\\n\\n        train_loss += loss.item() * inputs.size(0)  # Accumulate the training loss\\n\\n    train_loss = train_loss / len(train_loader.dataset)  # Calculate the average training loss\\n\\n    return train_loss\\n```\\n\\nIn this function, we set the model to training mode, loop through the training batches, move the inputs and labels to the device, perform the forward pass, calculate the loss, perform backpropagation, update the model parameters, and accumulate the training loss. Finally, we calculate the average training loss and return it.\\n\\nNext, let's create a function for the testing loop.\\n\\n```python\\ndef test_model(model, test_loader, criterion, device):\\n    model.eval()  # Set the model to evaluation mode\\n    test_loss = 0.0  # Initialize the test loss\\n    correct = 0  # Initialize the number of correct predictions\\n\\n    with torch.no_grad():  # Disable gradient calculation\\n        for inputs, labels in test_loader:  # Loop through the testing batches\\n            inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n            outputs = model(inputs)  # Forward pass\\n            test_loss += criterion(outputs, labels).item() * inputs.size(0)  # Accumulate the test loss\\n            _, predicted = outputs.max(1)  # Get the predicted labels\\n            correct += predicted.eq(labels).sum().item()  # Count the number of correct predictions\\n\\n    test_loss = test_loss / len(test_loader.dataset)  # Calculate the average test loss\\n    test_accuracy = correct / len(test_loader.dataset)  # Calculate the test accuracy\\n\\n    return test_loss, test_accuracy\\n```\\n\\nIn this function, we set the model to evaluation mode, loop through the testing batches, move the inputs and labels to the device, perform the forward pass, accumulate the test loss, count the number of correct predictions, and calculate the average test loss and test accuracy. Finally, we return the test loss and test accuracy.\\n\\nThese functions will allow us to train and evaluate our models in a more organized and reusable manner. Now, we can use these functions to train and evaluate our model v1. Let's give it a go in the next video.***and so on and so forth so you can play around with these values and see how the output shape changes as you pass data through a max pool layer and a convolutional layer and you can also change the kernel size and see how that affects the output shape as well. This is a great way to understand how these layers manipulate the shape of the data and how they can be used to extract features from the input.\\n\\nIn the next video, we'll continue to explore the other layers of the convolutional neural network and see how they affect the data. Keep experimenting and have fun with it!***Now that we have created the directory to save our model, we can proceed with saving the model itself. We will use the `torch.save` function to save the model to a file. Let's continue with the code.\\n\\n```python\\n# Save the model\\ntorch.save(model2.state_dict(), model_path / 'fashion_mnist_model.pth')\\n```\\n\\nIn this code, we are using the `torch.save` function to save the state dictionary of our model to a file called 'fashion_mnist_model.pth' in the 'models' directory that we created earlier.\\n\\nNow that we have saved our model, we can proceed with loading it back into memory. Let's continue with the code.\\n\\n```python\\n# Load the model\\nmodel = YourModelClass(*args, **kwargs)  # Instantiate your model\\nmodel.load_state_dict(torch.load(model_path / 'fashion_mnist_model.pth'))\\nmodel.eval()  # Set the model to evaluation mode\\n```\\n\\nIn this code, we are instantiating a new instance of our model class and then loading the state dictionary from the saved file back into the model. We then set the model to evaluation mode using `model.eval()`.\\n\\nBy following these steps, we have successfully saved our best performing model to a file and then loaded it back into memory. This allows us to use the model for making predictions or further training in the future.\\n\\nIf you have any questions or need further assistance, feel free to ask!***the transformed image so we're going to create a new variable called transformed and we're going to pass in our data transform and then we're going to pass in f so we're going to transform our original image and then we're going to plot it so we're going to go ax 1 dot m show and we're going to pass in the transformed image and then we're going to set the title so set title and we're going to set this to be the transformed image and then we're going to turn off the axes for the second plot so we're going to go axis and we're going to set that to false and then we're going to show the plot so plt dot show and then we're going to return the random image paths so that's our function there so let's see if this works so let's test it out so we're going to go visualize transformed images and we're going to pass in our image path list and we're going to pass in our data transform and we're going to pass in the number of images to transform so let's say we want to transform three images at a time and then we're going to set the random seed so we're going to set the seed to 42 and let's see what happens so we've got a pizza image here and then we've got the transformed image so we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image so we've got a way to visualize what our transformed images look like and this is a very important step when working with any kind of data set is to visualize what your transformed data looks like so that you can see if it's going to be suitable for your model and so that's how we can visualize our transformed images and in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video***# TITLE: \\n- Custom Data Loading: Replicating Image Folder Functionality\\n\\n## ABSTRACT: \\n- The document outlines the process of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It demonstrates the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set.\\n\\n## KEY POINTS:\\n- Subclassing torch.utils.data.Dataset to create a custom data set.\\n- Initializing the custom data set with a target directory and a transform.\\n- Creating attributes such as paths, transform, classes, and class to idx.\\n- Writing a function to load images and overriding the len and get item methods.\\n- Creating a function to display random images from the custom data set.\\n\\n## CONTEXT \\n- The document provides a detailed walkthrough of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. \\n- The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader.\\n\\n## REFLECTIONS\\n- How can the custom data set be further extended to handle additional data formats or preprocessing steps?\\n- What are the potential challenges of creating a custom data set, and how can they be addressed?\\n- The importance of visualizing data to ensure the correctness of the custom data loading process.\\n\\nRecap: The document provides a comprehensive guide to creating a custom data set in PyTorch, replicating the functionality of the original image folder data loader class. It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader. The importance of visualizing data to ensure the correctness of the custom data loading process is emphasized.***we don't use random seeds in practice but for educational purposes, we are using them to exemplify how we can get similar numbers on our page. However, ideally, regardless of the random seed used, our model's performance should be similar. \\n\\nWe are going to train for five epochs and create an instance of the `tiny vgg` model. The input shape for the model is three, as we are dealing with color images. The `hidden_units` is set to 10, in line with the CNN explainer website, and the `output_shape` is the number of classes in our training data set. \\n\\nFor the loss function, we are using the `nn.CrossEntropyLoss` as we are dealing with multi-class classification. As for the optimizer, we are trying the `Adam` optimizer with a learning rate of 0.001. The default learning rate for Adam is 1e-3.\\n\\nNow, let's move on to training and evaluating the model using the functions we have created in the previous sections.***we can make a prediction on our custom image. Let's see if we can get our model to predict on this image in the next video. I'll see you there!***# TITLE: \\n- Predicting on Custom Data with PyTorch: Overcoming Challenges and Building a Function\\n\\n## ABSTRACT: \\n- The document covers the process of making predictions on custom images using PyTorch, highlighting the challenges and steps involved in preparing the custom data for model prediction. It emphasizes the critical points of ensuring the data type, shape, and device compatibility with the model. The document also introduces the concept of functionizing the prediction process for ease of use and scalability.\\n\\n## KEY POINTS:\\n- Preparing Custom Data:\\n    - Ensuring the custom image is in the same data type, shape, and device as the model.\\n    - Adding a batch dimension to the image to align with the model's expectations.\\n- PyTorch Built-in Functions:\\n    - Exploring PyTorch's built-in functions for handling various data types, including images, text, and audio.\\n    - Custom data set classes can be written by subclassing torch.utils.data.dataset for specific requirements.\\n- Balancing Overfitting and Underfitting:\\n    - Understanding the balance between overfitting and underfitting in machine learning models.\\n- Predicting on Custom Data:\\n    - Highlighting the importance of addressing wrong data types, shapes, and devices when making predictions on custom data.\\n- Exercises and Extra Curriculum:\\n    - Encouraging practice through exercises and providing resources for further learning and exploration.\\n\\n## CONTEXT\\n- The document provides a detailed walkthrough of the process of preparing and making predictions on custom images using PyTorch. It covers the challenges faced, such as ensuring data compatibility with the model, and introduces the concept of functionizing the prediction process for scalability. The document emphasizes the importance of aligning the custom data with the model's expectations in terms of data type, shape, and device. It also encourages further learning through exercises and extra curriculum resources.\\n\\n- The process involves loading the custom image, transforming it to match the model's requirements, ensuring it is on the correct device, and adding a batch dimension. The document also discusses the balance between overfitting and underfitting in machine learning models and provides insights into PyTorch's built-in functions for handling diverse data types.\\n\\n- The critical takeaway is the significance of addressing wrong data types, shapes, and devices when making predictions on custom data. The document also encourages practical application through exercises and provides additional resources for further learning.\\n\\n## REFLECTIONS\\n- How can the challenges faced in preparing custom data for model prediction be mitigated effectively?\\n- What are the potential implications of incorrect data types, shapes, and devices when making predictions on custom data?\\n- The importance of functionizing the prediction process for scalability and ease of use is evident. How can this approach be further optimized for real-world applications?\\n\\n- The document provides a comprehensive understanding of the complexities involved in preparing and making predictions on custom data using PyTorch. It highlights the critical considerations and challenges, emphasizing the need for meticulous data preparation and alignment with the model's expectations. The concept of functionizing the prediction process adds a layer of scalability and practicality to the workflow, paving the way for further exploration and application in real-world scenarios.***\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(combined_note, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"Document: {combined_note}\"\"\"\n",
    "prompt = [\n",
    "        {'role': 'system', 'content': system_message_c},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "response = get_completion(prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Combined Note:\\n\\n## on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.\\n\\n## the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result\\n\\n## some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\n## In addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\n## Remember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!\\n\\n## torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.\\n\\n## # TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.\\n\\n## model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith Setup [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langsmith does not work - put in placeholder - need to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv('LANGSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__47bc0afe60cd4471a05f4f1578aac790\"\n",
    "!export LANGCHAIN_PROJECT=\"pt-large-date-95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading up v0.1.0 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Libraries to be imported\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\")\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ff260156d40>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in the following ways:\\n\\n1. Automated Testing: Langsmith can generate a large number of test cases with different inputs and expected outputs, allowing for comprehensive testing of software applications. This helps in identifying potential bugs, errors, and vulnerabilities.\\n\\n2. Test Coverage: Langsmith can analyze the codebase and provide insights on the areas that have been tested and those that have not. This helps in ensuring that all parts of the code are adequately tested, reducing the risk of undiscovered issues.\\n\\n3. Code Quality Analysis: Langsmith can analyze the source code and provide feedback on potential code smells, complexity, and maintainability issues. This helps in improving the overall quality of the code and making it more testable.\\n\\n4. Performance Testing: Langsmith can simulate different load conditions and stress test the software, identifying performance bottlenecks and suggesting improvements. This helps in ensuring that the application can handle real-world usage scenarios efficiently.\\n\\n5. Continuous Integration and Delivery: Langsmith can integrate with CI/CD pipelines to automatically run tests whenever there is a code change. This ensures that new changes do not introduce regressions and helps in maintaining a high level of software quality.\\n\\n6. Security Testing: Langsmith can analyze the codebase for potential security vulnerabilities and provide recommendations to mitigate them. This helps in identifying and addressing security risks early in the development process.\\n\\nOverall, Langsmith can significantly improve the testing process by automating repetitive tasks, providing code analysis insights, and ensuring code quality and performance.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame the LLM object\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"how can langsmith help with testing?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        Follow the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        Follow the guidelines provided in the style for reference on how to summarize the note.\n",
    "\n",
    "        <style>\n",
    "        {style}\n",
    "        </style>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"user\", \n",
    "        \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<style>\n",
      "        \n",
      "# TITLE: ChatPromptTemplate\n",
      "\n",
      "## ABSTRACT: \n",
      "- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\n",
      "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "\n",
      "## KEY POINTS:\n",
      "- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
      "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "\n",
      "## CONTEXT: \n",
      "- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
      "- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "- It accepts a dictionary as input and returns a ChatPromptValue.\n",
      "- The ChatPromptValue can be converted to a string or a list of chat messages.\n",
      "- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\n",
      "- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\n",
      "\n",
      "## REFLECTIONS:\n",
      "- How does the ChatPromptTemplate differ from the PromptTemplate?\n",
      "- What other methods or functionality does the ChatPromptTemplate provide?\n",
      "- How can the ChatPromptTemplate be used in different language models?\n",
      "- Are there any limitations or constraints when using the ChatPromptTemplate?\n",
      "- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\n",
      "- What are some real-world examples of using the ChatPromptTemplate in language models?\n",
      "- How can the ChatPromptTemplate be customized or extended for specific use cases?\n",
      "- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\n",
      "- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\n",
      "- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\n",
      "- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\n",
      "- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"ChatPromptTemplate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<style>\\n        \\n# TITLE: ChatPromptTemplate\\n\\n## ABSTRACT: \\n- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\\n- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\\n- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\\n\\n## KEY POINTS:\\n- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\\n- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\\n- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\\n\\n## CONTEXT: \\n- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\\n- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\\n- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\\n- It accepts a dictionary as input and returns a ChatPromptValue.\\n- The ChatPromptValue can be converted to a string or a list of chat messages.\\n- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\\n- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\\n\\n## REFLECTIONS:\\n- How does the ChatPromptTemplate differ from the PromptTemplate?\\n- What other methods or functionality does the ChatPromptTemplate provide?\\n- How can the ChatPromptTemplate be used in different language models?\\n- Are there any limitations or constraints when using the ChatPromptTemplate?\\n- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\\n- What are some real-world examples of using the ChatPromptTemplate in language models?\\n- How can the ChatPromptTemplate be customized or extended for specific use cases?\\n- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\\n- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\\n- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\\n- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\\n- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response answer: <style>\n",
      "        \n",
      "# TITLE: ChatPromptTemplate\n",
      "\n",
      "## ABSTRACT: \n",
      "- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\n",
      "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "\n",
      "## KEY POINTS:\n",
      "- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
      "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "\n",
      "## CONTEXT: \n",
      "- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
      "- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
      "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
      "- It accepts a dictionary as input and returns a ChatPromptValue.\n",
      "- The ChatPromptValue can be converted to a string or a list of chat messages.\n",
      "- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\n",
      "- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\n",
      "\n",
      "## REFLECTIONS:\n",
      "- How does the ChatPromptTemplate differ from the PromptTemplate?\n",
      "- What other methods or functionality does the ChatPromptTemplate provide?\n",
      "- How can the ChatPromptTemplate be used in different language models?\n",
      "- Are there any limitations or constraints when using the ChatPromptTemplate?\n",
      "- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\n",
      "- What are some real-world examples of using the ChatPromptTemplate in language models?\n",
      "- How can the ChatPromptTemplate be customized or extended for specific use cases?\n",
      "- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\n",
      "- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\n",
      "- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\n",
      "- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\n",
      "- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<style>\n",
       "        \n",
       "# TITLE: ChatPromptTemplate\n",
       "\n",
       "## ABSTRACT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## KEY POINTS:\n",
       "- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## CONTEXT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "- It accepts a dictionary as input and returns a ChatPromptValue.\n",
       "- The ChatPromptValue can be converted to a string or a list of chat messages.\n",
       "- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\n",
       "- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the ChatPromptTemplate differ from the PromptTemplate?\n",
       "- What other methods or functionality does the ChatPromptTemplate provide?\n",
       "- How can the ChatPromptTemplate be used in different language models?\n",
       "- Are there any limitations or constraints when using the ChatPromptTemplate?\n",
       "- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\n",
       "- What are some real-world examples of using the ChatPromptTemplate in language models?\n",
       "- How can the ChatPromptTemplate be customized or extended for specific use cases?\n",
       "- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\n",
       "- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\n",
       "- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\n",
       "- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\n",
       "- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(f\"Response answer: {response['answer']}\")  # This line will print the value of response['answer']\n",
    "\n",
    "display(Markdown(f\"{response['answer']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/dpvj/git/assistant/output.md', 'w') as f:\n",
    "    f.write(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<style>\n",
       "        \n",
       "# TITLE: ChatPromptTemplate\n",
       "\n",
       "## ABSTRACT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## KEY POINTS:\n",
       "- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## CONTEXT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "- It accepts a dictionary as input and returns a ChatPromptValue.\n",
       "- The ChatPromptValue can be converted to a string or a list of chat messages.\n",
       "- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\n",
       "- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the ChatPromptTemplate differ from the PromptTemplate?\n",
       "- What other methods or functionality does the ChatPromptTemplate provide?\n",
       "- How can the ChatPromptTemplate be used in different language models?\n",
       "- Are there any limitations or constraints when using the ChatPromptTemplate?\n",
       "- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\n",
       "- What are some real-world examples of using the ChatPromptTemplate in language models?\n",
       "- How can the ChatPromptTemplate be customized or extended for specific use cases?\n",
       "- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\n",
       "- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\n",
       "- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\n",
       "- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\n",
       "- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"{response['answer']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<style>\n",
       "        \n",
       "# TITLE: ChatPromptTemplate\n",
       "\n",
       "## ABSTRACT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface and is a basic building block of the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## KEY POINTS:\n",
       "- ChatPromptTemplate implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "\n",
       "## CONTEXT: \n",
       "- ChatPromptTemplate is a class that implements the Runnable interface in the LangChain Expression Language (LCEL).\n",
       "- It is a basic building block of the LCEL and supports various calls such as invoke, ainvoke, stream, astream, batch, abatch, and astream_log.\n",
       "- ChatPromptTemplate is used to create templates for generating chat prompts for language models.\n",
       "- It accepts a dictionary as input and returns a ChatPromptValue.\n",
       "- The ChatPromptValue can be converted to a string or a list of chat messages.\n",
       "- The ChatPromptTemplate is flexible and allows for different representations of chat messages, such as using tuples or instances of MessagePromptTemplate or BaseMessage.\n",
       "- The ChatPromptTemplate provides a lot of flexibility in constructing chat prompts for language models.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the ChatPromptTemplate differ from the PromptTemplate?\n",
       "- What other methods or functionality does the ChatPromptTemplate provide?\n",
       "- How can the ChatPromptTemplate be used in different language models?\n",
       "- Are there any limitations or constraints when using the ChatPromptTemplate?\n",
       "- How does the ChatPromptTemplate contribute to the overall functionality of the LangChain Expression Language (LCEL)?\n",
       "- What are some real-world examples of using the ChatPromptTemplate in language models?\n",
       "- How can the ChatPromptTemplate be customized or extended for specific use cases?\n",
       "- What are the advantages of using the ChatPromptTemplate over other methods of generating chat prompts?\n",
       "- Are there any best practices or recommended approaches for using the ChatPromptTemplate effectively?\n",
       "- How does the ChatPromptTemplate handle errors or exceptions in the prompt generation process?\n",
       "- What are some potential future developments or enhancements for the ChatPromptTemplate in the LangChain Expression Language (LCEL)?\n",
       "- Overall, the ChatPromptTemplate is a powerful tool for generating chat prompts in language models and provides flexibility and customization options for various use cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown(f\"{response['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing by providing a comprehensive and efficient testing framework. Here are some ways in which Langsmith can help with testing:\\n\\n1. Test Automation: Langsmith enables the automation of tests by providing a testing framework that allows developers to write tests in a clear and concise manner. This framework supports various testing methodologies, such as unit testing, integration testing, and end-to-end testing.\\n\\n2. Test Coverage: Langsmith helps in measuring the coverage of tests by providing tools and features to track which parts of the codebase have been tested. This ensures that all critical areas of the application are thoroughly tested, reducing the chances of undiscovered bugs.\\n\\n3. Test Orchestration: Langsmith allows developers to easily manage and run tests across different environments and configurations. It provides a way to define test suites, group related tests, and execute them in parallel or sequentially, improving the testing process's efficiency.\\n\\n4. Test Reporting: Langsmith generates detailed and comprehensive test reports, highlighting the test results, including pass/fail status, execution time, and any errors or exceptions encountered. These reports help in identifying and debugging issues quickly.\\n\\n5. Mocking and Stubbing: Langsmith provides mechanisms for creating mock objects or stubs, allowing developers to isolate components for testing. This is particularly useful when testing code that relies on external dependencies, as it enables developers to simulate their behavior, leading to more reliable and independent tests.\\n\\n6. Performance Testing: Langsmith offers tools and utilities for performance testing, enabling developers to evaluate the system's performance under different load conditions. This helps identify potential bottlenecks and optimize the application for better scalability and responsiveness.\\n\\n7. Continuous Integration/Continuous Delivery (CI/CD) Integration: Langsmith seamlessly integrates with CI/CD pipelines, allowing tests to be automatically triggered on code changes or deployments. This ensures that tests are run consistently and continuously, providing immediate feedback on the code's quality.\\n\\nOverall, Langsmith enhances the testing process by providing a robust framework, tools, and utilities that facilitate test automation, coverage analysis, test orchestration, reporting, mocking, and performance testing. These capabilities help ensure the reliability, stability, and quality of the software being developed.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can greatly assist with testing in several ways:\\n\\n1. Test Case Generation: Langsmith can automatically generate test cases based on the specifications and requirements of your software. It uses natural language processing techniques to understand the desired behavior of the system and generate test cases that cover various scenarios and edge cases. This can save significant time and effort in test case creation.\\n\\n2. Test Data Generation: Langsmith can also generate realistic and diverse test data for your software. It understands the data requirements and constraints of your system and creates test data that covers different data types, ranges, and combinations. This can help in ensuring thorough testing and identifying potential issues related to data handling.\\n\\n3. Test Automation: Langsmith can generate test scripts or code snippets in various programming languages to automate the execution of test cases. It can integrate with popular testing frameworks and tools to streamline the automation process. This can greatly accelerate the testing process and improve test coverage.\\n\\n4. Test Documentation: Langsmith can assist in creating comprehensive test documentation. It can generate test plans, test scripts, test reports, and other relevant documentation based on the identified test cases. This ensures that the testing process is well-documented and easily understandable for all stakeholders.\\n\\n5. Test Oracles: Langsmith can help in defining oracles for automated testing. An oracle is a mechanism to determine whether the actual behavior of the system matches the expected behavior. Langsmith can understand the expected outcomes of various functionalities and generate oracles to validate the test results automatically.\\n\\nOverall, Langsmith can enhance the efficiency and effectiveness of testing by automating test case generation, test data generation, test script creation, and test documentation. It can also improve the quality of testing by ensuring comprehensive coverage and accurate oracles.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f8e84927340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8e6bf61d80>, model_name='gpt-3.5-turbo-1106', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \\n\\ntext: ```{text}```\\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"You are a study note taking assistant for courses.\n",
    "\n",
    "Given the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \n",
    "\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_style = \"\"\"\n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_text = document.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ntext: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\\n\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studynote_messages = prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)\n",
    "studynote_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_response = chat(studynote_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ABSTRACT:\n",
       "The document discusses the Lang chain expression language, which allows for writing minimalist code to build chains within line chain. It emphasizes the advanced features of parallel execution, async, and streaming using the expression language. The document also explores the syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## KEY POINTS:\n",
       "- Lang chain expression language for building minimalist code to create chains within line chain.\n",
       "- Advanced features include parallel execution, async, and streaming.\n",
       "- Syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## CONTEXT:\n",
       "- The Lang chain expression language allows for writing minimalist code to build chains within line chain, making it easier to use advanced features like parallel execution, async, and streaming.\n",
       "- The pipe operator is used to string components together, making the code simpler and more flexible.\n",
       "- The runnable lambdas are used to wrap functions and create custom operations within the expression language.\n",
       "- The document provides examples of using the expression language to retrieve information in parallel and modify the output using runnable lambdas.\n",
       "- The expression language has pros such as minimalist code and advanced features, but also cons like increased abstraction and non-standard syntax.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the Lang chain expression language compare to other methods of building chains within line chain?\n",
       "- What are the potential use cases for the runnable lambdas within the expression language?\n",
       "- The expression language offers a minimalist approach and advanced features, but also introduces increased abstraction and non-standard syntax."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(studynote_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_schema = ResponseSchema(name=\"python_code\",\n",
    "                            description=\"Parse any code within the text and write the code in string with backticks. \\\n",
    "                                If there was no code found, then output as -1.\")\n",
    "webpage_schema = ResponseSchema(name=\"webpage_link\",\n",
    "                                    description=\"Was there any webpage links recommended. \\\n",
    "                                    If this information is not found, output -1.\")\n",
    "reading_schema = ResponseSchema(name=\"further_reading\",\n",
    "                                    description=\"Extract any recommendations on further research or reading on the subject. \\\n",
    "                                    Output them as a comma separated Python list. If none is recommended, output -1.\")\n",
    "\n",
    "response_schemas = [python_schema, \n",
    "                    webpage_schema,\n",
    "                    reading_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\\n\\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\\n\\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\\n}\\n```'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
      "\n",
      "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
      "\n",
      "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "\n",
      "text: today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\n",
      "\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\n",
      "\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studynote_text = document.page_content\n",
    "\n",
    "template_string2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
    "\n",
    "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
    "\n",
    "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "dict_prompt = ChatPromptTemplate.from_template(template=template_string2)\n",
    "\n",
    "dict_messages = dict_prompt.format_messages(text=studynote_text, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "print(dict_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_response = chat(messages)\n",
    "\n",
    "dict_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_code': -1,\n",
       " 'webpage_link': -1,\n",
       " 'further_reading': 'Lang chain expression language, Line chain products, Line chain abstraction'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "#extract dict key gift\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# Vector Data Memory\n",
    "# Entity Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": studynote_messages}, {\"output\": studynote_response})\n",
    "memory.save_context({\"input\": dict_messages}, {\"output\": dict_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\\nAI: ```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\n",
      "AI: ```json\n",
      "{\n",
      "\t\"python_code\": -1,\n",
      "\t\"webpage_link\": -1,\n",
      "\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\n",
      "}\n",
      "```\n",
      "Human: Hi, what is langchain about?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lang chain is a powerful expression language that allows for running retrievers in parallel and modifying the output using runnable lambdas. It has a specific syntax and is worth learning and experimenting with. It is used for line chain products and line chain abstraction.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is langchain about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)  \n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lang Chain Expression Language company'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = studynote_messages\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 1: input= text and output= summary\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Come up with a summary for the following text:\"\n",
    "    \"\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= search queries\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate some additional web search queries based on the summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"search_queries\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= topic\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a one line topic based summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, \n",
    "                     output_key=\"topic\"\n",
    "                    )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"search_queries\",\"topic\"],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\",\n",
       " 'summary': 'The text discusses the Lang chain expression language, which allows for minimalist code to build chains within Lang chain. It explores the advanced features of parallel execution, async, and streaming using the expression language. The text explains the syntax and shows examples of how to use the expression language with a practical use case. It also covers the pros and cons of using the expression language and provides a detailed explanation of how it works. Overall, the text provides a comprehensive overview of the Lang chain expression language and its potential benefits and drawbacks.',\n",
       " 'search_queries': '1. How to use Lang chain expression language for parallel execution?\\n2. What are the practical use cases for Lang chain expression language?\\n3. Pros and cons of using Lang chain expression language for async execution\\n4. What are the advanced features of Lang chain expression language?\\n5. Examples of minimalist code using Lang chain expression language\\n6. How does streaming work in Lang chain expression language?\\n7. Comparison of Lang chain expression language with other expression languages\\n8. Is Lang chain expression language suitable for large-scale applications?\\n9. Tips for optimizing code using Lang chain expression language\\n10. How to get started with Lang chain expression language programming?',\n",
       " 'topic': 'An in-depth exploration of the Lang chain expression language and its advanced features, syntax, practical use cases, and pros and cons.'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = document.page_content\n",
    "overall_chain(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over a larger document - Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working right now - need to revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/home/dpvj/SecondBrain/templates/Meeting Note Template.md\")\n",
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/pydantic/_migration.py:276: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wnat are the key points of the document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- [ ]  Follow-up meetings scheduled\\n- [ ]  Next steps and responsibilities assigned\\n\\n## Decisions\\n_List any decisions made during the meeting_\\n\\n## Next Steps\\n_Outline the next steps and responsibilities_\\n\\n## Follow-up\\n_Summarize any follow-up actions required_\\n\\n## Additional Notes\\n_Any additional notes or information related to the meeting_\\n\\n## Meeting Adjourned\\n_Time the meeting was adjourned_\\n\\n## Next Meeting\\n_Date, time, and agenda for the next meeting_'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.call_as_llm(f\"{qdocs} Question: Please list all the headers of the documents\") \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Multifunction Notetaking Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_format = studynote_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetingnote_format = \"\"\"\n",
    "## Agenda\n",
    "_Summarize the agenda of the meeting_\n",
    "_Ensure key stakeholders are participating & Leading_\n",
    "\n",
    "## Goals\n",
    "_What do we want to achieve from this meeting_\n",
    "_Align with why the meeting was called in the first place_\n",
    "\n",
    "## Discussion notes\n",
    "_Write the notes that are key to the goals & objectives, note who has said it_\n",
    "\n",
    "## Action items\n",
    "_Summarize the action items_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"studynote\", \n",
    "        \"description\": \"Good for taking notes for a course\", \n",
    "        \"prompt_template\": studynote_template,\n",
    "        \"prompt_style\": studynote_format\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meetingnote\",  # Changed from \"math\" to \"meetingnote\"\n",
    "        \"description\": \"Good for taking notes for a meeting\", \n",
    "        \"prompt_template\": meetingnote_template,\n",
    "        \"prompt_style\": meetingnote_format\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\nYou are expected to take notees for a course.\\n\\nGiven the course delimeted by tripple backticks, extract information into a study note following the format: \\n\\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ncourse: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\")]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains['studynote'].prompt.format_messages(format=studynote_style, text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:97\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:511\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    512\u001b[0m         _output_key\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    517\u001b[0m         _output_key\n\u001b[1;32m    518\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:92\u001b[0m, in \u001b[0;36mMultiRouteChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     91\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m---> 92\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouter_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mdestination) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mnext_inputs), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m route\u001b[38;5;241m.\u001b[39mdestination:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:40\u001b[0m, in \u001b[0;36mRouterChain.route\u001b[0;34m(self, inputs, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Route:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Route inputs to a destination chain.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        a Route object\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Route(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:61\u001b[0m, in \u001b[0;36mLLMRouterChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     57\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     58\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m     60\u001b[0m     Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:114\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m raised following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrap_text(url: str):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>begin this chain now I'm going to</td>\n",
       "      <td>612.959</td>\n",
       "      <td>6.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>implement it with this we will see that</td>\n",
       "      <td>615.880</td>\n",
       "      <td>6.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>line chain actually uses I think they</td>\n",
       "      <td>619.720</td>\n",
       "      <td>3.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>use</td>\n",
       "      <td>622.160</td>\n",
       "      <td>3.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>invoke so rather than call they would</td>\n",
       "      <td>623.360</td>\n",
       "      <td>5.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>what we we had earlier so yeah we have</td>\n",
       "      <td>1212.159</td>\n",
       "      <td>5.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>these two functions let's take those</td>\n",
       "      <td>1214.400</td>\n",
       "      <td>4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>okay we can see runable it's what we</td>\n",
       "      <td>1217.320</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>were doing before so that we could use</td>\n",
       "      <td>1219.240</td>\n",
       "      <td>5.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>this let's do it again</td>\n",
       "      <td>1221.280</td>\n",
       "      <td>5.879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text     start  duration\n",
       "226        begin this chain now I'm going to   612.959     6.761\n",
       "227  implement it with this we will see that   615.880     6.280\n",
       "228    line chain actually uses I think they   619.720     3.640\n",
       "229                                      use   622.160     3.480\n",
       "230    invoke so rather than call they would   623.360     5.919\n",
       "..                                       ...       ...       ...\n",
       "449   what we we had earlier so yeah we have  1212.159     5.161\n",
       "450     these two functions let's take those  1214.400     4.840\n",
       "451     okay we can see runable it's what we  1217.320     3.960\n",
       "452   were doing before so that we could use  1219.240     5.319\n",
       "453                   this let's do it again  1221.280     5.879\n",
       "\n",
       "[228 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = YouTubeTranscriptApi.get_transcripts([video_id], languages=['en'])\n",
    "ls = list(transcripts[0].values())[0]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(ls)\n",
    "\n",
    "filtered_df = df[(df['start'] >= 611) & (df['start'] <= 1224)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\n",
      "\n",
      "Key Links:\n",
      "Code from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
      "LangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\n",
      "GPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "soup = BeautifulSoup(requests.get('https://www.youtube.com/watch?v=DjuXACWYkkU').content)\n",
    "pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'AIzaSyDYyXnayylCG2L1ToqrZykiVA--QxZ7-3Y'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=video_id\n",
    ")\n",
    "response = request.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#videoListResponse',\n",
       " 'etag': 'VB1pOZ9dsyaJCmlUHZOx-KOEwbk',\n",
       " 'items': [{'kind': 'youtube#video',\n",
       "   'etag': 'mFwyljJthlBCHJcUCN2cbl3du5U',\n",
       "   'id': 'DjuXACWYkkU',\n",
       "   'snippet': {'publishedAt': '2023-11-16T14:35:01Z',\n",
       "    'channelId': 'UCC-lyoTfSrcJzA1ab3APAgw',\n",
       "    'title': 'Building a Research Assistant from Scratch',\n",
       "    'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher',\n",
       "    'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/default.jpg',\n",
       "      'width': 120,\n",
       "      'height': 90},\n",
       "     'medium': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/mqdefault.jpg',\n",
       "      'width': 320,\n",
       "      'height': 180},\n",
       "     'high': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/hqdefault.jpg',\n",
       "      'width': 480,\n",
       "      'height': 360},\n",
       "     'standard': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/sddefault.jpg',\n",
       "      'width': 640,\n",
       "      'height': 480},\n",
       "     'maxres': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/maxresdefault.jpg',\n",
       "      'width': 1280,\n",
       "      'height': 720}},\n",
       "    'channelTitle': 'LangChain',\n",
       "    'categoryId': '22',\n",
       "    'liveBroadcastContent': 'none',\n",
       "    'localized': {'title': 'Building a Research Assistant from Scratch',\n",
       "     'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher'}},\n",
       "   'contentDetails': {'duration': 'PT43M40S',\n",
       "    'dimension': '2d',\n",
       "    'definition': 'hd',\n",
       "    'caption': 'false',\n",
       "    'licensedContent': False,\n",
       "    'contentRating': {},\n",
       "    'projection': 'rectangular'},\n",
       "   'statistics': {'viewCount': '9045',\n",
       "    'likeCount': '307',\n",
       "    'favoriteCount': '0',\n",
       "    'commentCount': '27'}}],\n",
       " 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = response['items'][0]['snippet']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_date = response['items'][0]['snippet']['publishedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = response['items'][0]['statistics']['viewCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a note taking assistant for a courses. \\n    Given the following document, write key points.\\n    If the document is not relevant, write \"not relevant\".\\n    '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8TClnBZvafYSdsb9WiFpkNfbp1GOt', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1701971291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
