{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base OAI Libraries & Environment Setup\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "# For the Youtube Trancript Download\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import html2text\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text, model=None):\n",
    "    if model == 'gpt-4':\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_youtube(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Convert to text\n",
    "        transcript = ' '.join([t['text'] for t in transcript])\n",
    "\n",
    "        # Create document\n",
    "        document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "\n",
    "        return document\n",
    "    except:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Setup - gpt-3.5-turbo-1106 is a chat model\n",
    "client = OpenAI()\n",
    "model=\"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3271"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id = 'mmBo8nlu2j0' # Auto-Prompt Builder (with Hosted LangServe) smaller number of tokens 23 mins\n",
    "transcript=read_youtube(video_id).page_content\n",
    "get_num_tokens(transcript, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the splitting of the transcripts by chunks according to tokens\n",
    "It seems to be better to split by tokens - where we are getting exact as opposed to the Characters (via RecursiveCharacterTextSplitter) because we are achieving more accurate results based on input limits - which is our primary concern. By defining encoding name and model_name, we can get exact num_tokens as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "# chunks = splitter.split_documents([document])\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=15000, chunk_overlap=100, encoding_name=\"cl100k_base\", model_name=model)\n",
    "\n",
    "texts = text_splitter.split_text(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(texts[0], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_i = \"\"\"\n",
    "    You are a note taking assistant for a courses. \n",
    "    \n",
    "    Given the Document, write a note based on the following format and instructions defined in point format below:\n",
    "    \n",
    "    # TITLE: \n",
    "    - One line catchy title to capture the essence of the document.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_outputs = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    user_message = f\"\"\"Document: {texts[i]}\"\"\"\n",
    "    prompt = [\n",
    "            {'role': 'system', 'content': system_message_i},\n",
    "            {'role': 'user', 'content': user_message}\n",
    "        ]\n",
    "\n",
    "    response = get_completion(prompt, model=model)\n",
    "    \n",
    "    list_of_outputs.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_c = \"\"\"\n",
    "    You are a note taking assistant for a courses who summarizes several notes. Each note is separated by \"***\" sign\n",
    "    \n",
    "    Given these combined notes, compile a new note with the same headers but combining the points under each header. \n",
    "    Make sure there is no duplication of points.    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note = \"***\"\n",
    "\n",
    "for i in range(len(list_of_outputs)):\n",
    "    combined_note += list_of_outputs[i] + \"***\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*** on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.***the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result***some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\nIn addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\nRemember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!***torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.***# TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.***model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check***we've replicated the same model using nn.sequential which is a simpler way to define a neural network in PyTorch. We've used nn.sequential to define the layers of our model and set the in and out features accordingly. This allows us to create a neural network with the same architecture as before, but with less code.\\n\\nIn the next video, we'll move on to defining a loss function and an optimizer for our model. We'll also explore how to train and evaluate our model using the training and test data sets we created earlier.***hyperparameters that we've changed so far and then we're going to change the number of epochs so we're going to go from 100 to a thousand so we're going to train for longer so let's write some code to do that so we're going to create a new model so we're going to call this circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1***of combining linear and non-linear functions remains the same. So let's write some code to replicate these non-linear activation functions. We'll start by creating a simple non-linear function using Python. We'll create a function that applies the rectified linear unit (ReLU) function to an input. The ReLU function returns the maximum of 0 or the input value. Here's how we can write this function:\\n\\n```python\\ndef relu(x):\\n    return max(0, x)\\n```\\n\\nThis simple function demonstrates the essence of the ReLU activation function. It takes an input and returns the maximum of 0 or the input value. This is a basic representation of a non-linear activation function.\\n\\nNext, let's replicate the sigmoid function. The sigmoid function is another common non-linear activation function used in neural networks. It takes an input and returns a value between 0 and 1, which is often interpreted as a probability. Here's how we can write this function:\\n\\n```python\\nimport math\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\n```\\n\\nThese simple functions demonstrate the basic principles of non-linear activation functions. They take an input and apply a non-linear transformation to produce an output.\\n\\nFinally, let's replicate the ReLU and sigmoid functions using PyTorch. PyTorch provides built-in non-linear activation functions that we can use in our neural network models. Here's how we can use the ReLU and sigmoid functions in PyTorch:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\n# Using ReLU in PyTorch\\nrelu = nn.ReLU()\\n\\n# Using sigmoid in PyTorch\\nsigmoid = nn.Sigmoid()\\n```\\n\\nThese PyTorch functions provide the same non-linear transformations as the custom functions we wrote earlier. They can be easily integrated into our neural network models to introduce non-linearity.\\n\\nBy replicating these non-linear activation functions, we gain a deeper understanding of how they work and how they contribute to the power of neural networks in capturing complex patterns in data.\\n\\nIn conclusion, non-linear activation functions play a crucial role in enabling neural networks to model complex relationships in data. By replicating and understanding these functions, we can better appreciate their impact on the performance of neural network models.***solutions in the solutions folder but i'd highly recommend trying to do it on your own first and then checking the solutions if you get stuck and then if you'd like some extra curriculum i'd recommend checking out the torch metrics module for 10 minutes and then also the article beyond accuracy precision and recall by Will Koisrin for when to use precision and recall and then finally the classification report in scikit-learn so that's a fair bit of stuff to practice and i'd highly recommend going through it and then once you've done that you can come back and we can go through the solutions together and see how you went but i think that's a fair bit of stuff to practice and i'll leave that with you and i'll see you in the next video.***number 0 and we have 32 samples of 28 by 28 pixels and then we have a tensor of 32 labels so we have 32 labels for each of our 32 samples and then we can visualize a single image from the batch so we can go plt.imshow and we can pass in train features batch at index 0 so we're going to visualize the first image from the batch and then we can go plt.title and we can pass in train labels batch at index 0 so we're going to visualize the label of the first image from the batch and then we can go plt.show and we can see what this looks like so we've got a single image from the batch and we've got the label of that image and then we can do the same for the test data loader and visualize a single image from the test data loader and its label and that's how we can visualize a batch of images and their labels from the data loader. So we've covered a lot in this video. We've prepared our data into mini batches using the data loader and we've visualized a batch of images from the data loader. In the next video, we'll start building our convolutional neural network model to classify these images. I'll see you there!***'re right. We can create a function for our training loop and testing loop to make our code more modular and reusable. This will allow us to easily train and evaluate different models without having to rewrite the same code multiple times. Let's start by creating a function for the training loop.\\n\\n```python\\ndef train_model(model, train_loader, criterion, optimizer, device):\\n    model.train()  # Set the model to training mode\\n    train_loss = 0.0  # Initialize the training loss\\n\\n    for inputs, labels in train_loader:  # Loop through the training batches\\n        inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n        optimizer.zero_grad()  # Zero the gradients\\n        outputs = model(inputs)  # Forward pass\\n        loss = criterion(outputs, labels)  # Calculate the loss\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update the model parameters\\n\\n        train_loss += loss.item() * inputs.size(0)  # Accumulate the training loss\\n\\n    train_loss = train_loss / len(train_loader.dataset)  # Calculate the average training loss\\n\\n    return train_loss\\n```\\n\\nIn this function, we set the model to training mode, loop through the training batches, move the inputs and labels to the device, perform the forward pass, calculate the loss, perform backpropagation, update the model parameters, and accumulate the training loss. Finally, we calculate the average training loss and return it.\\n\\nNext, let's create a function for the testing loop.\\n\\n```python\\ndef test_model(model, test_loader, criterion, device):\\n    model.eval()  # Set the model to evaluation mode\\n    test_loss = 0.0  # Initialize the test loss\\n    correct = 0  # Initialize the number of correct predictions\\n\\n    with torch.no_grad():  # Disable gradient calculation\\n        for inputs, labels in test_loader:  # Loop through the testing batches\\n            inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n            outputs = model(inputs)  # Forward pass\\n            test_loss += criterion(outputs, labels).item() * inputs.size(0)  # Accumulate the test loss\\n            _, predicted = outputs.max(1)  # Get the predicted labels\\n            correct += predicted.eq(labels).sum().item()  # Count the number of correct predictions\\n\\n    test_loss = test_loss / len(test_loader.dataset)  # Calculate the average test loss\\n    test_accuracy = correct / len(test_loader.dataset)  # Calculate the test accuracy\\n\\n    return test_loss, test_accuracy\\n```\\n\\nIn this function, we set the model to evaluation mode, loop through the testing batches, move the inputs and labels to the device, perform the forward pass, accumulate the test loss, count the number of correct predictions, and calculate the average test loss and test accuracy. Finally, we return the test loss and test accuracy.\\n\\nThese functions will allow us to train and evaluate our models in a more organized and reusable manner. Now, we can use these functions to train and evaluate our model v1. Let's give it a go in the next video.***and so on and so forth so you can play around with these values and see how the output shape changes as you pass data through a max pool layer and a convolutional layer and you can also change the kernel size and see how that affects the output shape as well. This is a great way to understand how these layers manipulate the shape of the data and how they can be used to extract features from the input.\\n\\nIn the next video, we'll continue to explore the other layers of the convolutional neural network and see how they affect the data. Keep experimenting and have fun with it!***Now that we have created the directory to save our model, we can proceed with saving the model itself. We will use the `torch.save` function to save the model to a file. Let's continue with the code.\\n\\n```python\\n# Save the model\\ntorch.save(model2.state_dict(), model_path / 'fashion_mnist_model.pth')\\n```\\n\\nIn this code, we are using the `torch.save` function to save the state dictionary of our model to a file called 'fashion_mnist_model.pth' in the 'models' directory that we created earlier.\\n\\nNow that we have saved our model, we can proceed with loading it back into memory. Let's continue with the code.\\n\\n```python\\n# Load the model\\nmodel = YourModelClass(*args, **kwargs)  # Instantiate your model\\nmodel.load_state_dict(torch.load(model_path / 'fashion_mnist_model.pth'))\\nmodel.eval()  # Set the model to evaluation mode\\n```\\n\\nIn this code, we are instantiating a new instance of our model class and then loading the state dictionary from the saved file back into the model. We then set the model to evaluation mode using `model.eval()`.\\n\\nBy following these steps, we have successfully saved our best performing model to a file and then loaded it back into memory. This allows us to use the model for making predictions or further training in the future.\\n\\nIf you have any questions or need further assistance, feel free to ask!***the transformed image so we're going to create a new variable called transformed and we're going to pass in our data transform and then we're going to pass in f so we're going to transform our original image and then we're going to plot it so we're going to go ax 1 dot m show and we're going to pass in the transformed image and then we're going to set the title so set title and we're going to set this to be the transformed image and then we're going to turn off the axes for the second plot so we're going to go axis and we're going to set that to false and then we're going to show the plot so plt dot show and then we're going to return the random image paths so that's our function there so let's see if this works so let's test it out so we're going to go visualize transformed images and we're going to pass in our image path list and we're going to pass in our data transform and we're going to pass in the number of images to transform so let's say we want to transform three images at a time and then we're going to set the random seed so we're going to set the seed to 42 and let's see what happens so we've got a pizza image here and then we've got the transformed image so we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image so we've got a way to visualize what our transformed images look like and this is a very important step when working with any kind of data set is to visualize what your transformed data looks like so that you can see if it's going to be suitable for your model and so that's how we can visualize our transformed images and in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video***# TITLE: \\n- Custom Data Loading: Replicating Image Folder Functionality\\n\\n## ABSTRACT: \\n- The document outlines the process of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It demonstrates the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set.\\n\\n## KEY POINTS:\\n- Subclassing torch.utils.data.Dataset to create a custom data set.\\n- Initializing the custom data set with a target directory and a transform.\\n- Creating attributes such as paths, transform, classes, and class to idx.\\n- Writing a function to load images and overriding the len and get item methods.\\n- Creating a function to display random images from the custom data set.\\n\\n## CONTEXT \\n- The document provides a detailed walkthrough of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. \\n- The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader.\\n\\n## REFLECTIONS\\n- How can the custom data set be further extended to handle additional data formats or preprocessing steps?\\n- What are the potential challenges of creating a custom data set, and how can they be addressed?\\n- The importance of visualizing data to ensure the correctness of the custom data loading process.\\n\\nRecap: The document provides a comprehensive guide to creating a custom data set in PyTorch, replicating the functionality of the original image folder data loader class. It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader. The importance of visualizing data to ensure the correctness of the custom data loading process is emphasized.***we don't use random seeds in practice but for educational purposes, we are using them to exemplify how we can get similar numbers on our page. However, ideally, regardless of the random seed used, our model's performance should be similar. \\n\\nWe are going to train for five epochs and create an instance of the `tiny vgg` model. The input shape for the model is three, as we are dealing with color images. The `hidden_units` is set to 10, in line with the CNN explainer website, and the `output_shape` is the number of classes in our training data set. \\n\\nFor the loss function, we are using the `nn.CrossEntropyLoss` as we are dealing with multi-class classification. As for the optimizer, we are trying the `Adam` optimizer with a learning rate of 0.001. The default learning rate for Adam is 1e-3.\\n\\nNow, let's move on to training and evaluating the model using the functions we have created in the previous sections.***we can make a prediction on our custom image. Let's see if we can get our model to predict on this image in the next video. I'll see you there!***# TITLE: \\n- Predicting on Custom Data with PyTorch: Overcoming Challenges and Building a Function\\n\\n## ABSTRACT: \\n- The document covers the process of making predictions on custom images using PyTorch, highlighting the challenges and steps involved in preparing the custom data for model prediction. It emphasizes the critical points of ensuring the data type, shape, and device compatibility with the model. The document also introduces the concept of functionizing the prediction process for ease of use and scalability.\\n\\n## KEY POINTS:\\n- Preparing Custom Data:\\n    - Ensuring the custom image is in the same data type, shape, and device as the model.\\n    - Adding a batch dimension to the image to align with the model's expectations.\\n- PyTorch Built-in Functions:\\n    - Exploring PyTorch's built-in functions for handling various data types, including images, text, and audio.\\n    - Custom data set classes can be written by subclassing torch.utils.data.dataset for specific requirements.\\n- Balancing Overfitting and Underfitting:\\n    - Understanding the balance between overfitting and underfitting in machine learning models.\\n- Predicting on Custom Data:\\n    - Highlighting the importance of addressing wrong data types, shapes, and devices when making predictions on custom data.\\n- Exercises and Extra Curriculum:\\n    - Encouraging practice through exercises and providing resources for further learning and exploration.\\n\\n## CONTEXT\\n- The document provides a detailed walkthrough of the process of preparing and making predictions on custom images using PyTorch. It covers the challenges faced, such as ensuring data compatibility with the model, and introduces the concept of functionizing the prediction process for scalability. The document emphasizes the importance of aligning the custom data with the model's expectations in terms of data type, shape, and device. It also encourages further learning through exercises and extra curriculum resources.\\n\\n- The process involves loading the custom image, transforming it to match the model's requirements, ensuring it is on the correct device, and adding a batch dimension. The document also discusses the balance between overfitting and underfitting in machine learning models and provides insights into PyTorch's built-in functions for handling diverse data types.\\n\\n- The critical takeaway is the significance of addressing wrong data types, shapes, and devices when making predictions on custom data. The document also encourages practical application through exercises and provides additional resources for further learning.\\n\\n## REFLECTIONS\\n- How can the challenges faced in preparing custom data for model prediction be mitigated effectively?\\n- What are the potential implications of incorrect data types, shapes, and devices when making predictions on custom data?\\n- The importance of functionizing the prediction process for scalability and ease of use is evident. How can this approach be further optimized for real-world applications?\\n\\n- The document provides a comprehensive understanding of the complexities involved in preparing and making predictions on custom data using PyTorch. It highlights the critical considerations and challenges, emphasizing the need for meticulous data preparation and alignment with the model's expectations. The concept of functionizing the prediction process adds a layer of scalability and practicality to the workflow, paving the way for further exploration and application in real-world scenarios.***\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(combined_note, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"Document: {combined_note}\"\"\"\n",
    "prompt = [\n",
    "        {'role': 'system', 'content': system_message_c},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "response = get_completion(prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Combined Note:\\n\\n## on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.\\n\\n## the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result\\n\\n## some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\n## In addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\n## Remember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!\\n\\n## torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.\\n\\n## # TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.\\n\\n## model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith Setup [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langsmith does not work - put in placeholder - need to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv('LANGSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__47bc0afe60cd4471a05f4f1578aac790\"\n",
    "!export LANGCHAIN_PROJECT=\"pt-large-date-95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading up Langchain v0.1.0 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain Libraries\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Libraries to be imported\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Documents & Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_text(url: str):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quick Start |  Langchain Skip to main content  LangChain Docs Use cases Integrations Guides API More Versioning Changelog Developer's guide Templates Cookbooks Tutorials YouTube videos  LangS\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = scrap_text(\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\")\n",
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path and name\n",
    "file_path = \"/home/dpvj/git/assistant/studynote/transcript.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write the transcript content to the file\n",
    "    file.write(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(file_path)\n",
    "text_document = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "\n",
    "text_documents = text_splitter.split_documents(text_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"all right let's get started this is the opening eye prompt engineering page before we can start building with it though we need to set things some things up so I've created a f a fresh uh virtual environment what I'm going to do is I'm going to set up a l serve uh uh template a l or application with this the reason that I'm going to be using lay serve is it'll make it easy to deploy this once I finish creating so the first thing that I'm going to do is bootstrap a link serve project I'm going to do that with the Lang chain CLI I'm installing it here after it finishes installing I'm going to create a new app I will call it open AI prompter I am not going to add a package because I'm going to be creating my own I can then go inside it and if I open it up the main thing that I'm interested in is this Lang serve server right here which wraps around fast API and will make it really easy to deploy my Lang chain chain the last thing I'm going to do for setup is set up laying Smith so laying Smith is in uh private beta if you don't have access to it and you're seeing this and you want to follow along shoot me a DM on LinkedIn or Twitter and I will uh get you access to it um what it will make it really easy to do is debug as we go along and this will involve a lot of prompter engineering and so we'll see how that becomes very very helpful um someone in a previous video commented that I should go over how to set this up more so you can go to your projects let's even create a new project um let's call this uh open AI prompter um that's all we can do let's then go to setup and then we can just export some API Keys here um so you can just copy this you can paste it here you then need to add in your API key you can do that let me let me move my face over here and let's move this over here as well you can do that by going to API keys and all right good it's not showing mine so you can create an API key and then put it there um I've already did this ahead of time um so mine's all\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"already did this ahead of time um so mine's all set up but if you needed to do that this is how you do that all right so let's go back to this prompt engineering guide the first thing we're going to do we're just going to copy it all because we are going to be using this in our prompt to help us write better prompts let's paste it into a file and then I'm going to I'm going to create my chain in a Jupiter notebook and then I'm going to put it into uh and then I'm going to put it into this app and so the reason that I'm doing this there's uh there's there's one real reason and one fake reason the one real reason is that a lot this will it' be a pretty iterative process I'm going to do some prompt engineering some of that I'll do in Lang Smith but some of that I'll I'll do in uh The Notebook as well and we'll see why um and so having a notebook like environment is really really helpful because it's an iterative environment and I can really easy iterate the other reason I want to do this is I want to show how it's really easy to create a chain and then just export it from a notebook most of the time it's it's pretty simple so let's save this and now I've got this jup notebook let's load um what I just put in there all right so let's do that let's import uh we can print out the head I guess just to see it let's import some stuff from L chain that we're going to want so from L chain core. prompts import template from L core do output parsers string output parser from L chain Community do chat models import chat open AI so I'm importing three important things this is going pretty simple chain that I'm writing we'll see maybe we'll get more complex but it should just be pretty simple because I'm just going to use a model that has a long context window and can work with all these instructions so I'm importing a prompt template this will help me uh structure the inputs to the model I'm importing an output parser that'll basically just convert it from a message format which\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"just convert it from a message format which is how the newer models respond the chat messes respond with uh message but I I really want to set it's string so I'm getting that and then I'm importing chat opening app which is the L chain wrapper around the open AI models which are the ones that I'll be using now is the fun part where I'm going to write my template for creating or my template that's going to help me create a chain that can take in an objective and write a good prompt so let's add this there um and okay so so this is actually an interesting point that I'll get to later but basically what I'm doing right here is I'm going to have these instructions as a variable that I'll pass into the prompt and I'll I'll go into that more later but for now let's just assume that's what we're going to do and then let's add just some delimiters here and then let's say based on the above instructions help me write a good prompt I want a prompt that okay so what I actually want to write is not actually a prompt but I want to write a prompt template because I think most people write prompt templates because they want to write prompt templates that then they can use in their code or at least most linkchain users want to write prompt templates help me write a good prompt template this template should be a python s string it can take in any number of variables depending on my objective return your answer in the following format this is my objective this seems like a decent first pass I don't know we'll see um and then we're going to create the chain okay so as I mentioned we have this thing here um which I'm going to pass in the instructions to let's see what would happen if we didn't do that and if we just did like this which you could also easily do so if I did this I would have prompt equals prompt template from template template chain um equals prompt Shadow AI string up the parser chain. invoke and now I'm going to pass in objective what should my objective be um answer\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"objective what should my objective be um answer B answer a question based on a based on context provided and only on that context so this is a pretty typical rag prompt or retrieval augmented generation prompt you want the the language model to respond to a user question based on the context provided um so let's do this and I get ke because basically what's happening is in these instructions there's other um there's other curly brackets which aren't variables that I want to format they're just part of the text and so this often happens when you're working with code or something like that and we've got a a lot of questions about how to deal with it so my preferred way to deal with it is to do what I'd done earlier treat the the text that has these unwanted curly brackets as input variables and then okay so so one way to do this is you may be thinking okay so that means that I now have to provide text as text here and this is a little bit annoying because now you have two input variables it's not the end of the world but it's a little bit annoying and if if this chain is part of a larger chain then then it starts to get more and more complicated more and more annoying um and ah I had that should be taex this is erroring because the model's not long enough I could do some fancy rag um it's just like barely over the context window and I'm I'm going to want to use this model anyways because this task is kind of hard so I want a good model to do it um so this will just get me around that and then let me also set temperature equals to zero while I'm at it so I could do it this way I could pass in the text here um and this would work let's see what answer it gives um for the next time I'll use streaming um so we can start seeing what's going on earlier um all right it's writing long thing while while it's working okay so let's um this is a little unreadable um because it's not super readable what I'm going to do is all right so this is Lang Smith this is me debugging I\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"so this is Lang Smith this is me debugging I want to see the output um this is a all right so this is better output um I get uh oh interesting okay so all right so it's giving me some instructions on how to use it um I actually don't know if this is correct because of these curly brackets but anyways um what we can do is uh uh sorry back to this what we can do there's a lot I want to show off but first things we can do is partial The Prompt so we can now do this and that means we no longer have to pass this in so we're basically just partialing the prompt we're passing in some input variables ahead of time before we even construct the train and then because we know those are always going to be fixed and they're and they shouldn't be variables so we're going to pass them in and then we can do this I'm going to now change this to streaming for Chunk in or more accurately for token in chain. stream print token um all right I need to add something like this all right so there we go it's not exactly um what I really want I really want like just one I really just want this thing um templ should be a python see if this helps so it's not helping a ton I can go back in here and I can I can start doing some prompt engineering here if I want so one thing that I can do it's not really that helpful here because this is a really simple um chain but if I wanted to I could open up the playground and it's also not helpful here because my prompt is really really long and this isn't the grass oh I can okay that's slightly better so now I have this um and um let's let's just do like bunch of new that thing based on the above instructions help me write a good prompt template this prompt template should be a string that can be formatted as if a python fstring I can take in any number of variables depending on my objective um I can now run I can now run this okay okay that's pretty good all right so that's pretty good I'm I'm I'm relatively happy with this so um let's uh take this from\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"happy with this so um let's uh take this from the playground let's bring it back in the notebook 24 being a bit slow okay so it's generally doing pretty good I'm relatively happy with this I might do a little bit more prompt engineering by the time this gets out but I'm relatively happy with this for now so what I'm going to do is I'm going to move it into Lang serve now I'm going to deploy it with Lang serve so I am going to create a new file in here I am going to basically copy this chain over that's done I'm going to go into the server um so I'm going to edit this add the chain you want to add um from app. chain import chain chain path prompter cool um I'm going to go back here and let's try it out and see what happens L chain serve doesn't find the file that is right so L chain notebooks open AI prom what did I name it prompting oh right let specify that let's try serving it again and I'm going to poetry add open AI because I'm going to deploy this project later um and this will add it to the Pi Project file so it's going to save that um let's try it again cool so now if I go to Local Host 8,000 prompter playground I get this let's try it out I take this add it here all right so I get back some streaming um I get back some nice stuff okay so there's definitely some prompt engineering that I want to do um it's doing this um Let me let me pause the video a bit do some prompt engineering and then resume it in a second all right so I'm back with a little bit of a better prompt um and I will uh uh I I'll include links to all the code so I won't read out here basically now if I go here and I type in an objective this gets me um this gets me a pretty solid prompt that I'd use for for retrievable augmented generation so so that's basically how we get to something that's deployed with Ling serf um this is still all local and so for the final part of this I'll walk through how to do this um on laying Smith so that you can share it with other people as you see so I have\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"share it with other people as you see so I have my my personal tenant my personal tenant doesn't have access to uh laying serve deployments yet on Lang Smith so this is uh this is an alpha feature that we're still testing um it's it's it's only available to a few people um if you are interested in being an alpha tester please let us know um or please let me know and uh we are not letting a ton of people off because we want we do want to make some improvements um but as we let more and more people off um you know we'll we'll will uh let me know and and you can be one of them in order to show you briefly what it looks like I am uh going to switch to the Lang chain account I'm going to go to deployments um and you'll see once it loads that I have a bunch of deployments here these deployments are all connected to GitHub um so one thing that I'm going to do after this is I'm going to pause set up a GitHub um uh repo with this code so that I can easily deploy it um so let me pause and do that and then I'll come back and walk through a new deployment and see what happens all right so we're back so I'm going to create a new deployment I'm going to click here I'm going to choose this repo that I created open AI Auto prompter name it uh open AI prompting helper um I'm going to add some environment variables um namely I'm going to add open AI API key um I'm going to make it a secret G to pause while I put this in all right it's in um I automatically get a tracing project for this so that's a big benefit of deploying on Lang Smith is it automatically connects to everything else in Lang Smith um right now that's tracing we'll make that testing um and and prompts and other things in the short term um I can now submit this and it will spin up a deployment um and so here is this uh uh deployment as you can see it's uh it'll take a little bit so I'm going to pause and come back when it's finished deploying all right we're back our deployment succeeded we can now open the playground\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"succeeded we can now open the playground for this deployment so we can go here task um what what was the task we had way back in the day so let's just use the same task here we can see it streaming awesome so we get this result um one thing we can do is we can give thumbs up or thumbs down I like this one so I'm going to click thumbs up if I go back here I can see it's only one Trace count but I can see the traces for this so here this is the most recent Trace I can click into it this is a tracing from this is the same tracing that's in uh uh langing Smith as you've been using it so that's one of the big benefits as it comes with all this stuff it comes with feedback automatically hooked up so we have a score here um and yeah that's uh basically it for the video now hopefully this was pretty helpful in terms of getting um a pretty simple chain it's you know it's just a a prompt a model and an Alpa parer up and running using lell uh getting around some uh prompt uh annoyances with all the formatting of the curly brackets shwing how to set up langing Smith showing how to set up a laying serve project and then showing uh the new laying serve deployment feature as well hopefully you guys enjoyed this let me know in the comments have a good one\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'})]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_doc = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nQuick Start | \\uf8ff\\uf8ff Langchain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\uf8ff\\uf8ff LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube vi\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_doc[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-modules/model_io/prompts/quick_start\" data-has-hydrated=\"false\" dir=\"ltr\" lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"Docusaurus v2.4.3\" name=\"generator\"/>\n",
       "<title data-rh=\"true\">Quick Start |  Langchain</title><meta content=\"width=device-width,initial-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"https://python.langchain.com/img/parrot-chainlink-icon.png\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://python.langchain.com/img/parrot-chainlink-icon.png\" data-rh=\"true\" name=\"twitter:image\"/><meta content=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" data-rh=\"true\" property=\"og:url\"/><meta content=\"en\" data-rh=\"true\" name=\"docusaurus_locale\"/><meta content=\"en\" data-rh=\"true\" name=\"docsearch:language\"/><meta content=\"current\" data-rh=\"true\" name=\"docusaurus_version\"/><meta content=\"docs-default-current\" data-rh=\"true\" name=\"docusaurus_tag\"/><meta content=\"current\" data-rh=\"true\" name=\"docsearch:version\"/><meta content=\"docs-default-current\" data-rh=\"true\" name=\"docsearch:docusaurus_tag\"/><meta content=\"Quick Start |  Langchain\" data-rh=\"true\" property=\"og:title\"/><meta content=\"quick-start}\" data-rh=\"true\" name=\"description\"/><meta content=\"quick-start}\" data-rh=\"true\" property=\"og:description\"/><link data-rh=\"true\" href=\"/img/favicon.ico\" rel=\"icon\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" rel=\"canonical\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" hreflang=\"en\" rel=\"alternate\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" hreflang=\"x-default\" rel=\"alternate\"/><link crossorigin=\"anonymous\" data-rh=\"true\" href=\"https://VAU016LAWS-dsn.algolia.net\" rel=\"preconnect\"/><link href=\"/opensearch.xml\" rel=\"search\" title=\" Langchain\" type=\"application/opensearchdescription+xml\"/>\n",
       "<script src=\"/js/google_analytics.js\"></script>\n",
       "<script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F\"></script><link href=\"/assets/css/styles.48c66593.css\" rel=\"stylesheet\"/>\n",
       "<link as=\"script\" href=\"/assets/js/runtime~main.f00f11f4.js\" rel=\"preload\"/>\n",
       "<link as=\"script\" href=\"/assets/js/main.ee2bb98c.js\" rel=\"preload\"/>\n",
       "</head>\n",
       "<body class=\"navigation-with-keyboard\">\n",
       "<script>!function(){function e(e){document.documentElement.setAttribute(\"data-theme\",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem(\"theme\")}catch(e){}return e}();null!==t?e(t):window.matchMedia(\"(prefers-color-scheme: dark)\").matches?e(\"dark\"):(window.matchMedia(\"(prefers-color-scheme: light)\").matches,e(\"light\"))}()</script><div id=\"__docusaurus\">\n",
       "<div aria-label=\"Skip to main content\" role=\"region\"><a class=\"skipToContent_fXgn\" href=\"#__docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-expanded=\"false\" aria-label=\"Toggle navigation bar\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg aria-hidden=\"true\" height=\"30\" viewbox=\"0 0 30 30\" width=\"30\"><path d=\"M4 7h22M4 15h22M4 23h22\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\"> LangChain</b></a><a class=\"navbar__item navbar__link\" href=\"/docs/get_started/introduction\">Docs</a><a class=\"navbar__item navbar__link\" href=\"/docs/use_cases\">Use cases</a><a class=\"navbar__item navbar__link\" href=\"/docs/integrations/providers\">Integrations</a><a class=\"navbar__item navbar__link\" href=\"/docs/guides/debugging\">Guides</a><a class=\"navbar__item navbar__link\" href=\"https://api.python.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">API<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a><div class=\"navbar__item dropdown dropdown--hoverable\"><a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"navbar__link\" href=\"#\" role=\"button\">More</a><ul class=\"dropdown__menu\"><li><a class=\"dropdown__link\" href=\"/docs/packages\">Versioning</a></li><li><a class=\"dropdown__link\" href=\"/docs/changelog\">Changelog</a></li><li><a class=\"dropdown__link\" href=\"/docs/contributing\">Developer's guide</a></li><li><a class=\"dropdown__link\" href=\"/docs/templates/\">Templates</a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md\" rel=\"noopener noreferrer\" target=\"_blank\">Cookbooks<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"/docs/additional_resources/tutorials\">Tutorials</a></li><li><a class=\"dropdown__link\" href=\"/docs/additional_resources/youtube\">YouTube videos</a></li></ul></div></div><div class=\"navbar__items navbar__items--right\"><div class=\"navbar__item dropdown dropdown--hoverable dropdown--right\"><a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"navbar__link\" href=\"#\" role=\"button\"></a><ul class=\"dropdown__menu\"><li><a class=\"dropdown__link\" href=\"https://smith.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">LangSmith<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://docs.smith.langchain.com/\" rel=\"noopener noreferrer\" target=\"_blank\">LangSmith Docs<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langserve\" rel=\"noopener noreferrer\" target=\"_blank\">LangServe GitHub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langchain/tree/master/templates\" rel=\"noopener noreferrer\" target=\"_blank\">Templates GitHub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://templates.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Templates Hub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://smith.langchain.com/hub\" rel=\"noopener noreferrer\" target=\"_blank\">LangChain Hub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://js.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">JS/TS Docs<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><a class=\"navbar__item navbar__link\" href=\"https://chat.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Chat<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a><a aria-label=\"GitHub repository\" class=\"navbar__item navbar__link header-github-link\" href=\"https://github.com/langchain-ai/langchain\" rel=\"noopener noreferrer\" target=\"_blank\"></a><div class=\"toggle_vylO colorModeToggle_DEke\"><button aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\" class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" type=\"button\"><svg class=\"lightToggleIcon_pyhR\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\" fill=\"currentColor\"></path></svg><svg class=\"darkToggleIcon_wfgR\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\" fill=\"currentColor\"></path></svg></button></div><div class=\"searchBox_ZlJk\"><button aria-label=\"Search\" class=\"DocSearch DocSearch-Button\" type=\"button\"><span class=\"DocSearch-Button-Container\"><svg class=\"DocSearch-Search-Icon\" height=\"20\" viewbox=\"0 0 20 20\" width=\"20\"><path d=\"M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg><span class=\"DocSearch-Button-Placeholder\">Search</span></span><span class=\"DocSearch-Button-Keys\"></span></button></div></div></div><div class=\"navbar-sidebar__backdrop\" role=\"presentation\"></div></nav><div class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\" id=\"__docusaurus_skipToContent_fallback\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link\" href=\"/docs/get_started\">Get started</a></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/introduction\" tabindex=\"0\">Introduction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/installation\" tabindex=\"0\">Installation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/quickstart\" tabindex=\"0\">Quickstart</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/security\" tabindex=\"0\">Security</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/\">LangChain Expression Language</a><button aria-label=\"Toggle the collapsible sidebar category 'LangChain Expression Language'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/get_started\" tabindex=\"0\">Get started</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/why\" tabindex=\"0\">Why use LCEL</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/interface\" tabindex=\"0\">Interface</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/how_to/\" tabindex=\"0\">How to</a><button aria-label=\"Toggle the collapsible sidebar category 'How to'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/cookbook/\" tabindex=\"0\">Cookbook</a><button aria-label=\"Toggle the collapsible sidebar category 'Cookbook'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden\"><a class=\"menu__link\" href=\"/docs/expression_language/\" tabindex=\"0\">LangChain Expression Language (LCEL)</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/\">Modules</a><button aria-label=\"Toggle the collapsible sidebar category 'Modules'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/model_io/\" tabindex=\"0\">Model I/O</a><button aria-label=\"Toggle the collapsible sidebar category 'Model I/O'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden\"><a class=\"menu__link\" href=\"/docs/modules/model_io/\" tabindex=\"0\">Model I/O</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/quick_start\" tabindex=\"0\">Quickstart</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/concepts\" tabindex=\"0\">Concepts</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/model_io/prompts/\" tabindex=\"0\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category 'Prompts'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a aria-current=\"page\" class=\"menu__link menu__link--active\" href=\"/docs/modules/model_io/prompts/quick_start\" tabindex=\"0\">Quick Start</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/composition\" tabindex=\"0\">Composition</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/prompts/example_selector_types/\" tabindex=\"0\">Example Selector Types</a><button aria-label=\"Toggle the collapsible sidebar category 'Example Selector Types'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/example_selectors\" tabindex=\"0\">Example selectors</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/few_shot_examples\" tabindex=\"0\">Few-shot prompt templates</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/few_shot_examples_chat\" tabindex=\"0\">Few-shot examples for chat models</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/message_prompts\" tabindex=\"0\">Types of `MessagePromptTemplate`</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/partial\" tabindex=\"0\">Partial prompt templates</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/pipeline\" tabindex=\"0\">Pipeline</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/chat/\" tabindex=\"0\">Chat Models</a><button aria-label=\"Toggle the collapsible sidebar category 'Chat Models'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/llms/\" tabindex=\"0\">LLMs</a><button aria-label=\"Toggle the collapsible sidebar category 'LLMs'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/output_parsers/\" tabindex=\"0\">Output Parsers</a><button aria-label=\"Toggle the collapsible sidebar category 'Output Parsers'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/data_connection/\" tabindex=\"0\">Retrieval</a><button aria-label=\"Toggle the collapsible sidebar category 'Retrieval'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/agents/\" tabindex=\"0\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category 'Agents'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/chains\" tabindex=\"0\">Chains</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist menu__link--sublist-caret\" href=\"/docs/modules/memory/\" tabindex=\"0\">More</a></div></li></ul></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/langserve\">LangServe</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/langsmith/\">LangSmith</a><button aria-label=\"Toggle the collapsible sidebar category 'LangSmith'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/langgraph\">LangGraph</a></li></ul></nav><button aria-label=\"Collapse sidebar\" class=\"button button--secondary button--outline collapseSidebarButton_PEFL\" title=\"Collapse sidebar\" type=\"button\"><svg aria-hidden=\"true\" class=\"collapseSidebarButtonIcon_kv0_\" height=\"20\" width=\"20\"><g fill=\"#7a7a7a\"><path d=\"M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0\"></path><path d=\"M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0\"></path></g></svg></button></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav aria-label=\"Breadcrumbs\" class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg class=\"breadcrumbHomeIcon_YNFT\" viewbox=\"0 0 24 24\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/\" itemprop=\"item\"><span itemprop=\"name\">Modules</span></a><meta content=\"1\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/model_io/\" itemprop=\"item\"><span itemprop=\"name\">Model I/O</span></a><meta content=\"2\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/model_io/prompts/\" itemprop=\"item\"><span itemprop=\"name\">Prompts</span></a><meta content=\"3\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item breadcrumbs__item--active\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Quick Start</span><meta content=\"4\" itemprop=\"position\"/></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button class=\"clean-btn tocCollapsibleButton_TO0P\" type=\"button\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Quick Start</h1><p>Prompt templates are predefined recipes for generating prompts for\n",
       "language models.</p><p>A template may include instructions, few-shot examples, and specific\n",
       "context and questions appropriate for a given task.</p><p>LangChain provides tooling to create and work with prompt templates.</p><p>LangChain strives to create model agnostic templates to make it easy to\n",
       "reuse existing templates across different language models.</p><p>Typically, language models expect the prompt to either be a string or\n",
       "else a list of chat messages.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"prompttemplate\"><code>PromptTemplate</code><a aria-label=\"Direct link to prompttemplate\" class=\"hash-link\" href=\"#prompttemplate\" title=\"Direct link to prompttemplate\"></a></h2><p>Use <code>PromptTemplate</code> to create a template for a string prompt.</p><p>By default, <code>PromptTemplate</code> uses <a href=\"https://docs.python.org/3/library/stdtypes.html#str.format\" rel=\"noopener noreferrer\" target=\"_blank\">Pythons\n",
       "str.format</a>\n",
       "syntax for templating.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> PromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> PromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Tell me a {adjective} joke about {content}.\"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token builtin\" style=\"color:rgb(0, 112, 193)\">format</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">adjective</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"funny\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> content</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"chickens\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a funny joke about chickens.'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>The template supports any number of variables, including no variables:</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> PromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> PromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Tell me a joke\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token builtin\" style=\"color:rgb(0, 112, 193)\">format</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a joke'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>You can create custom prompt templates that format the prompt in any way\n",
       "you want. For more information, see <a href=\"/docs/modules/model_io/prompts/custom_prompt_template\">Custom Prompt\n",
       "Templates</a>.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"chatprompttemplate\"><code>ChatPromptTemplate</code><a aria-label=\"Direct link to chatprompttemplate\" class=\"hash-link\" href=\"#chatprompttemplate\" title=\"Direct link to chatprompttemplate\"></a></h2><p>The prompt to <a href=\"/docs/modules/model_io/chat\">chat models</a> is a list of chat messages.</p><p>Each chat message is associated with content, and an additional\n",
       "parameter called <code>role</code>. For example, in the OpenAI <a href=\"https://platform.openai.com/docs/guides/chat/introduction\" rel=\"noopener noreferrer\" target=\"_blank\">Chat Completions\n",
       "API</a>, a chat\n",
       "message can be associated with an AI assistant, a human or a system\n",
       "role.</p><p>Create a chat prompt template like this:</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> ChatPromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> ChatPromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">[</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"system\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"You are a helpful AI bot. Your name is {name}.\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"human\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Hello, how are you doing?\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"ai\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"I'm doing well, thanks!\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"human\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"{user_input}\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">]</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">messages </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">format_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">name</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Bob\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> user_input</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"What is your name?\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p><code>ChatPromptTemplate.from_messages</code> accepts a variety of message\n",
       "representations.</p><p>For example, in addition to using the 2-tuple representation of (type,\n",
       "content) used above, you could pass in an instance of\n",
       "<code>MessagePromptTemplate</code> or <code>BaseMessage</code>.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> HumanMessagePromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">messages </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> SystemMessage</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_openai </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> ChatOpenAI</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> ChatPromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">[</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        SystemMessage</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">            content</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">                </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"You are a helpful assistant that re-writes the user's text to \"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">                </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"sound more upbeat.\"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">            </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        HumanMessagePromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"{text}\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">]</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">messages </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">format_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">text</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"I don't like eating tasty things\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">print</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>This provides you with a lot of flexibility in how you construct your\n",
       "chat prompts.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"lcel\">LCEL<a aria-label=\"Direct link to LCEL\" class=\"hash-link\" href=\"#lcel\" title=\"Direct link to LCEL\"></a></h2><p><code>PromptTemplate</code> and <code>ChatPromptTemplate</code> implement the <a href=\"/docs/expression_language/interface\">Runnable\n",
       "interface</a>, the basic building\n",
       "block of the <a href=\"/docs/expression_language/\">LangChain Expression Language\n",
       "(LCEL)</a>. This means they support <code>invoke</code>,\n",
       "<code>ainvoke</code>, <code>stream</code>, <code>astream</code>, <code>batch</code>, <code>abatch</code>, <code>astream_log</code> calls.</p><p><code>PromptTemplate</code> accepts a dictionary (of the prompt variables) and\n",
       "returns a <code>StringPromptValue</code>. A <code>ChatPromptTemplate</code> accepts a\n",
       "dictionary and returns a <code>ChatPromptValue</code>.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">invoke</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">{</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"adjective\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"funny\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"content\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"chickens\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">}</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">StringPromptValue(text='Tell me a joke')</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_string</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a joke'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[HumanMessage(content='Tell me a joke')]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">invoke</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">{</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"text\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"i dont like eating tasty things.\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">}</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"),</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"> HumanMessage(content='i dont like eating tasty things.')]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_string</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">\"System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\\nHuman: i dont like eating tasty things.\"</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div></div></article><nav aria-label=\"Docs pages\" class=\"pagination-nav docusaurus-mt-lg\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/modules/model_io/prompts/\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Prompts</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/modules/model_io/prompts/composition\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Composition</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a class=\"table-of-contents__link toc-highlight\" href=\"#prompttemplate\"><code>PromptTemplate</code></a></li><li><a class=\"table-of-contents__link toc-highlight\" href=\"#chatprompttemplate\"><code>ChatPromptTemplate</code></a></li><li><a class=\"table-of-contents__link toc-highlight\" href=\"#lcel\">LCEL</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://discord.gg/cU2adEyC7w\" rel=\"noopener noreferrer\" target=\"_blank\">Discord<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://twitter.com/LangChainAI\" rel=\"noopener noreferrer\" target=\"_blank\">Twitter<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://github.com/langchain-ai/langchain\" rel=\"noopener noreferrer\" target=\"_blank\">Python<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://github.com/langchain-ai/langchainjs\" rel=\"noopener noreferrer\" target=\"_blank\">JS/TS<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Homepage<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://blog.langchain.dev\" rel=\"noopener noreferrer\" target=\"_blank\">Blog<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright  2024 LangChain, Inc.</div></div></div></footer></div>\n",
       "<script src=\"/assets/js/runtime~main.f00f11f4.js\"></script>\n",
       "<script src=\"/assets/js/main.ee2bb98c.js\"></script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PageElement.get_text of <!DOCTYPE html>\n",
       "\n",
       "<html class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-modules/model_io/prompts/quick_start\" data-has-hydrated=\"false\" dir=\"ltr\" lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"Docusaurus v2.4.3\" name=\"generator\"/>\n",
       "<title data-rh=\"true\">Quick Start |  Langchain</title><meta content=\"width=device-width,initial-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"https://python.langchain.com/img/parrot-chainlink-icon.png\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://python.langchain.com/img/parrot-chainlink-icon.png\" data-rh=\"true\" name=\"twitter:image\"/><meta content=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" data-rh=\"true\" property=\"og:url\"/><meta content=\"en\" data-rh=\"true\" name=\"docusaurus_locale\"/><meta content=\"en\" data-rh=\"true\" name=\"docsearch:language\"/><meta content=\"current\" data-rh=\"true\" name=\"docusaurus_version\"/><meta content=\"docs-default-current\" data-rh=\"true\" name=\"docusaurus_tag\"/><meta content=\"current\" data-rh=\"true\" name=\"docsearch:version\"/><meta content=\"docs-default-current\" data-rh=\"true\" name=\"docsearch:docusaurus_tag\"/><meta content=\"Quick Start |  Langchain\" data-rh=\"true\" property=\"og:title\"/><meta content=\"quick-start}\" data-rh=\"true\" name=\"description\"/><meta content=\"quick-start}\" data-rh=\"true\" property=\"og:description\"/><link data-rh=\"true\" href=\"/img/favicon.ico\" rel=\"icon\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" rel=\"canonical\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" hreflang=\"en\" rel=\"alternate\"/><link data-rh=\"true\" href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start\" hreflang=\"x-default\" rel=\"alternate\"/><link crossorigin=\"anonymous\" data-rh=\"true\" href=\"https://VAU016LAWS-dsn.algolia.net\" rel=\"preconnect\"/><link href=\"/opensearch.xml\" rel=\"search\" title=\" Langchain\" type=\"application/opensearchdescription+xml\"/>\n",
       "<script src=\"/js/google_analytics.js\"></script>\n",
       "<script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F\"></script><link href=\"/assets/css/styles.48c66593.css\" rel=\"stylesheet\"/>\n",
       "<link as=\"script\" href=\"/assets/js/runtime~main.f00f11f4.js\" rel=\"preload\"/>\n",
       "<link as=\"script\" href=\"/assets/js/main.ee2bb98c.js\" rel=\"preload\"/>\n",
       "</head>\n",
       "<body class=\"navigation-with-keyboard\">\n",
       "<script>!function(){function e(e){document.documentElement.setAttribute(\"data-theme\",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem(\"theme\")}catch(e){}return e}();null!==t?e(t):window.matchMedia(\"(prefers-color-scheme: dark)\").matches?e(\"dark\"):(window.matchMedia(\"(prefers-color-scheme: light)\").matches,e(\"light\"))}()</script><div id=\"__docusaurus\">\n",
       "<div aria-label=\"Skip to main content\" role=\"region\"><a class=\"skipToContent_fXgn\" href=\"#__docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-expanded=\"false\" aria-label=\"Toggle navigation bar\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg aria-hidden=\"true\" height=\"30\" viewbox=\"0 0 30 30\" width=\"30\"><path d=\"M4 7h22M4 15h22M4 23h22\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\"> LangChain</b></a><a class=\"navbar__item navbar__link\" href=\"/docs/get_started/introduction\">Docs</a><a class=\"navbar__item navbar__link\" href=\"/docs/use_cases\">Use cases</a><a class=\"navbar__item navbar__link\" href=\"/docs/integrations/providers\">Integrations</a><a class=\"navbar__item navbar__link\" href=\"/docs/guides/debugging\">Guides</a><a class=\"navbar__item navbar__link\" href=\"https://api.python.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">API<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a><div class=\"navbar__item dropdown dropdown--hoverable\"><a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"navbar__link\" href=\"#\" role=\"button\">More</a><ul class=\"dropdown__menu\"><li><a class=\"dropdown__link\" href=\"/docs/packages\">Versioning</a></li><li><a class=\"dropdown__link\" href=\"/docs/changelog\">Changelog</a></li><li><a class=\"dropdown__link\" href=\"/docs/contributing\">Developer's guide</a></li><li><a class=\"dropdown__link\" href=\"/docs/templates/\">Templates</a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md\" rel=\"noopener noreferrer\" target=\"_blank\">Cookbooks<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"/docs/additional_resources/tutorials\">Tutorials</a></li><li><a class=\"dropdown__link\" href=\"/docs/additional_resources/youtube\">YouTube videos</a></li></ul></div></div><div class=\"navbar__items navbar__items--right\"><div class=\"navbar__item dropdown dropdown--hoverable dropdown--right\"><a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"navbar__link\" href=\"#\" role=\"button\"></a><ul class=\"dropdown__menu\"><li><a class=\"dropdown__link\" href=\"https://smith.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">LangSmith<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://docs.smith.langchain.com/\" rel=\"noopener noreferrer\" target=\"_blank\">LangSmith Docs<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langserve\" rel=\"noopener noreferrer\" target=\"_blank\">LangServe GitHub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://github.com/langchain-ai/langchain/tree/master/templates\" rel=\"noopener noreferrer\" target=\"_blank\">Templates GitHub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://templates.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Templates Hub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://smith.langchain.com/hub\" rel=\"noopener noreferrer\" target=\"_blank\">LangChain Hub<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li><a class=\"dropdown__link\" href=\"https://js.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">JS/TS Docs<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"12\" viewbox=\"0 0 24 24\" width=\"12\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><a class=\"navbar__item navbar__link\" href=\"https://chat.langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Chat<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a><a aria-label=\"GitHub repository\" class=\"navbar__item navbar__link header-github-link\" href=\"https://github.com/langchain-ai/langchain\" rel=\"noopener noreferrer\" target=\"_blank\"></a><div class=\"toggle_vylO colorModeToggle_DEke\"><button aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\" class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" type=\"button\"><svg class=\"lightToggleIcon_pyhR\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\" fill=\"currentColor\"></path></svg><svg class=\"darkToggleIcon_wfgR\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\" fill=\"currentColor\"></path></svg></button></div><div class=\"searchBox_ZlJk\"><button aria-label=\"Search\" class=\"DocSearch DocSearch-Button\" type=\"button\"><span class=\"DocSearch-Button-Container\"><svg class=\"DocSearch-Search-Icon\" height=\"20\" viewbox=\"0 0 20 20\" width=\"20\"><path d=\"M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg><span class=\"DocSearch-Button-Placeholder\">Search</span></span><span class=\"DocSearch-Button-Keys\"></span></button></div></div></div><div class=\"navbar-sidebar__backdrop\" role=\"presentation\"></div></nav><div class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\" id=\"__docusaurus_skipToContent_fallback\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link\" href=\"/docs/get_started\">Get started</a></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/introduction\" tabindex=\"0\">Introduction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/installation\" tabindex=\"0\">Installation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/get_started/quickstart\" tabindex=\"0\">Quickstart</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/security\" tabindex=\"0\">Security</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/\">LangChain Expression Language</a><button aria-label=\"Toggle the collapsible sidebar category 'LangChain Expression Language'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/get_started\" tabindex=\"0\">Get started</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/why\" tabindex=\"0\">Why use LCEL</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/expression_language/interface\" tabindex=\"0\">Interface</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/how_to/\" tabindex=\"0\">How to</a><button aria-label=\"Toggle the collapsible sidebar category 'How to'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/expression_language/cookbook/\" tabindex=\"0\">Cookbook</a><button aria-label=\"Toggle the collapsible sidebar category 'Cookbook'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item hidden\"><a class=\"menu__link\" href=\"/docs/expression_language/\" tabindex=\"0\">LangChain Expression Language (LCEL)</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/\">Modules</a><button aria-label=\"Toggle the collapsible sidebar category 'Modules'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/model_io/\" tabindex=\"0\">Model I/O</a><button aria-label=\"Toggle the collapsible sidebar category 'Model I/O'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item hidden\"><a class=\"menu__link\" href=\"/docs/modules/model_io/\" tabindex=\"0\">Model I/O</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/quick_start\" tabindex=\"0\">Quickstart</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/concepts\" tabindex=\"0\">Concepts</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"true\" class=\"menu__link menu__link--sublist menu__link--active\" href=\"/docs/modules/model_io/prompts/\" tabindex=\"0\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category 'Prompts'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div><ul class=\"menu__list\" style=\"display:block;overflow:visible;height:auto\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a aria-current=\"page\" class=\"menu__link menu__link--active\" href=\"/docs/modules/model_io/prompts/quick_start\" tabindex=\"0\">Quick Start</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/composition\" tabindex=\"0\">Composition</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/prompts/example_selector_types/\" tabindex=\"0\">Example Selector Types</a><button aria-label=\"Toggle the collapsible sidebar category 'Example Selector Types'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/example_selectors\" tabindex=\"0\">Example selectors</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/few_shot_examples\" tabindex=\"0\">Few-shot prompt templates</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/few_shot_examples_chat\" tabindex=\"0\">Few-shot examples for chat models</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/message_prompts\" tabindex=\"0\">Types of `MessagePromptTemplate`</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/partial\" tabindex=\"0\">Partial prompt templates</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/model_io/prompts/pipeline\" tabindex=\"0\">Pipeline</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/chat/\" tabindex=\"0\">Chat Models</a><button aria-label=\"Toggle the collapsible sidebar category 'Chat Models'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/llms/\" tabindex=\"0\">LLMs</a><button aria-label=\"Toggle the collapsible sidebar category 'LLMs'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/model_io/output_parsers/\" tabindex=\"0\">Output Parsers</a><button aria-label=\"Toggle the collapsible sidebar category 'Output Parsers'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/data_connection/\" tabindex=\"0\">Retrieval</a><button aria-label=\"Toggle the collapsible sidebar category 'Retrieval'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/modules/agents/\" tabindex=\"0\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category 'Agents'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" href=\"/docs/modules/chains\" tabindex=\"0\">Chains</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist menu__link--sublist-caret\" href=\"/docs/modules/memory/\" tabindex=\"0\">More</a></div></li></ul></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/langserve\">LangServe</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a aria-expanded=\"false\" class=\"menu__link menu__link--sublist\" href=\"/docs/langsmith/\">LangSmith</a><button aria-label=\"Toggle the collapsible sidebar category 'LangSmith'\" class=\"clean-btn menu__caret\" type=\"button\"></button></div></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/langgraph\">LangGraph</a></li></ul></nav><button aria-label=\"Collapse sidebar\" class=\"button button--secondary button--outline collapseSidebarButton_PEFL\" title=\"Collapse sidebar\" type=\"button\"><svg aria-hidden=\"true\" class=\"collapseSidebarButtonIcon_kv0_\" height=\"20\" width=\"20\"><g fill=\"#7a7a7a\"><path d=\"M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0\"></path><path d=\"M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0\"></path></g></svg></button></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav aria-label=\"Breadcrumbs\" class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg class=\"breadcrumbHomeIcon_YNFT\" viewbox=\"0 0 24 24\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/\" itemprop=\"item\"><span itemprop=\"name\">Modules</span></a><meta content=\"1\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/model_io/\" itemprop=\"item\"><span itemprop=\"name\">Model I/O</span></a><meta content=\"2\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><a class=\"breadcrumbs__link\" href=\"/docs/modules/model_io/prompts/\" itemprop=\"item\"><span itemprop=\"name\">Prompts</span></a><meta content=\"3\" itemprop=\"position\"/></li><li class=\"breadcrumbs__item breadcrumbs__item--active\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Quick Start</span><meta content=\"4\" itemprop=\"position\"/></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button class=\"clean-btn tocCollapsibleButton_TO0P\" type=\"button\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Quick Start</h1><p>Prompt templates are predefined recipes for generating prompts for\n",
       "language models.</p><p>A template may include instructions, few-shot examples, and specific\n",
       "context and questions appropriate for a given task.</p><p>LangChain provides tooling to create and work with prompt templates.</p><p>LangChain strives to create model agnostic templates to make it easy to\n",
       "reuse existing templates across different language models.</p><p>Typically, language models expect the prompt to either be a string or\n",
       "else a list of chat messages.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"prompttemplate\"><code>PromptTemplate</code><a aria-label=\"Direct link to prompttemplate\" class=\"hash-link\" href=\"#prompttemplate\" title=\"Direct link to prompttemplate\"></a></h2><p>Use <code>PromptTemplate</code> to create a template for a string prompt.</p><p>By default, <code>PromptTemplate</code> uses <a href=\"https://docs.python.org/3/library/stdtypes.html#str.format\" rel=\"noopener noreferrer\" target=\"_blank\">Pythons\n",
       "str.format</a>\n",
       "syntax for templating.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> PromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> PromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Tell me a {adjective} joke about {content}.\"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token builtin\" style=\"color:rgb(0, 112, 193)\">format</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">adjective</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"funny\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> content</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"chickens\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a funny joke about chickens.'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>The template supports any number of variables, including no variables:</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> PromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> PromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Tell me a joke\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token builtin\" style=\"color:rgb(0, 112, 193)\">format</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a joke'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>You can create custom prompt templates that format the prompt in any way\n",
       "you want. For more information, see <a href=\"/docs/modules/model_io/prompts/custom_prompt_template\">Custom Prompt\n",
       "Templates</a>.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"chatprompttemplate\"><code>ChatPromptTemplate</code><a aria-label=\"Direct link to chatprompttemplate\" class=\"hash-link\" href=\"#chatprompttemplate\" title=\"Direct link to chatprompttemplate\"></a></h2><p>The prompt to <a href=\"/docs/modules/model_io/chat\">chat models</a> is a list of chat messages.</p><p>Each chat message is associated with content, and an additional\n",
       "parameter called <code>role</code>. For example, in the OpenAI <a href=\"https://platform.openai.com/docs/guides/chat/introduction\" rel=\"noopener noreferrer\" target=\"_blank\">Chat Completions\n",
       "API</a>, a chat\n",
       "message can be associated with an AI assistant, a human or a system\n",
       "role.</p><p>Create a chat prompt template like this:</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> ChatPromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> ChatPromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">[</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"system\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"You are a helpful AI bot. Your name is {name}.\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"human\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Hello, how are you doing?\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"ai\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"I'm doing well, thanks!\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"human\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"{user_input}\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">]</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">messages </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">format_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">name</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"Bob\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> user_input</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"What is your name?\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p><code>ChatPromptTemplate.from_messages</code> accepts a variety of message\n",
       "representations.</p><p>For example, in addition to using the 2-tuple representation of (type,\n",
       "content) used above, you could pass in an instance of\n",
       "<code>MessagePromptTemplate</code> or <code>BaseMessage</code>.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> HumanMessagePromptTemplate</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">messages </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> SystemMessage</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">from</span><span class=\"token plain\"> langchain_openai </span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">import</span><span class=\"token plain\"> ChatOpenAI</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\" style=\"display:inline-block\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_template </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> ChatPromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">[</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        SystemMessage</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">            content</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">                </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"You are a helpful assistant that re-writes the user's text to \"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">                </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"sound more upbeat.\"</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">            </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">        HumanMessagePromptTemplate</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">from_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"{text}\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">]</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">messages </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">format_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">text</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"I don't like eating tasty things\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(0, 0, 255)\">print</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token plain\">messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><p>This provides you with a lot of flexibility in how you construct your\n",
       "chat prompts.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"lcel\">LCEL<a aria-label=\"Direct link to LCEL\" class=\"hash-link\" href=\"#lcel\" title=\"Direct link to LCEL\"></a></h2><p><code>PromptTemplate</code> and <code>ChatPromptTemplate</code> implement the <a href=\"/docs/expression_language/interface\">Runnable\n",
       "interface</a>, the basic building\n",
       "block of the <a href=\"/docs/expression_language/\">LangChain Expression Language\n",
       "(LCEL)</a>. This means they support <code>invoke</code>,\n",
       "<code>ainvoke</code>, <code>stream</code>, <code>astream</code>, <code>batch</code>, <code>abatch</code>, <code>astream_log</code> calls.</p><p><code>PromptTemplate</code> accepts a dictionary (of the prompt variables) and\n",
       "returns a <code>StringPromptValue</code>. A <code>ChatPromptTemplate</code> accepts a\n",
       "dictionary and returns a <code>ChatPromptValue</code>.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> prompt_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">invoke</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">{</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"adjective\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"funny\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"content\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"chickens\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">}</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><span class=\"token plain\"></span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">StringPromptValue(text='Tell me a joke')</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_string</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">'Tell me a joke'</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">prompt_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[HumanMessage(content='Tell me a joke')]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val </span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">=</span><span class=\"token plain\"> chat_template</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">invoke</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">{</span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"text\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">:</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(163, 21, 21)\">\"i dont like eating tasty things.\"</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">}</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_messages</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"),</span><br/></span><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\"> HumanMessage(content='i dont like eating tasty things.')]</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">chat_val</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">.</span><span class=\"token plain\">to_string</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">(</span><span class=\"token punctuation\" style=\"color:rgb(4, 81, 165)\">)</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div><div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#000000;--prism-background-color:#F5F5F5\"><div class=\"codeBlockContent_biex\"><pre class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" tabindex=\"0\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#000000\"><span class=\"token plain\">\"System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\\nHuman: i dont like eating tasty things.\"</span><br/></span></code></pre><div class=\"buttonGroup__atx\"><button aria-label=\"Copy code to clipboard\" class=\"clean-btn\" title=\"Copy\" type=\"button\"><span aria-hidden=\"true\" class=\"copyButtonIcons_eSgA\"><svg class=\"copyButtonIcon_y97N\" viewbox=\"0 0 24 24\"><path d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\" fill=\"currentColor\"></path></svg><svg class=\"copyButtonSuccessIcon_LjdS\" viewbox=\"0 0 24 24\"><path d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\" fill=\"currentColor\"></path></svg></span></button></div></div></div></div></article><nav aria-label=\"Docs pages\" class=\"pagination-nav docusaurus-mt-lg\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/modules/model_io/prompts/\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Prompts</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/modules/model_io/prompts/composition\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Composition</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a class=\"table-of-contents__link toc-highlight\" href=\"#prompttemplate\"><code>PromptTemplate</code></a></li><li><a class=\"table-of-contents__link toc-highlight\" href=\"#chatprompttemplate\"><code>ChatPromptTemplate</code></a></li><li><a class=\"table-of-contents__link toc-highlight\" href=\"#lcel\">LCEL</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://discord.gg/cU2adEyC7w\" rel=\"noopener noreferrer\" target=\"_blank\">Discord<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://twitter.com/LangChainAI\" rel=\"noopener noreferrer\" target=\"_blank\">Twitter<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://github.com/langchain-ai/langchain\" rel=\"noopener noreferrer\" target=\"_blank\">Python<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://github.com/langchain-ai/langchainjs\" rel=\"noopener noreferrer\" target=\"_blank\">JS/TS<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://langchain.com\" rel=\"noopener noreferrer\" target=\"_blank\">Homepage<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li><li class=\"footer__item\"><a class=\"footer__link-item\" href=\"https://blog.langchain.dev\" rel=\"noopener noreferrer\" target=\"_blank\">Blog<svg aria-hidden=\"true\" class=\"iconExternalLink_nPIU\" height=\"13.5\" viewbox=\"0 0 24 24\" width=\"13.5\"><path d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\" fill=\"currentColor\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright  2024 LangChain, Inc.</div></div></div></footer></div>\n",
       "<script src=\"/assets/js/runtime~main.f00f11f4.js\"></script>\n",
       "<script src=\"/assets/js/main.ee2bb98c.js\"></script>\n",
       "</body>\n",
       "</html>>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = str(soup).strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.replace(\" \\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main content  \\n LangChainDocsUse casesIntegrationsGuidesAPI  \\nMore  \\nVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  \\nChat  \\n  \\nLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  \\nSearch  \\nLangServeLangGraph  \\nGet started  \\nIntroductionInstallationQuickstartSecurity  \\nLangChain Expression Language  \\nGet startedWhy use LCELInterfaceLangChain Expression Language (LCEL)  \\nHow to  \\nCookbook  \\nModules  \\nChains  \\nModel I/O  \\nModel I/OQuickstartConcepts  \\nPrompts  \\nQuick StartCompositionExample selectorsFew-shot prompt templatesFew-shot examples for chat modelsTypes of `MessagePromptTemplate`Partial prompt templatesPipeline  \\nExample Selector Types  \\nChat Models  \\nLLMs  \\nOutput Parsers  \\nRetrieval  \\nAgents  \\nMore  \\nLangSmith  \\nModulesModel I/OPromptsQuick Start  \\nOn this page  \\nQuick StartPromptTemplate\\u200bChatPromptTemplate\\u200bLCEL\\u200b\"),\n",
       " Document(page_content='Prompt templates are predefined recipes for generating prompts for language models.  \\nA template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.  \\nLangChain provides tooling to create and work with prompt templates.  \\nLangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.  \\nTypically, language models expect the prompt to either be a string or else a list of chat messages.', metadata={'Header 1': 'Quick Start'}),\n",
       " Document(page_content='Use PromptTemplate to create a template for a string prompt.  \\nBy default, PromptTemplate uses Pythons str.format syntax for templating.  \\nfrom langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template( \"Tell me a {adjective} joke about {content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")  \\n\\'Tell me a funny joke about chickens.\\'  \\nThe template supports any number of variables, including no variables:  \\nfrom langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke\")prompt_template.format()  \\n\\'Tell me a joke\\'  \\nYou can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.', metadata={'Header 1': 'Quick Start', 'Header 2': 'PromptTemplate\\u200b'}),\n",
       " Document(page_content='The prompt to chat models is a list of chat messages.  \\nEach chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.  \\nCreate a chat prompt template like this:  \\nfrom langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), (\"human\", \"Hello, how are you doing?\"), (\"ai\", \"I\\'m doing well, thanks!\"), (\"human\", \"{user_input}\"), ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")  \\nChatPromptTemplate.from_messages accepts a variety of message representations.  \\nFor example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage.  \\nfrom langchain.prompts import HumanMessagePromptTemplatefrom langchain_core.messages import SystemMessagefrom langchain_openai import ChatOpenAIchat_template = ChatPromptTemplate.from_messages( [ SystemMessage( content=( \"You are a helpful assistant that re-writes the user\\'s text to \" \"sound more upbeat.\" ) ), HumanMessagePromptTemplate.from_template(\"{text}\"), ])messages = chat_template.format_messages(text=\"I don\\'t like eating tasty things\")print(messages)  \\n[SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\"I don\\'t like eating tasty things\")]  \\nThis provides you with a lot of flexibility in how you construct your chat prompts.', metadata={'Header 1': 'Quick Start', 'Header 2': 'ChatPromptTemplate\\u200b'}),\n",
       " Document(page_content='PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.  \\nPromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.  \\nprompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})prompt_val  \\nStringPromptValue(text=\\'Tell me a joke\\')  \\nprompt_val.to_string()  \\n\\'Tell me a joke\\'  \\nprompt_val.to_messages()  \\n[HumanMessage(content=\\'Tell me a joke\\')]  \\nchat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})  \\nchat_val.to_messages()  \\n[SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\\'i dont like eating tasty things.\\')]  \\nchat_val.to_string()  \\n\"System: You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\\\\nHuman: i dont like eating tasty things.\"', metadata={'Header 1': 'Quick Start', 'Header 2': 'LCEL\\u200b'}),\n",
       " Document(page_content='Previous  \\nPrompts  \\nNext  \\nComposition  \\nPromptTemplateChatPromptTemplateLCEL  \\nCommunity  \\nDiscordTwitter  \\nGitHub  \\nPythonJS/TS  \\nMore  \\nHomepageBlog  \\nCopyright  2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(s)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in html_header_splits:\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main content   LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat    LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  LangServeLangGraph  Get started  IntroductionInstallationQuickstartSecurity  LangChain Expression Language  Get startedWhy use LCELInterfaceLangChain Expression Language (LCEL)  How to  Cookbook  Modules  Chains  Model I/O  Model I/OQuickstartConcepts  Prompts  Quick StartCompositionExample selectorsFew-shot prompt templatesFew-shot examples for chat modelsTypes of `MessagePromptTemplate`Partial prompt templatesPipeline  Example Selector Types  Chat Models  LLMs  Output Parsers  Retrieval  Agents  More  LangSmith  ModulesModel I/OPromptsQuick Start  On this page  Quick StartPromptTemplate\\u200bChatPromptTemplate\\u200bLCEL\\u200b\"),\n",
       " Document(page_content='Prompt templates are predefined recipes for generating prompts for language models.  A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.  LangChain provides tooling to create and work with prompt templates.  LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.  Typically, language models expect the prompt to either be a string or else a list of chat messages.', metadata={'Header 1': 'Quick Start'}),\n",
       " Document(page_content='Use PromptTemplate to create a template for a string prompt.  By default, PromptTemplate uses Pythons str.format syntax for templating.  from langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template( \"Tell me a {adjective} joke about {content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")  \\'Tell me a funny joke about chickens.\\'  The template supports any number of variables, including no variables:  from langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke\")prompt_template.format()  \\'Tell me a joke\\'  You can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.', metadata={'Header 1': 'Quick Start', 'Header 2': 'PromptTemplate\\u200b'}),\n",
       " Document(page_content='The prompt to chat models is a list of chat messages.  Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.  Create a chat prompt template like this:  from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), (\"human\", \"Hello, how are you doing?\"), (\"ai\", \"I\\'m doing well, thanks!\"), (\"human\", \"{user_input}\"), ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")  ChatPromptTemplate.from_messages accepts a variety of message representations.  For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage.  from langchain.prompts import HumanMessagePromptTemplatefrom langchain_core.messages import SystemMessagefrom langchain_openai import ChatOpenAIchat_template = ChatPromptTemplate.from_messages( [ SystemMessage( content=( \"You are a helpful assistant that re-writes the user\\'s text to \" \"sound more upbeat.\" ) ), HumanMessagePromptTemplate.from_template(\"{text}\"), ])messages = chat_template.format_messages(text=\"I don\\'t like eating tasty things\")print(messages)  [SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\"I don\\'t like eating tasty things\")]  This provides you with a lot of flexibility in how you construct your chat prompts.', metadata={'Header 1': 'Quick Start', 'Header 2': 'ChatPromptTemplate\\u200b'}),\n",
       " Document(page_content='PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.  PromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.  prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})prompt_val  StringPromptValue(text=\\'Tell me a joke\\')  prompt_val.to_string()  \\'Tell me a joke\\'  prompt_val.to_messages()  [HumanMessage(content=\\'Tell me a joke\\')]  chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})  chat_val.to_messages()  [SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\\'i dont like eating tasty things.\\')]  chat_val.to_string()  \"System: You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\\\\nHuman: i dont like eating tasty things.\"', metadata={'Header 1': 'Quick Start', 'Header 2': 'LCEL\\u200b'}),\n",
       " Document(page_content='Previous  Prompts  Next  Composition  PromptTemplateChatPromptTemplateLCEL  Community  DiscordTwitter  GitHub  PythonJS/TS  More  HomepageBlog  Copyright  2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "docs.extend(text_documents)\n",
    "docs.extend(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"all right let's get started this is the opening eye prompt engineering page before we can start building with it though we need to set things some things up so I've created a f a fresh uh virtual environment what I'm going to do is I'm going to set up a l serve uh uh template a l or application with this the reason that I'm going to be using lay serve is it'll make it easy to deploy this once I finish creating so the first thing that I'm going to do is bootstrap a link serve project I'm going to do that with the Lang chain CLI I'm installing it here after it finishes installing I'm going to create a new app I will call it open AI prompter I am not going to add a package because I'm going to be creating my own I can then go inside it and if I open it up the main thing that I'm interested in is this Lang serve server right here which wraps around fast API and will make it really easy to deploy my Lang chain chain the last thing I'm going to do for setup is set up laying Smith so laying Smith is in uh private beta if you don't have access to it and you're seeing this and you want to follow along shoot me a DM on LinkedIn or Twitter and I will uh get you access to it um what it will make it really easy to do is debug as we go along and this will involve a lot of prompter engineering and so we'll see how that becomes very very helpful um someone in a previous video commented that I should go over how to set this up more so you can go to your projects let's even create a new project um let's call this uh open AI prompter um that's all we can do let's then go to setup and then we can just export some API Keys here um so you can just copy this you can paste it here you then need to add in your API key you can do that let me let me move my face over here and let's move this over here as well you can do that by going to API keys and all right good it's not showing mine so you can create an API key and then put it there um I've already did this ahead of time um so mine's all\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"already did this ahead of time um so mine's all set up but if you needed to do that this is how you do that all right so let's go back to this prompt engineering guide the first thing we're going to do we're just going to copy it all because we are going to be using this in our prompt to help us write better prompts let's paste it into a file and then I'm going to I'm going to create my chain in a Jupiter notebook and then I'm going to put it into uh and then I'm going to put it into this app and so the reason that I'm doing this there's uh there's there's one real reason and one fake reason the one real reason is that a lot this will it' be a pretty iterative process I'm going to do some prompt engineering some of that I'll do in Lang Smith but some of that I'll I'll do in uh The Notebook as well and we'll see why um and so having a notebook like environment is really really helpful because it's an iterative environment and I can really easy iterate the other reason I want to do this is I want to show how it's really easy to create a chain and then just export it from a notebook most of the time it's it's pretty simple so let's save this and now I've got this jup notebook let's load um what I just put in there all right so let's do that let's import uh we can print out the head I guess just to see it let's import some stuff from L chain that we're going to want so from L chain core. prompts import template from L core do output parsers string output parser from L chain Community do chat models import chat open AI so I'm importing three important things this is going pretty simple chain that I'm writing we'll see maybe we'll get more complex but it should just be pretty simple because I'm just going to use a model that has a long context window and can work with all these instructions so I'm importing a prompt template this will help me uh structure the inputs to the model I'm importing an output parser that'll basically just convert it from a message format which\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"just convert it from a message format which is how the newer models respond the chat messes respond with uh message but I I really want to set it's string so I'm getting that and then I'm importing chat opening app which is the L chain wrapper around the open AI models which are the ones that I'll be using now is the fun part where I'm going to write my template for creating or my template that's going to help me create a chain that can take in an objective and write a good prompt so let's add this there um and okay so so this is actually an interesting point that I'll get to later but basically what I'm doing right here is I'm going to have these instructions as a variable that I'll pass into the prompt and I'll I'll go into that more later but for now let's just assume that's what we're going to do and then let's add just some delimiters here and then let's say based on the above instructions help me write a good prompt I want a prompt that okay so what I actually want to write is not actually a prompt but I want to write a prompt template because I think most people write prompt templates because they want to write prompt templates that then they can use in their code or at least most linkchain users want to write prompt templates help me write a good prompt template this template should be a python s string it can take in any number of variables depending on my objective return your answer in the following format this is my objective this seems like a decent first pass I don't know we'll see um and then we're going to create the chain okay so as I mentioned we have this thing here um which I'm going to pass in the instructions to let's see what would happen if we didn't do that and if we just did like this which you could also easily do so if I did this I would have prompt equals prompt template from template template chain um equals prompt Shadow AI string up the parser chain. invoke and now I'm going to pass in objective what should my objective be um answer\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"objective what should my objective be um answer B answer a question based on a based on context provided and only on that context so this is a pretty typical rag prompt or retrieval augmented generation prompt you want the the language model to respond to a user question based on the context provided um so let's do this and I get ke because basically what's happening is in these instructions there's other um there's other curly brackets which aren't variables that I want to format they're just part of the text and so this often happens when you're working with code or something like that and we've got a a lot of questions about how to deal with it so my preferred way to deal with it is to do what I'd done earlier treat the the text that has these unwanted curly brackets as input variables and then okay so so one way to do this is you may be thinking okay so that means that I now have to provide text as text here and this is a little bit annoying because now you have two input variables it's not the end of the world but it's a little bit annoying and if if this chain is part of a larger chain then then it starts to get more and more complicated more and more annoying um and ah I had that should be taex this is erroring because the model's not long enough I could do some fancy rag um it's just like barely over the context window and I'm I'm going to want to use this model anyways because this task is kind of hard so I want a good model to do it um so this will just get me around that and then let me also set temperature equals to zero while I'm at it so I could do it this way I could pass in the text here um and this would work let's see what answer it gives um for the next time I'll use streaming um so we can start seeing what's going on earlier um all right it's writing long thing while while it's working okay so let's um this is a little unreadable um because it's not super readable what I'm going to do is all right so this is Lang Smith this is me debugging I\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"so this is Lang Smith this is me debugging I want to see the output um this is a all right so this is better output um I get uh oh interesting okay so all right so it's giving me some instructions on how to use it um I actually don't know if this is correct because of these curly brackets but anyways um what we can do is uh uh sorry back to this what we can do there's a lot I want to show off but first things we can do is partial The Prompt so we can now do this and that means we no longer have to pass this in so we're basically just partialing the prompt we're passing in some input variables ahead of time before we even construct the train and then because we know those are always going to be fixed and they're and they shouldn't be variables so we're going to pass them in and then we can do this I'm going to now change this to streaming for Chunk in or more accurately for token in chain. stream print token um all right I need to add something like this all right so there we go it's not exactly um what I really want I really want like just one I really just want this thing um templ should be a python see if this helps so it's not helping a ton I can go back in here and I can I can start doing some prompt engineering here if I want so one thing that I can do it's not really that helpful here because this is a really simple um chain but if I wanted to I could open up the playground and it's also not helpful here because my prompt is really really long and this isn't the grass oh I can okay that's slightly better so now I have this um and um let's let's just do like bunch of new that thing based on the above instructions help me write a good prompt template this prompt template should be a string that can be formatted as if a python fstring I can take in any number of variables depending on my objective um I can now run I can now run this okay okay that's pretty good all right so that's pretty good I'm I'm I'm relatively happy with this so um let's uh take this from\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"happy with this so um let's uh take this from the playground let's bring it back in the notebook 24 being a bit slow okay so it's generally doing pretty good I'm relatively happy with this I might do a little bit more prompt engineering by the time this gets out but I'm relatively happy with this for now so what I'm going to do is I'm going to move it into Lang serve now I'm going to deploy it with Lang serve so I am going to create a new file in here I am going to basically copy this chain over that's done I'm going to go into the server um so I'm going to edit this add the chain you want to add um from app. chain import chain chain path prompter cool um I'm going to go back here and let's try it out and see what happens L chain serve doesn't find the file that is right so L chain notebooks open AI prom what did I name it prompting oh right let specify that let's try serving it again and I'm going to poetry add open AI because I'm going to deploy this project later um and this will add it to the Pi Project file so it's going to save that um let's try it again cool so now if I go to Local Host 8,000 prompter playground I get this let's try it out I take this add it here all right so I get back some streaming um I get back some nice stuff okay so there's definitely some prompt engineering that I want to do um it's doing this um Let me let me pause the video a bit do some prompt engineering and then resume it in a second all right so I'm back with a little bit of a better prompt um and I will uh uh I I'll include links to all the code so I won't read out here basically now if I go here and I type in an objective this gets me um this gets me a pretty solid prompt that I'd use for for retrievable augmented generation so so that's basically how we get to something that's deployed with Ling serf um this is still all local and so for the final part of this I'll walk through how to do this um on laying Smith so that you can share it with other people as you see so I have\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"share it with other people as you see so I have my my personal tenant my personal tenant doesn't have access to uh laying serve deployments yet on Lang Smith so this is uh this is an alpha feature that we're still testing um it's it's it's only available to a few people um if you are interested in being an alpha tester please let us know um or please let me know and uh we are not letting a ton of people off because we want we do want to make some improvements um but as we let more and more people off um you know we'll we'll will uh let me know and and you can be one of them in order to show you briefly what it looks like I am uh going to switch to the Lang chain account I'm going to go to deployments um and you'll see once it loads that I have a bunch of deployments here these deployments are all connected to GitHub um so one thing that I'm going to do after this is I'm going to pause set up a GitHub um uh repo with this code so that I can easily deploy it um so let me pause and do that and then I'll come back and walk through a new deployment and see what happens all right so we're back so I'm going to create a new deployment I'm going to click here I'm going to choose this repo that I created open AI Auto prompter name it uh open AI prompting helper um I'm going to add some environment variables um namely I'm going to add open AI API key um I'm going to make it a secret G to pause while I put this in all right it's in um I automatically get a tracing project for this so that's a big benefit of deploying on Lang Smith is it automatically connects to everything else in Lang Smith um right now that's tracing we'll make that testing um and and prompts and other things in the short term um I can now submit this and it will spin up a deployment um and so here is this uh uh deployment as you can see it's uh it'll take a little bit so I'm going to pause and come back when it's finished deploying all right we're back our deployment succeeded we can now open the playground\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"succeeded we can now open the playground for this deployment so we can go here task um what what was the task we had way back in the day so let's just use the same task here we can see it streaming awesome so we get this result um one thing we can do is we can give thumbs up or thumbs down I like this one so I'm going to click thumbs up if I go back here I can see it's only one Trace count but I can see the traces for this so here this is the most recent Trace I can click into it this is a tracing from this is the same tracing that's in uh uh langing Smith as you've been using it so that's one of the big benefits as it comes with all this stuff it comes with feedback automatically hooked up so we have a score here um and yeah that's uh basically it for the video now hopefully this was pretty helpful in terms of getting um a pretty simple chain it's you know it's just a a prompt a model and an Alpa parer up and running using lell uh getting around some uh prompt uh annoyances with all the formatting of the curly brackets shwing how to set up langing Smith showing how to set up a laying serve project and then showing uh the new laying serve deployment feature as well hopefully you guys enjoyed this let me know in the comments have a good one\", metadata={'source': '/home/dpvj/git/assistant/studynote/transcript.txt'}),\n",
       " Document(page_content=\"Skip to main content   LangChainDocsUse casesIntegrationsGuidesAPI  More  VersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos  Chat    LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs  Search  LangServeLangGraph  Get started  IntroductionInstallationQuickstartSecurity  LangChain Expression Language  Get startedWhy use LCELInterfaceLangChain Expression Language (LCEL)  How to  Cookbook  Modules  Chains  Model I/O  Model I/OQuickstartConcepts  Prompts  Quick StartCompositionExample selectorsFew-shot prompt templatesFew-shot examples for chat modelsTypes of `MessagePromptTemplate`Partial prompt templatesPipeline  Example Selector Types  Chat Models  LLMs  Output Parsers  Retrieval  Agents  More  LangSmith  ModulesModel I/OPromptsQuick Start  On this page  Quick StartPromptTemplate\\u200bChatPromptTemplate\\u200bLCEL\\u200b\"),\n",
       " Document(page_content='Prompt templates are predefined recipes for generating prompts for language models.  A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.  LangChain provides tooling to create and work with prompt templates.  LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.  Typically, language models expect the prompt to either be a string or else a list of chat messages.', metadata={'Header 1': 'Quick Start'}),\n",
       " Document(page_content='Use PromptTemplate to create a template for a string prompt.  By default, PromptTemplate uses Pythons str.format syntax for templating.  from langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template( \"Tell me a {adjective} joke about {content}.\")prompt_template.format(adjective=\"funny\", content=\"chickens\")  \\'Tell me a funny joke about chickens.\\'  The template supports any number of variables, including no variables:  from langchain.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke\")prompt_template.format()  \\'Tell me a joke\\'  You can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.', metadata={'Header 1': 'Quick Start', 'Header 2': 'PromptTemplate\\u200b'}),\n",
       " Document(page_content='The prompt to chat models is a list of chat messages.  Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.  Create a chat prompt template like this:  from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), (\"human\", \"Hello, how are you doing?\"), (\"ai\", \"I\\'m doing well, thanks!\"), (\"human\", \"{user_input}\"), ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")  ChatPromptTemplate.from_messages accepts a variety of message representations.  For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage.  from langchain.prompts import HumanMessagePromptTemplatefrom langchain_core.messages import SystemMessagefrom langchain_openai import ChatOpenAIchat_template = ChatPromptTemplate.from_messages( [ SystemMessage( content=( \"You are a helpful assistant that re-writes the user\\'s text to \" \"sound more upbeat.\" ) ), HumanMessagePromptTemplate.from_template(\"{text}\"), ])messages = chat_template.format_messages(text=\"I don\\'t like eating tasty things\")print(messages)  [SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\"I don\\'t like eating tasty things\")]  This provides you with a lot of flexibility in how you construct your chat prompts.', metadata={'Header 1': 'Quick Start', 'Header 2': 'ChatPromptTemplate\\u200b'}),\n",
       " Document(page_content='PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.  PromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.  prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})prompt_val  StringPromptValue(text=\\'Tell me a joke\\')  prompt_val.to_string()  \\'Tell me a joke\\'  prompt_val.to_messages()  [HumanMessage(content=\\'Tell me a joke\\')]  chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})  chat_val.to_messages()  [SystemMessage(content=\"You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\"), HumanMessage(content=\\'i dont like eating tasty things.\\')]  chat_val.to_string()  \"System: You are a helpful assistant that re-writes the user\\'s text to sound more upbeat.\\\\nHuman: i dont like eating tasty things.\"', metadata={'Header 1': 'Quick Start', 'Header 2': 'LCEL\\u200b'}),\n",
       " Document(page_content='Previous  Prompts  Next  Composition  PromptTemplateChatPromptTemplateLCEL  Community  DiscordTwitter  GitHub  PythonJS/TS  More  HomepageBlog  Copyright  2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Embeddings & Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vector = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "  docs,\n",
    "  embedding=embeddings,\n",
    "  persist_directory='./data'\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all right let's get started this is the opening eye prompt engineering page before we can start building with it though we need to set things some things up so I've created a f a fresh uh virtual environment what I'm going to do is I'm going to set up a l serve uh uh template a l or application with this the reason that I'm going to be using lay serve is it'll make it easy to deploy this once I finish creating so the first thing that I'm going to do is bootstrap a link serve project I'm going to do that with the Lang chain CLI I'm installing it here after it finishes installing I'm going to create a new app I will call it open AI prompter I am not going to add a package because I'm going to be creating my own I can then go inside it and if I open it up the main thing that I'm interested in is this Lang serve server right here which wraps around fast API and will make it really easy to deploy my Lang chain chain the last thing I'm going to do for setup is set up laying Smith so laying Smith is in uh private beta if you don't have access to it and you're seeing this and you want to follow along shoot me a DM on LinkedIn or Twitter and I will uh get you access to it um what it will make it really easy to do is debug as we go along and this will involve a lot of prompter engineering and so we'll see how that becomes very very helpful um someone in a previous video commented that I should go over how to set this up more so you can go to your projects let's even create a new project um let's call this uh open AI prompter um that's all we can do let's then go to setup and then we can just export some API Keys here um so you can just copy this you can paste it here you then need to add in your API key you can do that let me let me move my face over here and let's move this over here as well you can do that by going to API keys and all right good it's not showing mine so you can create an API key and then put it there um I've already did this ahead of time um so mine's all set up but if you needed to do that this is how you do that all right so let's go back to this prompt engineering guide the first thing we're going to do we're just going to copy it all because we are going to be using this in our prompt to help us write better prompts let's paste it into a file and then I'm going to I'm going to create my chain in a Jupiter notebook and then I'm going to put it into uh and then I'm going to put it into this app and so the reason that I'm doing this there's uh there's there's one real reason and one fake reason the one real reason is that a lot this will it' be a pretty iterative process I'm going to do some prompt engineering some of that I'll do in Lang Smith but some of that I'll I'll do in uh The Notebook as well and we'll see why um and so having a notebook like environment is really really helpful because it's an iterative environment and I can really easy iterate the other reason I want to do this is I want to show how it's really easy to create a chain and then just export it from a notebook most of the time it's it's pretty simple so let's save this and now I've got this jup notebook let's load um what I just put in there all right so let's do that let's import uh we can print out the head I guess just to see it let's import some stuff from L chain that we're going to want so from L chain core. prompts import template from L core do output parsers string output parser from L chain Community do chat models import chat open AI so I'm importing three important things this is going pretty simple chain that I'm writing we'll see maybe we'll get more complex but it should just be pretty simple because I'm just going to use a model that has a long context window and can work with all these instructions so I'm importing a prompt template this will help me uh structure the inputs to the model I'm importing an output parser that'll basically just convert it from a message format which is how the newer models respond the chat messes\n"
     ]
    }
   ],
   "source": [
    "query = \"What is lPrompt Engineering\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can provide tools and frameworks for implementing automated testing of software applications. This can help in quickly and efficiently running tests and identifying any issues or bugs in the code.\\n\\n2. Test Case Management: Langsmith can offer solutions for managing and organizing test cases, making it easier for QA teams to create, execute, and track test cases for different scenarios.\\n\\n3. Performance Testing: Langsmith can assist in conducting performance testing to evaluate the speed, responsiveness, and stability of software applications under various conditions.\\n\\n4. Test Data Management: Langsmith can provide tools for managing test data, ensuring that the right data is available for testing different aspects of the software.\\n\\n5. Continuous Integration and Continuous Testing: Langsmith can support the implementation of continuous integration and continuous testing processes, enabling developers to quickly identify and fix issues in the code.\\n\\nOverall, Langsmith can help in streamlining the testing process, improving the efficiency and effectiveness of testing efforts, and ensuring the quality and reliability of software applications.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame the LLM object\n",
    "llm = ChatOpenAI(model=model, verbose=True)\n",
    "llm.invoke(\"how can langsmith help with testing?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        Follow the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        Follow the guidelines provided in the style for reference on how to summarize the note.\n",
    "\n",
    "        <style>\n",
    "        {style}\n",
    "        </style>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"user\", \n",
    "        \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Prompt Engineering\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response answer: <style>\n",
      "        \n",
      "    # TITLE: \n",
      "    - Prompt Engineering\n",
      "    \n",
      "    ## ABSTRACT: \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models, including instructions, few-shot examples, and specific context and questions. LangChain provides tooling for creating and working with prompt templates, aiming to create model agnostic templates for easy reuse across different language models.\n",
      "    \n",
      "    ## KEY POINTS:\n",
      "    - Prompt templates: Predefined recipes for generating prompts for language models.\n",
      "    - LangChain tooling: Provides tools for creating and working with prompt templates.\n",
      "    - Model agnostic templates: Designed for easy reuse across different language models.\n",
      "    - Prompt structure: Language models expect the prompt to be either a string or a list of chat messages.\n",
      "    \n",
      "    ## CONTEXT \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models. These templates include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. The goal is to create model agnostic templates to make it easy to reuse existing templates across different language models. Language models expect the prompt to either be a string or a list of chat messages.\n",
      "    - The document discusses the process of prompt engineering and the use of LangChain CLI to set up a virtual environment and create an application called \"open AI prompter.\" The author mentions the use of LangChain CLI to bootstrap a Lang serve project and the importance of using LangSmith for debugging during prompt engineering. The author also highlights the iterative nature of prompt engineering and the ease of creating and exporting a chain from a notebook. The document further details the deployment of the prompter using Lang serve and the process of sharing it with others using LangSmith.\n",
      "    \n",
      "    ## REFLECTIONS\n",
      "    - How can prompt engineering contribute to the efficiency of language models?\n",
      "    - What are the advantages of using model agnostic prompt templates?\n",
      "    - How does the use of LangChain CLI simplify the process of creating prompt templates?\n",
      "    \n",
      "    * The document provides a comprehensive overview of prompt engineering and the tools provided by LangChain for creating and working with prompt templates. It emphasizes the importance of model agnostic templates and the iterative nature of prompt engineering. Additionally, it highlights the deployment process using Lang serve and the sharing capabilities of LangSmith. Overall, it offers valuable insights into the practical application of prompt engineering in language model development.\n",
      "    \n",
      "        </style>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response answer: {response['answer']}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f2aac22c790>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n        You are a world class note taking assistant.\\n        You summarize notes into concise manner summarizing the most relevant information.\\n        Follow the information in given in the text, do not make things up - unless you are asked for examples.\\n        Follow the guidelines provided in the style for reference on how to summarize the note.\\n\\n        <style>\\n        \\n    # TITLE: \\n    - As per user provided Title.\\n    \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate 3 to 5 questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n    \\n        </style>\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Use the following document to summarize into a note under the specified title\\n        the note must follow a specified styleand  specified style:\\n        \\n        <document>\\n        {context}\\n        </document>\\n        \\n        title: {input}'))])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f2aac22cca0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f2aac22f070>, openai_api_key='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', openai_proxy='')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(retriever, document_chain) \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChatPromptTemplate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m]),\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "chain = create_retrieval_chain(retriever, document_chain) | output_parser\n",
    "chain.invoke({\"input\": \"ChatPromptTemplate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperSeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing by providing a comprehensive and efficient testing framework. Here are some ways in which Langsmith can help with testing:\\n\\n1. Test Automation: Langsmith enables the automation of tests by providing a testing framework that allows developers to write tests in a clear and concise manner. This framework supports various testing methodologies, such as unit testing, integration testing, and end-to-end testing.\\n\\n2. Test Coverage: Langsmith helps in measuring the coverage of tests by providing tools and features to track which parts of the codebase have been tested. This ensures that all critical areas of the application are thoroughly tested, reducing the chances of undiscovered bugs.\\n\\n3. Test Orchestration: Langsmith allows developers to easily manage and run tests across different environments and configurations. It provides a way to define test suites, group related tests, and execute them in parallel or sequentially, improving the testing process's efficiency.\\n\\n4. Test Reporting: Langsmith generates detailed and comprehensive test reports, highlighting the test results, including pass/fail status, execution time, and any errors or exceptions encountered. These reports help in identifying and debugging issues quickly.\\n\\n5. Mocking and Stubbing: Langsmith provides mechanisms for creating mock objects or stubs, allowing developers to isolate components for testing. This is particularly useful when testing code that relies on external dependencies, as it enables developers to simulate their behavior, leading to more reliable and independent tests.\\n\\n6. Performance Testing: Langsmith offers tools and utilities for performance testing, enabling developers to evaluate the system's performance under different load conditions. This helps identify potential bottlenecks and optimize the application for better scalability and responsiveness.\\n\\n7. Continuous Integration/Continuous Delivery (CI/CD) Integration: Langsmith seamlessly integrates with CI/CD pipelines, allowing tests to be automatically triggered on code changes or deployments. This ensures that tests are run consistently and continuously, providing immediate feedback on the code's quality.\\n\\nOverall, Langsmith enhances the testing process by providing a robust framework, tools, and utilities that facilitate test automation, coverage analysis, test orchestration, reporting, mocking, and performance testing. These capabilities help ensure the reliability, stability, and quality of the software being developed.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can greatly assist with testing in several ways:\\n\\n1. Test Case Generation: Langsmith can automatically generate test cases based on the specifications and requirements of your software. It uses natural language processing techniques to understand the desired behavior of the system and generate test cases that cover various scenarios and edge cases. This can save significant time and effort in test case creation.\\n\\n2. Test Data Generation: Langsmith can also generate realistic and diverse test data for your software. It understands the data requirements and constraints of your system and creates test data that covers different data types, ranges, and combinations. This can help in ensuring thorough testing and identifying potential issues related to data handling.\\n\\n3. Test Automation: Langsmith can generate test scripts or code snippets in various programming languages to automate the execution of test cases. It can integrate with popular testing frameworks and tools to streamline the automation process. This can greatly accelerate the testing process and improve test coverage.\\n\\n4. Test Documentation: Langsmith can assist in creating comprehensive test documentation. It can generate test plans, test scripts, test reports, and other relevant documentation based on the identified test cases. This ensures that the testing process is well-documented and easily understandable for all stakeholders.\\n\\n5. Test Oracles: Langsmith can help in defining oracles for automated testing. An oracle is a mechanism to determine whether the actual behavior of the system matches the expected behavior. Langsmith can understand the expected outcomes of various functionalities and generate oracles to validate the test results automatically.\\n\\nOverall, Langsmith can enhance the efficiency and effectiveness of testing by automating test case generation, test data generation, test script creation, and test documentation. It can also improve the quality of testing by ensuring comprehensive coverage and accurate oracles.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f8e84927340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8e6bf61d80>, model_name='gpt-3.5-turbo-1106', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \\n\\ntext: ```{text}```\\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"You are a study note taking assistant for courses.\n",
    "\n",
    "Given the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \n",
    "\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_style = \"\"\"\n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_text = document.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ntext: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\\n\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studynote_messages = prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)\n",
    "studynote_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_response = chat(studynote_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ABSTRACT:\n",
       "The document discusses the Lang chain expression language, which allows for writing minimalist code to build chains within line chain. It emphasizes the advanced features of parallel execution, async, and streaming using the expression language. The document also explores the syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## KEY POINTS:\n",
       "- Lang chain expression language for building minimalist code to create chains within line chain.\n",
       "- Advanced features include parallel execution, async, and streaming.\n",
       "- Syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## CONTEXT:\n",
       "- The Lang chain expression language allows for writing minimalist code to build chains within line chain, making it easier to use advanced features like parallel execution, async, and streaming.\n",
       "- The pipe operator is used to string components together, making the code simpler and more flexible.\n",
       "- The runnable lambdas are used to wrap functions and create custom operations within the expression language.\n",
       "- The document provides examples of using the expression language to retrieve information in parallel and modify the output using runnable lambdas.\n",
       "- The expression language has pros such as minimalist code and advanced features, but also cons like increased abstraction and non-standard syntax.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the Lang chain expression language compare to other methods of building chains within line chain?\n",
       "- What are the potential use cases for the runnable lambdas within the expression language?\n",
       "- The expression language offers a minimalist approach and advanced features, but also introduces increased abstraction and non-standard syntax."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(studynote_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_schema = ResponseSchema(name=\"python_code\",\n",
    "                            description=\"Parse any code within the text and write the code in string with backticks. \\\n",
    "                                If there was no code found, then output as -1.\")\n",
    "webpage_schema = ResponseSchema(name=\"webpage_link\",\n",
    "                                    description=\"Was there any webpage links recommended. \\\n",
    "                                    If this information is not found, output -1.\")\n",
    "reading_schema = ResponseSchema(name=\"further_reading\",\n",
    "                                    description=\"Extract any recommendations on further research or reading on the subject. \\\n",
    "                                    Output them as a comma separated Python list. If none is recommended, output -1.\")\n",
    "\n",
    "response_schemas = [python_schema, \n",
    "                    webpage_schema,\n",
    "                    reading_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\\n\\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\\n\\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\\n}\\n```'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
      "\n",
      "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
      "\n",
      "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "\n",
      "text: today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\n",
      "\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\n",
      "\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studynote_text = document.page_content\n",
    "\n",
    "template_string2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
    "\n",
    "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
    "\n",
    "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "dict_prompt = ChatPromptTemplate.from_template(template=template_string2)\n",
    "\n",
    "dict_messages = dict_prompt.format_messages(text=studynote_text, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "print(dict_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_response = chat(messages)\n",
    "\n",
    "dict_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_code': -1,\n",
       " 'webpage_link': -1,\n",
       " 'further_reading': 'Lang chain expression language, Line chain products, Line chain abstraction'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "#extract dict key gift\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# Vector Data Memory\n",
    "# Entity Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": studynote_messages}, {\"output\": studynote_response})\n",
    "memory.save_context({\"input\": dict_messages}, {\"output\": dict_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\\nAI: ```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\n",
      "AI: ```json\n",
      "{\n",
      "\t\"python_code\": -1,\n",
      "\t\"webpage_link\": -1,\n",
      "\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\n",
      "}\n",
      "```\n",
      "Human: Hi, what is langchain about?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lang chain is a powerful expression language that allows for running retrievers in parallel and modifying the output using runnable lambdas. It has a specific syntax and is worth learning and experimenting with. It is used for line chain products and line chain abstraction.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is langchain about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)  \n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lang Chain Expression Language company'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = studynote_messages\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 1: input= text and output= summary\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Come up with a summary for the following text:\"\n",
    "    \"\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= search queries\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate some additional web search queries based on the summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"search_queries\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= topic\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a one line topic based summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, \n",
    "                     output_key=\"topic\"\n",
    "                    )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"search_queries\",\"topic\"],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\",\n",
       " 'summary': 'The text discusses the Lang chain expression language, which allows for minimalist code to build chains within Lang chain. It explores the advanced features of parallel execution, async, and streaming using the expression language. The text explains the syntax and shows examples of how to use the expression language with a practical use case. It also covers the pros and cons of using the expression language and provides a detailed explanation of how it works. Overall, the text provides a comprehensive overview of the Lang chain expression language and its potential benefits and drawbacks.',\n",
       " 'search_queries': '1. How to use Lang chain expression language for parallel execution?\\n2. What are the practical use cases for Lang chain expression language?\\n3. Pros and cons of using Lang chain expression language for async execution\\n4. What are the advanced features of Lang chain expression language?\\n5. Examples of minimalist code using Lang chain expression language\\n6. How does streaming work in Lang chain expression language?\\n7. Comparison of Lang chain expression language with other expression languages\\n8. Is Lang chain expression language suitable for large-scale applications?\\n9. Tips for optimizing code using Lang chain expression language\\n10. How to get started with Lang chain expression language programming?',\n",
       " 'topic': 'An in-depth exploration of the Lang chain expression language and its advanced features, syntax, practical use cases, and pros and cons.'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = document.page_content\n",
    "overall_chain(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over a larger document - Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working right now - need to revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/home/dpvj/SecondBrain/templates/Meeting Note Template.md\")\n",
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/pydantic/_migration.py:276: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wnat are the key points of the document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- [ ]  Follow-up meetings scheduled\\n- [ ]  Next steps and responsibilities assigned\\n\\n## Decisions\\n_List any decisions made during the meeting_\\n\\n## Next Steps\\n_Outline the next steps and responsibilities_\\n\\n## Follow-up\\n_Summarize any follow-up actions required_\\n\\n## Additional Notes\\n_Any additional notes or information related to the meeting_\\n\\n## Meeting Adjourned\\n_Time the meeting was adjourned_\\n\\n## Next Meeting\\n_Date, time, and agenda for the next meeting_'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.call_as_llm(f\"{qdocs} Question: Please list all the headers of the documents\") \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Multifunction Notetaking Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_format = studynote_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetingnote_format = \"\"\"\n",
    "## Agenda\n",
    "_Summarize the agenda of the meeting_\n",
    "_Ensure key stakeholders are participating & Leading_\n",
    "\n",
    "## Goals\n",
    "_What do we want to achieve from this meeting_\n",
    "_Align with why the meeting was called in the first place_\n",
    "\n",
    "## Discussion notes\n",
    "_Write the notes that are key to the goals & objectives, note who has said it_\n",
    "\n",
    "## Action items\n",
    "_Summarize the action items_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"studynote\", \n",
    "        \"description\": \"Good for taking notes for a course\", \n",
    "        \"prompt_template\": studynote_template,\n",
    "        \"prompt_style\": studynote_format\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meetingnote\",  # Changed from \"math\" to \"meetingnote\"\n",
    "        \"description\": \"Good for taking notes for a meeting\", \n",
    "        \"prompt_template\": meetingnote_template,\n",
    "        \"prompt_style\": meetingnote_format\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\nYou are expected to take notees for a course.\\n\\nGiven the course delimeted by tripple backticks, extract information into a study note following the format: \\n\\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ncourse: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\")]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains['studynote'].prompt.format_messages(format=studynote_style, text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:97\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:511\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    512\u001b[0m         _output_key\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    517\u001b[0m         _output_key\n\u001b[1;32m    518\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:92\u001b[0m, in \u001b[0;36mMultiRouteChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     91\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m---> 92\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouter_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mdestination) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mnext_inputs), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m route\u001b[38;5;241m.\u001b[39mdestination:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:40\u001b[0m, in \u001b[0;36mRouterChain.route\u001b[0;34m(self, inputs, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Route:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Route inputs to a destination chain.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        a Route object\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Route(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:61\u001b[0m, in \u001b[0;36mLLMRouterChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     57\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     58\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m     60\u001b[0m     Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:114\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m raised following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrap_text(url: str):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>begin this chain now I'm going to</td>\n",
       "      <td>612.959</td>\n",
       "      <td>6.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>implement it with this we will see that</td>\n",
       "      <td>615.880</td>\n",
       "      <td>6.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>line chain actually uses I think they</td>\n",
       "      <td>619.720</td>\n",
       "      <td>3.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>use</td>\n",
       "      <td>622.160</td>\n",
       "      <td>3.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>invoke so rather than call they would</td>\n",
       "      <td>623.360</td>\n",
       "      <td>5.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>what we we had earlier so yeah we have</td>\n",
       "      <td>1212.159</td>\n",
       "      <td>5.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>these two functions let's take those</td>\n",
       "      <td>1214.400</td>\n",
       "      <td>4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>okay we can see runable it's what we</td>\n",
       "      <td>1217.320</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>were doing before so that we could use</td>\n",
       "      <td>1219.240</td>\n",
       "      <td>5.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>this let's do it again</td>\n",
       "      <td>1221.280</td>\n",
       "      <td>5.879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text     start  duration\n",
       "226        begin this chain now I'm going to   612.959     6.761\n",
       "227  implement it with this we will see that   615.880     6.280\n",
       "228    line chain actually uses I think they   619.720     3.640\n",
       "229                                      use   622.160     3.480\n",
       "230    invoke so rather than call they would   623.360     5.919\n",
       "..                                       ...       ...       ...\n",
       "449   what we we had earlier so yeah we have  1212.159     5.161\n",
       "450     these two functions let's take those  1214.400     4.840\n",
       "451     okay we can see runable it's what we  1217.320     3.960\n",
       "452   were doing before so that we could use  1219.240     5.319\n",
       "453                   this let's do it again  1221.280     5.879\n",
       "\n",
       "[228 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = YouTubeTranscriptApi.get_transcripts([video_id], languages=['en'])\n",
    "ls = list(transcripts[0].values())[0]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(ls)\n",
    "\n",
    "filtered_df = df[(df['start'] >= 611) & (df['start'] <= 1224)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\n",
      "\n",
      "Key Links:\n",
      "Code from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
      "LangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\n",
      "GPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "soup = BeautifulSoup(requests.get('https://www.youtube.com/watch?v=DjuXACWYkkU').content)\n",
    "pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'AIzaSyDYyXnayylCG2L1ToqrZykiVA--QxZ7-3Y'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=video_id\n",
    ")\n",
    "response = request.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#videoListResponse',\n",
       " 'etag': 'VB1pOZ9dsyaJCmlUHZOx-KOEwbk',\n",
       " 'items': [{'kind': 'youtube#video',\n",
       "   'etag': 'mFwyljJthlBCHJcUCN2cbl3du5U',\n",
       "   'id': 'DjuXACWYkkU',\n",
       "   'snippet': {'publishedAt': '2023-11-16T14:35:01Z',\n",
       "    'channelId': 'UCC-lyoTfSrcJzA1ab3APAgw',\n",
       "    'title': 'Building a Research Assistant from Scratch',\n",
       "    'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher',\n",
       "    'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/default.jpg',\n",
       "      'width': 120,\n",
       "      'height': 90},\n",
       "     'medium': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/mqdefault.jpg',\n",
       "      'width': 320,\n",
       "      'height': 180},\n",
       "     'high': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/hqdefault.jpg',\n",
       "      'width': 480,\n",
       "      'height': 360},\n",
       "     'standard': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/sddefault.jpg',\n",
       "      'width': 640,\n",
       "      'height': 480},\n",
       "     'maxres': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/maxresdefault.jpg',\n",
       "      'width': 1280,\n",
       "      'height': 720}},\n",
       "    'channelTitle': 'LangChain',\n",
       "    'categoryId': '22',\n",
       "    'liveBroadcastContent': 'none',\n",
       "    'localized': {'title': 'Building a Research Assistant from Scratch',\n",
       "     'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher'}},\n",
       "   'contentDetails': {'duration': 'PT43M40S',\n",
       "    'dimension': '2d',\n",
       "    'definition': 'hd',\n",
       "    'caption': 'false',\n",
       "    'licensedContent': False,\n",
       "    'contentRating': {},\n",
       "    'projection': 'rectangular'},\n",
       "   'statistics': {'viewCount': '9045',\n",
       "    'likeCount': '307',\n",
       "    'favoriteCount': '0',\n",
       "    'commentCount': '27'}}],\n",
       " 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = response['items'][0]['snippet']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_date = response['items'][0]['snippet']['publishedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = response['items'][0]['statistics']['viewCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a note taking assistant for a courses. \\n    Given the following document, write key points.\\n    If the document is not relevant, write \"not relevant\".\\n    '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8TClnBZvafYSdsb9WiFpkNfbp1GOt', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1701971291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
