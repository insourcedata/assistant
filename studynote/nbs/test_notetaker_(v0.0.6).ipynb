{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base OAI Libraries & Environment Setup\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import html2text\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading up Langchain v0.1.0 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#New Libraries to be imported\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text, model=None):\n",
    "    if model == 'gpt-4':\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_youtube(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Convert to text\n",
    "        transcript = ' '.join([t['text'] for t in transcript])\n",
    "\n",
    "        # Create document\n",
    "        document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "\n",
    "        return document\n",
    "    except:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_input():\n",
    "    \n",
    "    yt_ids = []\n",
    "\n",
    "    while True:\n",
    "        yt_id = input(\"Enter a video_id (format 'KQjZ68mToWo') or 'Done': \")\n",
    "        if yt_id.lower() == 'done':\n",
    "            break\n",
    "        if len(yt_id) == 11 and yt_id.isalnum():\n",
    "            yt_ids.append(yt_id)\n",
    "        else:\n",
    "            print(\"Invalid format. Please enter a valid video_id.\")\n",
    "\n",
    "    return yt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_input():\n",
    "    urls = []\n",
    "\n",
    "    while True: \n",
    "        url = input(\"Enter a web address (format 'https://www.example.com') or 'done': \") \n",
    "        if url.lower() == 'done': \n",
    "            break\n",
    "        if url.startswith('http://') or url.startswith('https://'): \n",
    "            urls.append(url) \n",
    "        else: print(\"Invalid format. Please enter a valid web address.\")\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_loader(yt_ids):\n",
    "    \n",
    "    yt_docs = []\n",
    "    \n",
    "    for id in yt_ids:\n",
    "        loader = YoutubeLoader.from_youtube_url(f\"https://www.youtube.com/watch?v={id}\",\n",
    "        add_video_info=True,\n",
    "        language=[\"en\", \"id\"],\n",
    "        translation=\"en\",\n",
    "        )\n",
    "    \n",
    "        yt_docs.extend(RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50).split_documents(loader.load()))\n",
    "    \n",
    "    return yt_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_loader(urls):\n",
    "    web_docs = []\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\"),\n",
    "        (\"h4\", \"Header 4\"),\n",
    "        (\"h5\", \"Header 5\"),\n",
    "        (\"h6\", \"Header 6\"),\n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "        html_text = str(BeautifulSoup(requests.get(url).content, \"html.parser\"))\n",
    "        html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on).split_text(html_text)\n",
    "\n",
    "    for doc in html_splitter:\n",
    "        doc.page_content = doc.page_content.replace(\"\\n\", \"\")\n",
    "        \n",
    "    web_docs.extend(html_splitter)\n",
    "\n",
    "    return web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_loader(urls):\n",
    "    web_docs = []\n",
    "\n",
    "    for url in urls:\n",
    "        loader = WebBaseLoader(url)\n",
    "        web_doc = loader.load()\n",
    "        for doc in web_doc:\n",
    "            doc.page_content = doc.page_content.replace(\"\\n\", \" \")\n",
    "            doc.page_content = re.sub(r'[^\\x00-\\x7F]+',' ', doc.page_content)\n",
    "        \n",
    "        web_docs.extend(RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50).split_documents(web_doc))\n",
    "    \n",
    "    return web_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Setup - gpt-3.5-turbo-1106 is a chat model\n",
    "client = OpenAI()\n",
    "model=\"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read bookmarks.md file\n",
    "with open('/home/dpvj/git/assistant/studynote/bookmarks/bookmarks.md', 'r') as f:\n",
    "    bookmarks = f.read()\n",
    "\n",
    "bookmarks = bookmarks.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where there is youtube in the line, extract the video id\n",
    "yt_ids=[b.split('youtube.com')[1].split(')')[0].split('?v=')[-1][:11] for b in bookmarks if 'youtube.com' in b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvAPnpSfSGo',\n",
       " 'ro312jDqAh0',\n",
       " 'Uh9bYiVrW_s',\n",
       " 'mmBo8nlu2j0',\n",
       " 'O0dUOtOIrfs',\n",
       " '8OJC21T2SL4',\n",
       " 'wLRHwKuKvOE']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_docs = yt_loader(yt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yt_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2302\n",
      "2303\n",
      "1195\n",
      "2262\n",
      "614\n",
      "2257\n",
      "2295\n",
      "178\n",
      "2400\n",
      "1346\n",
      "2297\n",
      "2286\n",
      "384\n",
      "2184\n",
      "2354\n",
      "2308\n",
      "2345\n",
      "2309\n",
      "2388\n",
      "2317\n",
      "2325\n",
      "995\n",
      "2339\n",
      "2433\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "for doc in yt_docs:\n",
    "    print(get_num_tokens(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ongoing Problem: Cant get Youtube Chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking all urls that does not need login information - makeshift for now because I only excluded medium sites\n",
    "urls = [b.split(')')[0].split('(')[-1] for b in bookmarks if 'youtube.com' not in b and 'medium.com' not in b and 'betterprogramming.pub' not in b]\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = url_loader(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RAG |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookRAGOn this pageRAGLet s look at adding in a retrieval step to a prompt and LLM, which adds up to a  retrieval-augmented generation  chain%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())chain.invoke(\"where did harrison work?\")\\'Harrison worked at Kensho.\\'template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a Kensho.\\'Conversational Retrieval Chain We can easily add in conversation history. This primarily means adding in chat_message_historyfrom langchain.schema import format_documentfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_stringfrom langchain_core.runnables import RunnableParallelfrom langchain.prompts.prompt import PromptTemplate_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.Chat History:{chat_history}Follow Up Input: {question}Standalone question:\"\"\"CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"ANSWER_PROMPT = ChatPromptTemplate.from_template(template)DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")def _combine_documents(    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\\\n\\\\n\"):    doc_strings = [format_document(doc, document_prompt) for doc in docs]    return document_separator.join(doc_strings)_inputs = RunnableParallel(    standalone_question=RunnablePassthrough.assign(        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])    )    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),)_context = {    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,    \"question\": lambda x: x[\"standalone_question\"],}conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()conversational_qa_chain.invoke(    {        \"question\": \"where did harrison work?\",        \"chat_history\": [],    })AIMessage(content=\\'Harrison was employed at Kensho.\\')conversational_qa_chain.invoke(    {        \"question\": \"where did he work?\",        \"chat_history\": [            HumanMessage(content=\"Who wrote this notebook?\"),            AIMessage(content=\"Harrison\"),        ],    })AIMessage(content=\\'Harrison worked at Kensho.\\')With Memory and returning source documents This shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.from operator import itemgetterfrom langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(    return_messages=True, output_key=\"answer\", input_key=\"question\")# First we add a step to load memory# This adds a \"memory\" key to the input objectloaded_memory = RunnablePassthrough.assign(    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),)# Now we calculate the standalone questionstandalone_question = {    \"standalone_question\": {        \"question\": lambda x: x[\"question\"],        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),    }    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),}# Now we retrieve the documentsretrieved_documents = {    \"docs\": itemgetter(\"standalone_question\") | retriever,    \"question\": lambda x: x[\"standalone_question\"],}# Now we construct the inputs for the final promptfinal_inputs = {    \"context\": lambda x: _combine_documents(x[\"docs\"]),    \"question\": itemgetter(\"question\"),}# And finally, we do the part that returns the answersanswer = {    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),    \"docs\": itemgetter(\"docs\"),}# And now we put it all together!final_chain = loaded_memory | standalone_question | retrieved_documents | answerinputs = {\"question\": \"where did harrison work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison was employed at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}# Note that the memory does not save automatically# This will be improved in the future# For now you need to save it yourselfmemory.save_context(inputs, {\"answer\": result[\"answer\"].content})memory.load_memory_variables({}){\\'history\\': [HumanMessage(content=\\'where did harrison work?\\'),  AIMessage(content=\\'Harrison was employed at Kensho.\\')]}inputs = {\"question\": \"but where did he really work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison actually worked at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}PreviousPrompt + LLMNextMultiple chainsConversational Retrieval ChainWith Memory and returning source documentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en'}),\n",
       " Document(page_content='Chroma |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreComponentsLLMsChat modelsDocument loadersDocument transformersText embedding modelsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAstra DBAtlasAwaDBAzure Cosmos DBAzure AI SearchBagelDBBaidu Cloud ElasticSearch VectorSearchBigQuery Vector SearchChromaClarifaiClickHouseDashVectorDatabricks Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissFaiss (Async)Google Vertex AI Vector SearchSAP HANA Cloud Vector EngineHippoHologresJaguar Vector DatabaseKDB.AILanceDBLanternLLMRailsMarqoMeilisearchMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVecto.rsPGVectorPineconeQdrantRedisRocksetScaNNSemaDBSingleStoreDBscikit-learnSQLite-VSSStarRocksSupabase (Postgres)SurrealDBTairTencent Cloud VectorDBTigrisTileDBTimescale Vector (Postgres)TypesenseUSearchValdVearchVectaraVespaviking DBWeaviateXataYellowbrickZepZillizRetrieversToolsAgents and toolkitsMemoryCallbacksChat loadersAdaptersStoresComponentsVector storesChromaOn this pageChromaChroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.Install Chroma with:pip install chromadbChroma runs in various modes. See below for examples of each integrated with LangChain. - in-memory - in a python script or jupyter notebook - in-memory with persistance - in a script or notebook and save/load to disk - in a docker container - as a server running your local machine or in the cloudLike any other database, you can: - .add - .get - .update - .upsert - .delete - .peek - and .query runs the similarity search.View full docs at docs. To access these methods directly, you can do ._collection.method()Basic Example In this basic example, we take the most recent State of the Union Address, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.# importfrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.embeddings.sentence_transformer import (    SentenceTransformerEmbeddings,)from langchain_community.vectorstores import Chroma# load the document and split it into chunksloader = TextLoader(\"../../modules/state_of_the_union.txt\")documents = loader.load()# split it into chunkstext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)# create the open-source embedding functionembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")# load it into Chromadb = Chroma.from_documents(docs, embedding_function)# query itquery = \"What did the president say about Ketanji Brown Jackson\"docs = db.similarity_search(query)# print resultsprint(docs[0].page_content)/Users/jeff/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html  from .autonotebook import tqdm as notebook_tqdmTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Basic Example (including saving to disk) Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.Caution: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other s work. As a best practice, only have one client per path running at any given time.# save to diskdb2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")docs = db2.similarity_search(query)# load from diskdb3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)docs = db3.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Passing a Chroma Client into Langchain You can also create a Chroma Client and pass it to LangChain. This is particularly useful if you want easier access to the underlying database.You can also specify the collection name that you want LangChain to use.import chromadbpersistent_client = chromadb.PersistentClient()collection = persistent_client.get_or_create_collection(\"collection_name\")collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])langchain_chroma = Chroma(    client=persistent_client,    collection_name=\"collection_name\",    embedding_function=embedding_function,)print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Insert of existing embedding ID: 1Add of existing embedding ID: 2Insert of existing embedding ID: 2Add of existing embedding ID: 3Insert of existing embedding ID: 3There are 3 in the collectionBasic Example (using the Docker Container) You can also run the Chroma Server in a Docker container separately, create a Client to connect to it, and then pass that to LangChain.Chroma has the ability to handle multiple Collections of documents, but the LangChain interface expects one, so we need to specify the collection name. The default collection name used by LangChain is  langchain .Here is how to clone, build, and run the Docker Image:git clone git@github.com:chroma-core/chroma.gitEdit the docker-compose.yml file and add ALLOW_RESET=TRUE under environment    ...    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config log_config.yml    environment:      - IS_PERSISTENT=TRUE      - ALLOW_RESET=TRUE    ports:      - 8000:8000    ...Then run docker-compose up -d --build# create the chroma clientimport uuidimport chromadbfrom chromadb.config import Settingsclient = chromadb.HttpClient(settings=Settings(allow_reset=True))client.reset()  # resets the databasecollection = client.create_collection(\"my_collection\")for doc in docs:    collection.add(        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content    )# tell LangChain to use our client and collection namedb4 = Chroma(    client=client,    collection_name=\"my_collection\",    embedding_function=embedding_function,)query = \"What did the president say about Ketanji Brown Jackson\"docs = db4.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Update and Delete While building toward a real application, you want to go beyond adding data, and also update and delete data.Chroma has users provide ids to simplify the bookkeeping here. ids can be the name of the file, or a combined has like filename_paragraphNumber, etc.Chroma supports all these operations - though some of them are still being integrated all the way through the LangChain interface. Additional workflow improvements will be added soon.Here is a basic example showing how to do various operations:# create simple idsids = [str(i) for i in range(1, len(docs) + 1)]# add dataexample_db = Chroma.from_documents(docs, embedding_function, ids=ids)docs = example_db.similarity_search(query)print(docs[0].metadata)# update the metadata for a documentdocs[0].metadata = {    \"source\": \"../../modules/state_of_the_union.txt\",    \"new_value\": \"hello world\",}example_db.update_document(ids[0],', metadata={'source': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'title': 'Chroma | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chroma is a AI-native', 'language': 'en'}),\n",
       " Document(page_content='\"hello world\",}example_db.update_document(ids[0], docs[0])print(example_db._collection.get(ids=[ids[0]]))# delete the last documentprint(\"count before\", example_db._collection.count())example_db._collection.delete(ids=[ids[-1]])print(\"count after\", example_db._collection.count()){\\'source\\': \\'../../../state_of_the_union.txt\\'}{\\'ids\\': [\\'1\\'], \\'embeddings\\': None, \\'metadatas\\': [{\\'new_value\\': \\'hello world\\', \\'source\\': \\'../../../state_of_the_union.txt\\'}], \\'documents\\': [\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\']}count before 46count after 45Use OpenAI Embeddings Many people like to use OpenAIEmbeddings, here is how to set that up.# get a token: https://platform.openai.com/account/api-keysfrom getpass import getpassfrom langchain_openai import OpenAIEmbeddingsOPENAI_API_KEY = getpass()import osos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEYembeddings = OpenAIEmbeddings()new_client = chromadb.EphemeralClient()openai_lc_client = Chroma.from_documents(    docs, embeddings, client=new_client, collection_name=\"openai_collection\")query = \"What did the president say about Ketanji Brown Jackson\"docs = openai_lc_client.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Other Information Similarity search with score The returned distance score is cosine distance. Therefore, a lower score is better.docs = db.similarity_search_with_score(query)docs[0](Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'}), 1.1972057819366455)Retriever options This section goes over different options for how to use Chroma as a retriever.MMR In addition to using similarity search in the retriever object, you can also use mmr.retriever = db.as_retriever(search_type=\"mmr\")retriever.get_relevant_documents(query)[0]Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'})Filtering on metadata It can be helpful to narrow down the collection before working with it.For example, collections can be filtered on metadata using the get method.# filter collection for updated sourceexample_db.get(where={\"source\": \"some_other_source\"}){\\'ids\\': [], \\'embeddings\\': None, \\'metadatas\\': [], \\'documents\\': []}PreviousBigQuery Vector SearchNextClarifaiBasic ExampleBasic Example (including saving to disk)Passing a Chroma Client into LangchainBasic Example (using the Docker Container)Update and DeleteUse OpenAI EmbeddingsOther InformationSimilarity search with scoreRetriever optionsFiltering on metadataCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'title': 'Chroma | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chroma is a AI-native', 'language': 'en'}),\n",
       " Document(page_content=\"Summarization |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchUse casesQ&A with RAGSQLTool useInteracting with APIsChatbotsExtractionSummarizationTaggingWeb scrapingCode understandingSynthetic data generationGraph queryingUse casesSummarizationOn this pageSummarizationOpen In ColabUse case Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.LLMs are a great tool for this given their proficiency in understanding and synthesizing text.In this walkthrough we ll go over how to perform document summarization using LLMs.Overview A central question for building a summarizer is how to pass your documents into the LLM s context window. Two common approaches for this are:Stuff: Simply  stuff  all your documents into a single prompt. This is the simplest approach (see here for more on the StuffDocumentsChains, which is used for this method).Map-reduce: Summarize each document on it s own in a  map  step and then  reduce  the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).Quickstart To give you a sneak preview, either pipeline can be wrapped in a single object: load_summarize_chain.Suppose we want to summarize a blog post. We can create this in a few lines of code.First set environment variables and install packages:%pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8)Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0)Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4)Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299)Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0)Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1)Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5)Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3)Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12)Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2)Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1)Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2)Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4)Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1)Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1)Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0)Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1)Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3)Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9)Collecting tqdm (from openai)  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)       57.6/57.6 kB 2.7 MB/s eta 0:00:00Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0)Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0)Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1)Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20)Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1)Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3)Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9)Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33)Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42)Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5)Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3)Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0)Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4)Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0)Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.3)Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16)Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)Requirement already satisfied: h11>=0.8 in\", metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content='(8.1.7)Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2)Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)Installing collected packages: tqdm  Attempting uninstall: tqdm    Found existing installation: tqdm 4.64.1    Uninstalling tqdm-4.64.1:      Successfully uninstalled tqdm-4.64.1ERROR: pip\\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible.Successfully installed tqdm-4.66.1We can use chain_type=\"stuff\", especially if using larger context window models such as:16k token OpenAI gpt-3.5-turbo-1106100k token Anthropic Claude-2We can also supply chain_type=\"map_reduce\" or chain_type=\"refine\" (read more here).from langchain.chains.summarize import load_summarize_chainfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import ChatOpenAIloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")chain = load_summarize_chain(llm, chain_type=\"stuff\")chain.run(docs)\\'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.\\'Option 1. Stuff When we use load_summarize_chain with chain_type=\"stuff\", we will use the StuffDocumentsChain.The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM:from langchain.chains.combine_documents.stuff import StuffDocumentsChainfrom langchain.chains.llm import LLMChainfrom langchain.prompts import PromptTemplate# Define promptprompt_template = \"\"\"Write a concise summary of the following:\"{text}\"CONCISE SUMMARY:\"\"\"prompt = PromptTemplate.from_template(prompt_template)# Define LLM chainllm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")llm_chain = LLMChain(llm=llm, prompt=prompt)# Define StuffDocumentsChainstuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")docs = loader.load()print(stuff_chain.run(docs))The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems.Great! We can see that we reproduce the earlier result using the load_summarize_chain.Go deeper You can easily customize the prompt.You can easily try different LLMs, (e.g., Claude) via the llm parameter.Option 2. Map-Reduce Let s unpack the map reduce approach. For this, we ll first map each document to an individual summary using an LLMChain. Then we ll use a ReduceDocumentsChain to combine those summaries into a single global summary.First, we specify the LLMChain to use for mapping each document to an individual summary:from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChainfrom langchain.text_splitter import CharacterTextSplitterllm = ChatOpenAI(temperature=0)# Mapmap_template = \"\"\"The following is a set of documents{docs}Based on this list of docs, please identify the main themes Helpful Answer:\"\"\"map_prompt = PromptTemplate.from_template(map_template)map_chain = LLMChain(llm=llm, prompt=map_prompt)We can also use the Prompt Hub to store and fetch prompts.This will work with your LangSmith API key.For example, see the map prompt here.from langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")map_chain = LLMChain(llm=llm, prompt=map_prompt)The ReduceDocumentsChain handles taking the document mapping results and reducing them into a single output. It wraps a generic CombineDocumentsChain (like StuffDocumentsChain) but adds the ability to collapse documents before passing it to the CombineDocumentsChain if their cumulative size exceeds token_max. In this example, we can actually re-use our chain for combining our docs to also collapse our docs.So if the cumulative number of tokens in our mapped documents exceeds 4000 tokens, then we ll recursively pass in the documents in batches of \\\\< 4000 tokens to our StuffDocumentsChain to create batched summaries. And once those batched summaries are cumulatively less than 4000 tokens, we ll pass them all one last time to the StuffDocumentsChain to create the final summary.# Reducereduce_template = \"\"\"The following is set of summaries:{docs}Take these and distill it into a final, consolidated summary of the main themes. Helpful Answer:\"\"\"reduce_prompt = PromptTemplate.from_template(reduce_template)# Note we can also get this from the prompt hub, as noted abovereduce_prompt = hub.pull(\"rlm/map-prompt\")reduce_promptChatPromptTemplate(input_variables=[\\'docs\\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'docs\\'], template=\\'The following is a set of documents:\\\\n{docs}\\\\nBased on this list of docs, please identify the main themes \\\\nHelpful Answer:\\'))])# Run chainreduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)# Takes a list of documents, combines them into a single string, and passes this to an LLMChaincombine_documents_chain = StuffDocumentsChain(    llm_chain=reduce_chain, document_variable_name=\"docs\")# Combines and iteratively reduces the mapped documentsreduce_documents_chain = ReduceDocumentsChain(    # This is final chain that is called.    combine_documents_chain=combine_documents_chain,    # If documents exceed context for `StuffDocumentsChain`    collapse_documents_chain=combine_documents_chain,    # The maximum number of tokens to group documents into.    token_max=4000,)Combining our map and reduce chains into one:# Combining documents by mapping a chain over them, then combining resultsmap_reduce_chain = MapReduceDocumentsChain(    # Map chain    llm_chain=map_chain,    # Reduce chain    reduce_documents_chain=reduce_documents_chain,    # The variable name in the llm_chain to put the documents in    document_variable_name=\"docs\",    # Return the results of the map steps in the output    return_intermediate_steps=False,)text_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)Created a chunk of size 1003, which is longer than the specified 1000print(map_reduce_chain.run(split_docs))Based on the list of documents provided, the main themes can be identified as follows:1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver.2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent\\'s capabilities.3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results.4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods.5. Tool use: The documents highlight the agent\\'s ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information.6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of', metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content='of the capabilities and potential applications of such agents.7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents.8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources.Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges.Go deeper CustomizationAs shown above, you can customize the LLMs and prompts for map and reduce stages.Real-world use-caseSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!  The blog post and associated repo also introduce clustering as a means of summarization.This opens up a third path beyond the stuff or map-reduce approaches that is worth considering.Option 3. Refine Refine is similar to map-reduce:The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.This can be easily run with the chain_type=\"refine\" specified.chain = load_summarize_chain(llm, chain_type=\"refine\")chain.run(split_docs)\\'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\\'It s also possible to supply a prompt and return intermediate steps.prompt_template = \"\"\"Write a concise summary of the following:{text}CONCISE SUMMARY:\"\"\"prompt = PromptTemplate.from_template(prompt_template)refine_template = (    \"Your job is to produce a final summary\\\\n\"    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"    \"We have the opportunity to refine the existing summary\"    \"(only if needed) with some more context below.\\\\n\"    \"------------\\\\n\"    \"{text}\\\\n\"    \"------------\\\\n\"    \"Given the new context, refine the original summary in Italian\"    \"If the context isn\\'t useful, return the original summary.\")refine_prompt = PromptTemplate.from_template(refine_template)chain = load_summarize_chain(    llm=llm,    chain_type=\"refine\",    question_prompt=prompt,    refine_prompt=refine_prompt,    return_intermediate_steps=True,    input_key=\"input_documents\",    output_key=\"output_text\",)result = chain({\"input_documents\": split_docs}, return_only_outputs=True)print(result[\"output_text\"])Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit  di contesto che limita l\\'inclusione di informazioni storiche dettagliate e la difficolt  di pianificazione a lungo termine e decomposizione delle attivit . Inoltre, l\\'affidabilit  dell\\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti   incerta, poich  i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci , il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\\'ambito degli agenti autonomi.print(\"\\\\n\\\\n\".join(result[\"intermediate_steps\"][:3]))This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.Splitting and summarizing in a single chain For convenience, we can wrap both the text splitting of our long document and summarizing in a single AnalyzeDocumentsChain.from langchain.chains import AnalyzeDocumentChainsummarize_document_chain = AnalyzeDocumentChain(    combine_docs_chain=chain, text_splitter=text_splitter)summarize_document_chain.run(docs[0].page_content)ValueError: `run` not supported when there is not exactly one output key. Got [\\'output_text\\', \\'intermediate_steps\\'].PreviousExtractionNextTaggingUse caseOverviewQuickstartOption 1. StuffGo deeperOption 2. Map-ReduceGo deeperOption 3. RefineSplitting and summarizing in a single chainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content=\"Chains |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.LCEL is great for constructing your own chains, but it s also nice to have chains that you can use off-the-shelf. There are two types of off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are rather standalone classes.We are working creating methods that create LCEL versions of all chains. We are doing this for a few reasons.Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.LCEL Chains Below is a table of all LCEL chain constructors. In addition, we report on:Chain ConstructorThe constructor function for this chain. These are all methods that return LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnable If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnable If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generates queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains Below we report on the legacy chain types that exist. We will maintain support for these until we are able to create a LCEL alternative. We report on:ChainName of the chain, or name of the constructor method. If constructor method, this will return a Chain subclass.Function CallingWhether this requires OpenAI Function Calling.Other ToolsOther tools used in the chain.When to UseOur commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respondOpenAPIEndpointChainOpenAPI SpecSimilar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpointsConversationalRetrievalChainRetrieverThis chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.ReduceDocumentsChainThis chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. Useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do in parallel.MapReduceDocumentsChainThis chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).MapRerankDocumentsChainThis calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this when you want to enforce that a chain s answer follows some principles.LLMChainElasticsearchDatabaseChainElasticSearch InstanceThis chain converts a natural language question to an ElasticSearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elastic Search databaseFlareChainThis implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.ArangoGraphQAChainArango GraphThis chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs an Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.KuzuQAChainKuzu GraphThis chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NeptuneOpenCypherQAChainNeptune GraphThis chain constructs an Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphSparqlChainGraph that works with SparQLThis chain constructs an SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.LLMMathThis chain converts a user question to a math problem and then executes it (using numexpr)LLMCheckerChainThis chain uses a second LLM call to varify its initial answer. Use this when you to have an extra layer of validation on the initial LLM call.LLMSummarizationCheckerThis chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost).create_citation_fuzzy_match_chain Uses OpenAI function calling to answer questions and cite its sources.create_extraction_chain Uses OpenAI Function calling to extract information from text.create_extraction_chain_pydantic Uses OpenAI function calling to extract information from text into a Pydantic model. Compared to create_extraction_chain this has a tighter integration with Pydantic.get_openapi_chain OpenAPI SpecUses OpenAI function calling to query an OpenAPI.create_qa_with_structure_chain Uses OpenAI function calling to do question answering over text and respond in a specific format.create_qa_with_sources_chain Uses OpenAI function calling to answer questions with citations.QAGenerationChainCreates both questions and answers from documents. Can be used to generate question/answer pairs for evaluation of retrieval projects.RetrievalQAWithSourcesChainRetrieverDoes question answering over\", metadata={'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en'}),\n",
       " Document(page_content='question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over load_qa_with_sources_chain when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in).load_qa_with_sources_chainRetrieverDoes question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).RetrievalQARetrieverThis chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a respoinse.MultiPromptChainThis chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.MultiRetrievalQAChainRetrieverThis chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential options.load_summarize_chainLLMRequestsChainThis chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more generalPreviousTools as OpenAI FunctionsNext[Beta] MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en'}),\n",
       " Document(page_content='Prompt + LLM |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookPrompt + LLMOn this pagePrompt + LLMThe most common and valuable composition is taking:PromptTemplate / ChatPromptTemplate -> LLM / ChatModel -> OutputParserAlmost any other chains you build will use this building block.PromptTemplate + LLM The simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output.Note, you can mix and match PromptTemplate/ChatPromptTemplates and LLMs/ChatModels as you like here.%pip install  upgrade  quiet langchain langchain-openaifrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")model = ChatOpenAI()chain = prompt | modelchain.invoke({\"foo\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\", additional_kwargs={}, example=False)Often times we want to attach kwargs that ll be passed to each model call. Here are a few examples of that:Attaching Stop Sequences chain = prompt | model.bind(stop=[\"\\\\n\"])chain.invoke({\"foo\": \"bears\"})AIMessage(content=\\'Why did the bear never wear shoes?\\', additional_kwargs={}, example=False)Attaching Function Call information functions = [    {        \"name\": \"joke\",        \"description\": \"A joke\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},                \"punchline\": {                    \"type\": \"string\",                    \"description\": \"The punchline for the joke\",                },            },            \"required\": [\"setup\", \"punchline\"],        },    }]chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)chain.invoke({\"foo\": \"bears\"}, config={})AIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'joke\\', \\'arguments\\': \\'{\\\\n  \"setup\": \"Why don\\\\\\'t bears wear shoes?\",\\\\n  \"punchline\": \"Because they have bear feet!\"\\\\n}\\'}}, example=False)PromptTemplate + LLM + OutputParser We can also add in an output parser to easily transform the raw LLM/ChatModel output into a more workable formatfrom langchain_core.output_parsers import StrOutputParserchain = prompt | model | StrOutputParser()Notice that this now returns a string - a much more workable format for downstream taskschain.invoke({\"foo\": \"bears\"})\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"Functions Output Parser When you specify the function to return, you may just want to parse that directlyfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParserchain = (    prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonOutputFunctionsParser())chain.invoke({\"foo\": \"bears\"}){\\'setup\\': \"Why don\\'t bears like fast food?\", \\'punchline\\': \"Because they can\\'t catch it!\"}from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = (    prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke({\"foo\": \"bears\"})\"Why don\\'t bears wear shoes?\"Simplifying input To make invocation even simpler, we can add a RunnableParallel to take care of creating the prompt input dict for us:from langchain_core.runnables import RunnableParallel, RunnablePassthroughmap_ = RunnableParallel(foo=RunnablePassthrough())chain = (    map_    | prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke(\"bears\")\"Why don\\'t bears wear shoes?\"Since we re composing our map with another Runnable, we can even use some syntactic sugar and just use a dict:chain = (    {\"foo\": RunnablePassthrough()}    | prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke(\"bears\")\"Why don\\'t bears like fast food?\"PreviousCookbookNextRAGPromptTemplate + LLMAttaching Stop SequencesAttaching Function Call informationPromptTemplate + LLM + OutputParserFunctions Output ParserSimplifying inputCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser', 'title': 'Prompt + LLM | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'The most common and valuable composition is taking:', 'language': 'en'}),\n",
       " Document(page_content=\"Introduction |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphGet startedIntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by language models. It enables applications that:Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)This framework consists of several parts.LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.LangServe: A library for deploying LangChain chains as a REST API.LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.Together, these products simplify the entire application lifecycle:Develop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.Deploy: Turn any chain into an API with LangServe.LangChain Libraries The main value props of the LangChain packages are:Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or notOff-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasksOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.The LangChain libraries themselves are made up of several different packages.langchain-core: Base abstractions and LangChain Expression Language.langchain-community: Third party integrations.langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.Get started Here s how to install LangChain, set up your environment, and start building.We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.Read up on our Security best practices to make sure you're developing safely with LangChain.noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.LangChain Expression Language (LCEL) LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest  prompt + LLM  chain to the most complex chains.Overview: LCEL and its benefitsInterface: The standard interface for LCEL objectsHow-to: Key features of LCELCookbook: Example code for accomplishing common tasksModules LangChain provides standard, extendable interfaces and integrations for the following modules:Model I/O Interface with language modelsRetrieval Interface with application-specific dataAgents Let models choose which tools to use given high-level directivesExamples, ecosystem, and resources Use cases Walkthroughs and techniques for common end-to-end use cases, like:Document question answeringChatbotsAnalyzing structured dataand much more...Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.Guides Best practices for developing with LangChain.API reference Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.Developer's guide Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.Community Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM s.PreviousGet startedNextInstallationLangChain LibrariesGet startedLangChain Expression Language (LCEL)ModulesExamples, ecosystem, and resourcesUse casesIntegrationsGuidesAPI referenceDeveloper's guideCommunityCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.\", metadata={'source': 'https://python.langchain.com/docs/get_started/introduction', 'title': 'Introduction | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'LangChain is a framework for developing applications powered by language models. It enables applications that:', 'language': 'en'}),\n",
       " Document(page_content='Quickstart |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphGet startedQuickstartOn this pageQuickstartIn this quickstart we\\'ll show you how to:Get setup with LangChain, LangSmith and LangServeUse the most basic and common components of LangChain: prompt templates, models, and output parsersUse LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chainingBuild a simple application with LangChainTrace your application with LangSmithServe your application with LangServeThat\\'s a fair amount to cover! Let\\'s dive in.Setup Jupyter Notebook This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See here for instructions on how to install.Installation To install LangChain run:PipCondapip install langchainconda install langchain -c conda-forgeFor more details, see our Installation guide.LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:export LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"Building with LangChain LangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes an LLM to determine whether or not it needs to fetch data to answer questions. We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.LLM Chain For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.OpenAILocalFirst we\\'ll need to import the LangChain x OpenAI integration package.pip install langchain-openaiAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export OPENAI_API_KEY=\"...\"We can then initialize the model:from langchain_openai import ChatOpenAIllm = ChatOpenAI()If you\\'d prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:from langchain_openai import ChatOpenAIllm = ChatOpenAI(openai_api_key=\"...\")Ollama allows you to run open-source large language models, such as Llama 2, locally.First, follow these instructions to set up and run a local Ollama instance:DownloadFetch a model via ollama pull llama2Then, make sure the Ollama server is running. After that, you can do:from langchain_community.llms import Ollamallm = Ollama(model=\"llama2\")Once you\\'ve installed and initialized the LLM of your choice, we can try using it! Let\\'s ask it what LangSmith is - this is something that wasn\\'t present in the training data so it shouldn\\'t have a very good response.llm.invoke(\"how can langsmith help with testing?\")We can also guide it\\'s response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are world class technical documentation writer.\"),    (\"user\", \"{input}\")])We can now combine these into a simple LLM chain:chain = prompt | llm We can now invoke it and ask the same question. It still won\\'t know the answer, but it should respond in a more proper tone for a technical writer!chain.invoke({\"input\": \"how can langsmith help with testing?\"})The output of a ChatModel (and therefore, of this chain) is a message. However, it\\'s often much more convenient to work with strings. Let\\'s add a simple output parser to convert the chat message to a string.from langchain_core.output_parsers import StrOutputParseroutput_parser = StrOutputParser()We can now add this to the previous chain:chain = prompt | llm | output_parserWe can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).chain.invoke({\"input\": \"how can langsmith help with testing?\"})Diving Deeper We\\'ve now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.Retrieval Chain In order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. This requires installing BeautifulSoup:```shellpip install beautifulsoup4After that, we can import and use WebBaseLoader.from langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")docs = loader.load()Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.For embedding models, we once again provide examples for accessing via OpenAI or via local models.OpenAILocalMake sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()Make sure you have Ollama running (same set up as with the LLM).from langchain_community.embeddings import OllamaEmbeddingsembeddings = OllamaEmbeddings()Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity\\'s sake.First we need to install the required packages for that:pip install faiss-cpuThen we can build our index:from langchain_community.vectorstores import FAISSfrom langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)vector = FAISS.from_documents(documents, embeddings)Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.First, let\\'s set up the chain that takes a question and the retrieved documents and generates an answer.from langchain.chains.combine_documents import create_stuff_documents_chainprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:<context>{context}</context>Question: {input}\"\"\")document_chain = create_stuff_documents_chain(llm, prompt)If we wanted to, we could run this ourselves by passing in documents directly:from langchain_core.documents import Documentdocument_chain.invoke({    \"input\": \"how can langsmith help with testing?\",    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]})However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.from langchain.chains import create_retrieval_chainretriever = vector.as_retriever()retrieval_chain = create_retrieval_chain(retriever, document_chain)We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer keyresponse = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})print(response[\"answer\"])# LangSmith offers several features that can help with testing:...This answer should be much more accurate!Diving Deeper We\\'ve now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.Conversation Retrieval Chain The chain we\\'ve created so far can only answer single questions. One of the main', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'}),\n",
       " Document(page_content='can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?We can still use the create_retrieval_chain function, but we need to change two things:The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.The final LLM chain should likewise take the whole history into accountUpdating RetrievalIn order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.from langchain.chains import create_history_aware_retrieverfrom langchain_core.prompts import MessagesPlaceholder# First we need a prompt that we can pass into an LLM to generate this search queryprompt = ChatPromptTemplate.from_messages([    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")])retriever_chain = create_history_aware_retriever(llm, retriever, prompt)We can test this out by passing in an instance where the user is asking a follow up question.from langchain_core.messages import HumanMessage, AIMessagechat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retriever_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.prompt = ChatPromptTemplate.from_messages([    (\"system\", \"Answer the user\\'s questions based on the below context:\\\\n\\\\n{context}\"),    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),])document_chain = create_stuff_documents_chain(llm, prompt)retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)We can now test this out end-to-end:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retrieval_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})We can see that this gives a coherent answer - we\\'ve successfully turned our retrieval chain into a chatbot!Agent We\\'ve so far create examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access two tools:The retriever we just created. This will let it easily answer questions about LangSmithA search tool. This will let it easily answer questions that require up to date information.First, let\\'s set up a tool for the retriever we just created:from langchain.tools.retriever import create_retriever_toolretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)The search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:export TAVILY_API_KEY=...If you do not want to set up an API key, you can skip creating this tool.from langchain_community.tools.tavily_search import TavilySearchResultssearch = TavilySearchResults()We can now create a list of the tools we want to work with:tools = [retriever_tool, search]Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent\\'s Getting Started documentationInstall langchain hub firstpip install langchainhubNow we can use it to get a predefined promptfrom langchain_openai import ChatOpenAIfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutor# Get the prompt to use - you can modify this!prompt = hub.pull(\"hwchase17/openai-functions-agent\")llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})We can ask it about the weather:agent_executor.invoke({\"input\": \"what is the weather in SF?\"})We can have conversations with it:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]agent_executor.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})Diving Deeper We\\'ve now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.Serving with LangServe Now that we\\'ve built an application, we need to serve it. That\\'s where LangServe comes in. LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\\'ll show how you can deploy your app with LangServe.While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.Install with:pip install \"langserve[all]\"Server To create a server for our application we\\'ll make a serve.py file. This will contain our logic for serving our application. It consists of three things:The definition of our chain that we just built aboveOur FastAPI appA definition of a route from which to serve the chain, which is done with langserve.add_routes#!/usr/bin/env pythonfrom typing import Listfrom fastapi import FastAPIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.vectorstores import FAISSfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.tools.retriever import create_retriever_toolfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langchain_openai import ChatOpenAIfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutorfrom langchain.pydantic_v1 import BaseModel, Fieldfrom langchain_core.messages import BaseMessagefrom langserve import add_routes# 1. Load Retrieverloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")docs = loader.load()text_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)embeddings = OpenAIEmbeddings()vector = FAISS.from_documents(documents, embeddings)retriever = vector.as_retriever()# 2. Create Toolsretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)search = TavilySearchResults()tools = [retriever_tool, search]# 3. Create Agentprompt = hub.pull(\"hwchase17/openai-functions-agent\")llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)# 4. App definitionapp = FastAPI(  title=\"LangChain Server\",  version=\"1.0\",  description=\"A simple API server using LangChain\\'s Runnable interfaces\",)# 5. Adding chain route# We need to add these input/output schemas because the current AgentExecutor# is lacking in schemas.class Input(BaseModel):    input: str    chat_history: List[BaseMessage] = Field(        ...,        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},    )class Output(BaseModel):    output: stradd_routes(    app,    agent_executor.with_types(input_type=Input, output_type=Output),    path=\"/agent\",)if __name__ == \"__main__\":    import uvicorn    uvicorn.run(app, host=\"localhost\", port=8000)And that\\'s it! If we execute this file:python serve.pywe should see our chain being served at localhost:8000.Playground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.Client Now let\\'s set up a client for programmatically interacting with our service. We can easily do this with the [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain as if it were running client-side.from langserve import RemoteRunnableremote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")remote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})To learn more about the many other features of LangServe head here.Next steps We\\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey, we recommend you read the following (in order):All of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'}),\n",
       " Document(page_content='to better understand how to create custom chains.Model IO covers more details of prompts, LLMs, and output parsers.Retrieval covers more details of everything related to retrievalAgents covers details of everything related to agentsExplore common end-to-end use cases and template applicationsRead up on LangSmith, the platform for debugging, testing, monitoring and moreLearn more about serving your applications with LangServePreviousInstallationNextSecuritySetupJupyter NotebookInstallationLangSmithBuilding with LangChainLLM ChainDiving DeeperRetrieval ChainDiving DeeperConversation Retrieval ChainAgentDiving DeeperServing with LangServeServerPlaygroundClientNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'})]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496\n",
      "2260\n",
      "1255\n",
      "3056\n",
      "2374\n",
      "1806\n",
      "2031\n",
      "343\n",
      "1122\n",
      "938\n",
      "2143\n",
      "2242\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "for doc in web_docs:\n",
    "    print(get_num_tokens(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loader & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "docs.extend(yt_docs)\n",
    "docs.extend(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hey everyone this is Harrison from Lang  chain here I wanted to do a video on  Lang graph which is a new library we  released um pretty recently and how you  can use l graph to create some  multi-agent workflows so for those of  you who aren't caught up on L graph we  have a whole series of videos going  through it and it's basically a way to  dynamically  create agent-like workflows as graphs so  what do I mean by that by an agent-like  workflow I I generally mean running a  language model in a loop in a variety of  ways um and there's different structure  and how you might want to run that  language model in a loop and so L graph  is useful for defining that structure  and specifically it allows for the  definition of Cycles because this is a  loop after all um another way to think  about this um is thinking about it as a  way to define kind of like State  machines um and and so a state machine  machine and a labeled directed graph are  pretty similar and that you have  different nodes and if if you think of  this as a state machine then a node  would be a state and if you think of it  um as as uh uh kind of like in this  multi-agent workflow then that node  state is an agent um and then you have  these edges between them and so in a  graph that's an edge in a state machine  that is uh you know transition  probabilities and when you think of  these multi-agent workflows then those  are basically how do these agents  communicate to each other so we'll think  about building multi-agent workflows  using L graph the nodes are the agents  the edges are how they communicate um if  you haven't already checked out L graph  again highly recommend that you check  out the this the video that we have at a  high level the syntax looks like this  it's very similar to network X you  define a graph that tracks some State  over time you then add nodes into that  graph you then Define edges between  those nodes and then you can use it um  like you would uh any other um L chain  chain so today I want to talk about  three different variants of multi-agent  workflows that we're going to use laying  graph to create so the first one is what  we're calling multi-agent  collaboration and so this is largely  defined as you have multiple agents  working on the same state of messages so  when you think about multiple agents  collaborating they can either kind of  like share State between them or they  can be kind of like independent and work  on their own and then maybe pass like  final responses to the other and so in a  multi-agent collaboration they share the  state and so specifically this this  example that we've set up it has two  different agents and these are basically  prompts plus llms so there's a prompts  plus an llm for a researcher and there's  a prompt plus an llm for a chart  generator they each have a few different  tools that they can call but basically  the way that it works is we first call  the researcher uh node or the researcher  agent we get a message  and then from there there's three  different ways that it can go one if the  researcher says to call a function then  we call that tool and then we go back to  the researcher two if the researcher  says a message that that says final  answer so we in the prompt we say like  if you're done say final answer so if  the researcher says final answer then it  returns to the user and then three if it  just sends a normal message there's no  tool calls and there's no final answer  then we let the chart generator take a  look at the state that's accumulated  um and kind of uh respond after that so  we're going to we're going to walk  through what this looks like um we're  first going to set up everything we've  need I've already done this um  importantly we're going to be using Lang  Smith here to kind of like track the the  multi-agents and see what's going on for  for a really good debugging experience  um Lang Smith right now is in private  beta if you don't have access shoot me a  DM on LinkedIn or Twitter and we can get  you access so the first thing we're  going to do do is we're going to Define  this helper function that creates an  agent so basically we're going to create  uh these two kind of like agents up here  um and these agents are parameterized by  an llm the tools they have and a system  message and so then we're taking kind of  like uh we're basically creating this  mini chain this very simple mini chain  that is a prompt which has uh the system  message in it um and then it's a it's a  call to the llm and it has access to  some tool so let's define this helper  function we're then going to define the  tools that we want the agents to be able  to call so we'll have one Search tool  we'll use tavil search for that and then  we'll have one python tool that can run  python  code um now we can start to create our  graph so first we're going to define the  state of what we want to track so we  want to track the messages and so again  like each agent will add a message to  this messages property and then we also  want to track who the most recent sender  is and the reason reason we need this  info is because after we call this tool  we're going to check who the sender was  and we're going to return to them so  this is just some some State that's  useful to track over  time um so let's define this state um  we're now going to Define some agent  nodes so here we've created a little  helper function to create an agent node  and so specifically this agent node is  going to uh take in state agent name  it's going to call the agent on the  state and it's then going to look at the  uh uh result we need to do a little bit  of kind of like converting here so the  agent is going to respond with an AI  message but when we add that to the  messages States that's accumulated over  time we want to represent that as a  human message actually um because we  basically want the next AI that sees  this to kind of like work with that and  so we're going to if it's a function  message then we're going to represent it  as a function message because it will  just say a function message otherwise  we're going to cast it to a human  message  and we're going to give it a specific  name we're going to give it the name of  the agent so that it knows basically is  this message that it sees again because  we're keeping track of This Global State  and so in the global State when it sees  a human message is it from this  researcher or is it from this charge  generator um and so we're going to do  that and we're going to create two nodes  we're going to create this research  agent node um by basically partialing  out the agent node function um and then  we're going to create this chart  generator  node  we're now going to define the tool node  this is the node that just runs the  tools it basically looks at the most  recent message it loads the tool  arguments it kind of it will call it it  will call the tool executor the tool  executor is just a simple wrapper around  tools that makes it easy to call them  and then it will convert the response  into a function message and append that  um to the messages  property and now we need the edge logic  so this is the this is the uh big router  logic here um which basically defines uh  some of the logic of where to go  so we look at the messages we look at  the last message if there's a function  call in the last message then we go to  the call tool or then we return call  tool and we'll see what that leads to  later on if there's final answer in the  last message then we return end um  otherwise we return continue and so  let's see how we use this in the graph  so first we create this graph let me run  this m first first we create this graph  with the agent State then we add the  three nodes that we have the research  node the chart node and the tool node um  we then add this conditional Edge so the  researcher after we call the researcher  um it uses the router logic if if the  router returns continue then we go to  chart generator so if the researcher  returns basically something that doesn't  have a tool and something that isn't  final answer then we return continue in  which case we go to the chart generator  if the router returns call tool then we  call the tool and if it returns end then  we end similar with the chart generator  um and then we add this conditional Edge  after call tool so after we call the  tool we basically look at who the sender  is and if the sender is the researcher  then we go to the researcher if the  sender is the charge generator then we  go to the chart generator and we set the  entry point to be the researcher  node now we can invoke it um we'll use  the stream method so that we can see  things being printed out um we'll pass  in this human message asking it to to  fetch the UK's GDP over the past 5 years  then draw a line graph of it um and then  after we code it finish it um and we'll  run this it'll take a little bit so I'll  probably pause the video here or I'll  kick off the I'll kick it off pause it  come back when it's finished and then we  can see what it looks like in Lang Smith  as  well all right so it's finished so it  was streaming out so we can take a look  at the whole stream of things in here if  I can figure out how to expand it so we  can see the researcher um you know the  first thing it does is it queries UK GDP  data for the past 5 years and then calls  a tool um and then it kind of keeps on  going um at some point there's this  chart so you know it PL plots it over  the past uh three years it looks like so  it didn't quite get all the all the data  it needs so you can see uh yeah you can  see it says that the data for years 2018  2019 is missing um and so it plots that  and then this is just the final response  so that's why it shows up so big if we  want to see this in a little bit nicer  view because this is a lot to digest we  can go to Lang Smith um and so Lang  Smith uh at the top of the\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 4632, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"Smith um and so Lang  Smith uh at the top of the notebook I  sent I set it so that it would log to  this specific project the multi-agent  collaboration I can see that I've got  one run can see some basic stats on it  like it took over a minute it's got some  tokens on it um if I click into it I can  now see exactly what was going on under  the hood so first it called the  researcher and we can click into here  and we can see the exact call that it  made to open AI um and so there was this  system prompt and then uh this was the  user's first message and then output  this  it then called toil um which is the  Search tool we can see the results there  it then called the researcher again we  can see that you know there's now more  in it because we start to build up this  message Bank we can see it calls toil  again okay so now the researcher and it  doesn't call toil okay interesting let's  see what's going on here so at the  bottom it basically says okay so now  it's uh it's responding what the values  are um and there's no tool call and  there's no uh uh thing saying that it's  finished so that means that it goes to  the next one which is the chart  generator so it calls the chart  generator under the hood that calls open  AI um we can see that down here we get a  function call and this is running python  code so it prints this out calls the  python reppel  um uh we can see that it runs this um  the chart generator then goes um and the  chart generator says uh you know here's  the line graph see the chart above the  researcher then goes and it says final  answer so now that there's final answer  it's finished and that's basically what  happened um so that's the multi-agent  collaboration bit the the kind of like  specific thing about this that's  interesting is it has This Global state  of messages that each llm sees and a  penss to so it keeps on adding to this  over time and so on one hand this is  good because it allows each agent to see  kind of like exactly the other steps  that the other one is doing and that's  why we called it collaboration it's very  collaborative the other version of this  is that you have agents that have their  own independent scratch pads um and so  we'll take a look at the next  multi-agent example that we have which  is this agent  supervisor so here we have the  supervisor which basically routes  between different independent agents so  these agents will be Lang chain agents  um so they'll basically be an llm that's  run with the agent executor in a loop  the llm decides what to do um the agent  executive then calls the tool goes back  to the llm calls another tool goes back  to the llm and then then finishes and uh  then there's basically the supervisor  the supervisor has its own list of  messages and essentially what it will do  is it will call agent One agent one will  go do its work but it will return only  the final answer only that final answer  is then added the supervisor only sees  his final answer and then based on that  it can call another agent or it can call  another agent another way to think about  this is that this is essentially um you  know the supervisor an agent and it's  got a bunch of tools and these tools  themselves are agents so it's a slightly  different framing than before it's not  as collaborative they're not working on  kind of like the same  sections so let's get this started um  again we're going to be uh I'm going to  let me restart my kernel we're going to  be using Lang Smith um so again uh hit  me up if you don't have access to that  um first thing we're going to do is  we're going to create some tools we'll  use the same two tools as before the  tavil tool and the python  tool we'll next Define some helpers  again to create these agents um as the  individual nodes so we have this create  agent function similar to last time the  difference is now that it's actually  doing a little bit more work under the  hood so it's not just a prompt plus a  language model it's actually creating an  open AI tool agent which is an agent  class that we have in Lang chain from  there it's creating an agent executor  which is also in Lang chain and we're  returning that that's the final  agent so let's run that then what we're  doing is we're also creating similar to  last time this agent node thing which  does this human message converion just  like we did last time is taking the  agent result um which is an AI message  and converting it into human message  with a name so we know kind of like  where it's coming  from now we create the agent supervisor  so this is going to be the the you know  system that's responsible for looking at  the different agents that it has and  deciding which one to to pass it on to  so  here you can see that we're defining a  bunch of stuff we're defining system  prompt your supervisor task with  managing conversation a between the  following workers and it's basically  using this function definition to select  the next Ro to send things to um and uh  it's or it's selecting finish finish is  another option we're creating this  prompt template we're creating this llm  and we're creating this supervisor chain  um which basically has a prompt um it's  got an llm with some functions that it  can call specifically we're forcing it  to call this route function where it  selects the next best role and then  we're pressing the the response in Json  um and we're using that determine where  to go to  next now that we've got that we can  create the graph so we've got this agent  State um which is messages and then a  next field um we've got this research  agent so we create the agent we then  create the node for it so we partial out  agent node we then do the same for the  code agent um and then we start creating  the graph so we create the graph with  the agent State and we add the three  nodes we now connect the edges in the  graph okay so for all the members um and  again if we look at where members is  defined members is defined up here so  it's the researcher encoder it's two  it's the two basically sub nodes or sub  agents um so for those after we call  those agents we go back to the  supervisor um then we also have this  conditional Edge so from the supervisor  um after the supervisor um is run we  look at what's in next and we look in  the conditional map and the conditional  map is basically a mapping between the  the members so uh and and finish so so  you know if if it says researcher then  we go to research node if it says coder  then we go to coder node if it says  finish then we finish we set the entry  point to the supervisor and then we run  the  graph now we can do the same thing we  can invoke the team um we can uh see  what happens um and see how it  performs all right so it runs it um you  can see that it calls the coder so here  we're asking it to print out hello or to  say hello world and print it to the  terminal so it calls the coder and then  it finishes here we can write a brief  research report on pasas and it will do  that while that's running um we can  maybe look in Lang Smith um to see the  previous ones so here we have the coding  one um we can see that we first called  the supervisor agent it's got this call  to open AI um again it's got the system  prompt and then the human um and then it  decides what to do next and it's calling  it's using this function call we can see  it has this function definition of Route  here based on that it goes to the coder  so now there's the coder agent the coder  agent does several things under the hood  it calls open AI first um it does a  function call it then calls the python  reppel um and then it calls open AI  again and so when we go back to the  supervisor if we look at what the  supervisor see says it just sees the  response from the coder so there's a  human message with the name coder and it  gets this response it doesn't see any of  these calls though or any of the calls  to the python reppel and then we get a  response and it says to finish so this  is an example of where you know there's  many calls but they're kind of hidden  within one agent  State we can also see that it finished  writing the research report on pasas so  if we go back here we can see that it's  finished running and if we take a look  it now calls the researcher agent under  the hood and we can see here if you know  this is interesting because this took a  pretty long time 45 seconds we can  quickly drill in and see that the the  longest call was this call to open AI um  inside the researcher agent where  basically it uh uh  has it has all these uh uh information  from the internet and it writes this  long research  report so that's pretty much it for the  agent supervisor now I want to talk  about the third and final type of agent  that we have an example of which is the  hierarchical agent team  so this is very similar to the previous  concept it's just changing what's going  on in the uh  uh it's just changing what's going on in  the sub things so now each agent node is  itself a different kind of like  supervisor agent setup and so what this  means is that it's going to be a lot  more kind of like a l graph um and a a a  little bit less of L  chain we can get started by setting up  the environment similar to last time we  now create the tools so there's two  different sets of tools that we need to  create one's for the research team it's  the Searcher and web scraper the others  for this document authoring team and  it's these three tools here so we can do  that we can load to villy we can create  our scrape web pages tool and then we  can start creating these uh other tools  as well um and these are for the writer  team so we create one that creates an  outline read documents write documents  edit documents um and then a python  reppel tool as well we we're going to  create a few helper utilities that make  it easier to basically construct this  pretty complicated graph so we have one\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 4632, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"this  pretty complicated graph so we have one  that creates an agent we then have an  agent node um and we then have this team  supervisor idea and so these are kind of  like taking a lot of the concepts that  we had in the last notebook and  extending them now we can Define these  agent teams they're hierarchical teams  so we have this research team This  research team has a few different  attributes on it the main things here  are this messages um we create uh we  initi in llm we create the search agent  and then the search node the research  agent and then the research node and now  we have the supervisor agent as  well we now put this all together um I'm  not going to go over all the edges  because it's very similar to some of the  notebooks that we did before but the end  result is that we have this uh uh graph  that we create in order for this graph  to be used in as a subgraph in another  component we're going to add this enter  chain function which basically just is  just a converter to make it easier to  use we can use this directly um so we  can try out and again this is just one  of the sub teams but we can try asking  it a question um and we can see that it  does a search it has a search to so  it'll get a response from there and it  will give us an  answer we can see that we get back a  response here and if we look at that in  lsmith we can see that it's doing a  pretty similar thing to before um it's  got the supervisor agent it's calling  search and then it responds with  supervisor we're now going to put  together another agent or another graph  of Agents so it's this document writing  team um so here we are defining a bunch  of stuff we're defining the state that  we're tracking it's largely these  messages um there's then uh some logic  that's run before each worker agent  begins um so that's more aware of the  current state of everything that exists  this is basically with populating  relevant context we then create a bunch  of agents and a bunch of uh uh agent  nodes um um and then we create the dock  writing supervisor um now that that's  done we can create the graph so we're  adding a bunch of nodes we're adding a  bunch of edges we've got some  conditional routing edges um we set the  entry point um and now that we have this  we can do something simple like write an  outline for a poem and write the poem to  disk we can see that this is finished  and if we look at this in Lang Smith we  can see that it also looks fairly  similar as to what we were doing before  we've got the supervisor we do a bunch  of note taking and then the supervisor  finishes it doesn't see any of the  internal things it just sees the final  response and we can now put this  together so we now have a layer on top  of this we've got this team supervisor  um and now we have this top level graph  State it's just a list of messages we  have some helper functions we start  creating the graph we add the research  team as its own node it's got a little  bit of uh it's got a little bit of  wrappers around the previous chain to  like get the previous message and then p  and then at the begin beginning pass it  that into the chain and then call join  graph to join it with the rest of the  stuff same uh uh with the paper writing  team we then add the supervisor we then  add some nodes we compile it all right  um and so now we can uh do a more  complex thing where we call asket to  write you know this brief research  report on the North American sturgeon  include a chart so let's kick this off  and I'll come back when this is  done all right so it's done took a while  if we go to Lang Smith we can see that  we have this most recent Trace took 400  seconds if we click inside it we can see  that there is a lot going on so a lot of  calls um down the hood it looks like  there's this big call to the paper  writing team at one point that took a  while there was also this big call to  the research team to start that took a  while I can click into any of these I  can see like what exactly is going on at  any point in time what exactly was  happening um so this is pretty helpful  for understanding what's going on um  yeah wow that's a lot of tokens um  anyways yeah that's pretty much it for  the multi-agent stuff hope this was  helpful I think uh you know the the  benefits of thinking about things as a  multi-agent way are really that it I  think it helps compar compartmentalize  what you expect different functions to  be happening inside the system so you  know each agent basically has its own  prompt its own tools even its own llm at  times and so you can really focus it on  just doing specific things as opposed to  having one General autonomous agent with  lots of tools and so even if you you  know even if you have all those same  tools and you just organize it a bit  more hierarchically that can often be  helpful for for doing kind of like  complex tasks as well as just a good  mental model to how to think about  things so hopefully this has been a good  introduction to that these are again  probably only a few of the ways that you  can create multi-agent workflows with L  graph really excited to see what other  ones  emerge\", metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 4632, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain'}),\n",
       " Document(page_content=\"today I finally get to talk to you about  something that I and others have been  working on for a very long time that  something is one of the sort of secrets  to How I build good like AI assistants  agents and simply more controllable  deterministic dialogue with AI that  something is what we call a semantic  router now a semantic router is  something that you can think of as a  almost fuzzy but deterministic layer on  top of your chap Bots or really anything  that is processing natural language the  main purpose of the semantic R library  is to act as a super fast decision  making layer for llms so rather than  saying to an llm you know which tool  should I use as we do with agents it  takes a long time when you do that with  semantic router it's like in almost  instant it's incredibly fast and the way  that we set it up is more deterministic  because we can provide a list of queries  that should trigger a particular  response or particular tool usage or you  know anything we can imagine and at the  same time that list of responses is  represented within semantic Vector space  so it's deterministic in that it will  trigger if we hit one of those sponsors  but we will also reach a responses  around those queries or we call them  utterances that we have to find and I've  been using this for chatbots and agents  for this specific library for the past  two months and honestly the thought for  me of agents and chap Bots being  deployed without this layer to have more  determinacy control over the chatbot I  think is a little bit crazy and I just  would not ever put a chatbot out there  without having a semantic routing layer  so with all that in mind that's have a  look at how we actually use this Library  so to get started with this semantic  router Library we can first check out  repo so it's orelo Labs semantic router  and this gives you everything that you  need to get started okay we describe  everything there but if you really just  want to jump straight into it you can go  to our introduction notebook here I'm  going to open it up in  collab and we will find ourselves here  so to get started we just B install the  library so right now we're on  0.0.4 which is basically one of the  earliest versions there's a lot of  things cool things that will'll be  adding soon and it's also open source so  if people want to contribute they can  now one thing that we have particularly  when using it with Google collab at the  moment is that we'll we'll have this  annoying little thing that happens where  we will need to restart after installing  the prerequisites otherwise we'll get  this attribute error so we just need to  go restart session and then we run again  and what I'm first going to do is Define  some routes that we're going to use and  we're going to test against so the first  one of those is going to be protective  route so this is where you would  probably want to add some guard rails to  your chatbots or agents so maybe one of  those would be you don't want it to  begin talking about politics so if a  user asks a question that we would  Define as politics we want it to trigger  this route and then we can protect  against that and we can return you know  a specific predefined response or just  remind the llm to tell the user that you  cannot talk about politics so we Define  the politics route and then we'll just  find another one so we can see kind of  how they interact this one's going to be  a general sort of chitchat uh small talk  route how's the weather today how are  things going so on and so on right then  what we want to do is initialize an  embedding model and you can either use  cair or open AI as I know many of you  will be using open AI or open AI here  but I would actually recommend trying  out coh's embedding models they do work  a little better in most use cases at  least that's what I found so for Co you  would go to dashboard. co here. for  openai we naturally go to platform.  open.com we get an API key and I'm just  going to run this cell and it will pop  up with a little input box tell me to  input my API key so I'm going to do that  now we're ready to initialize what's  called a route layer so a route layer is  essentially a layer containing different  routes and it handles the decision  making process as to whether we should  go with one route or another route or no  route there is currently two route  layers available in the library the main  one is the rout layer this is based on  the idea of a pure semantic search we  also have the hybrid route layer that  we're still working on and still  improving but that will allow us to use  both semantic space and also a more  term-based traditional Vector space as  well so that might be particularly use  for for uh specific terminology like in  the medical domain in the finance domain  and other places as well for now let's  stick with the standard rout layer and  we can test it okay so I'm going to run  these  three and let's see what we get so don't  you love politics okay our route Choice  okay so this is the the route that has  been chosen is the politics route this  function call this is that's related to  our Dynamic routes we'll talk about that  more in the future for now this what we  have here with the function called equal  to non is what we call a static route  how's the weather today okay so that's  our our chitchat that obviously triggers  our chitchat and then I'm interested in  learning about llama 2 it's not really  related to either of the routes that  we've defined so it returns none now  let's go with something else maybe I  want to ask about the agent's opinions  on a particular particular political  party that's a you know something that  we don't want people doing in most cases  so I can say okay what do you  think about the in England we have the  labor party for  example so what do you think about the  labor party see what it says okay cool  we have politics so we trigger that  route and then what we do obviously is  actually let me get the route Choice  from that so a  route what we would do is we just do  some if else logic so if route name  equals  politics we would return hi  sorry I can't talk about  politics please go away or something  along those lines right  and then we're triggering that and  obviously we just have like this if else  logic that does different things but  then okay this is a very basic this is  the introduction let me just show you  very quickly how we might integrate this  with a line chain agent so returning to  our dos we have this number three basic  Line train agent of course we also have  these as well if you want to check them  out but let's go to the line chain agent  first so I'm going to open again in  collab so we have this notebook and I'll  go through it in a little more Det  detail uh later but we have this system  message okay you're a helpful personal  trainer some so on he's also acts like a  noble British  gentleman and I had this little bit here  remember to read the system notes  provided with your user queries this is  where I'm going to be inputting the  logic from our semantic router there are  many different ways of doing this this  just one of the ways that I quite like  to be honest it's almost like you added  suggestive layer to your agents based on  the semantic router so I've defined a  agent here and I'm going to inut my  query I want to know uh should I buy  Optimum Nutrition whey or my protein  right so talk about whey  protein and you can see output well it  depends you prefer your way with a side  of optimal nutrition or MP so I don't  think it even knows what oh okay it  knows that they're Brands cool so so  that's good but I'm you know I want this  a assistant to act like a personal  trainer that has their own brand and all  these other things so what I've done is  I've created a semantic router augmented  query what this has done it's taken this  query process it through the semantic  router and then we've added this uh  extra logic layer based on what the  semantic router says which adds  different prompts to the user query  based on that via the system note so in  this one I've added one that talks about  okay different types of proteins and  products essentially and what it does is  it says okay remember you are not  affiliated with any supplement Brands  you have your own brand big AI that  sells the best products like p100 way  protein I don't know if anyone will get  that and it's a super joke but I liked  it so then the output becomes why not  try the big AI p00 way protein it's the  best just like me it's  funny um  so we have that and then I have I should  show you the routes actually so we have  this get time route which triggers a  function supplement brand which is one  we just saw busit acquiring product and  one of those obviously is the time rout  it's getting the current time for you  and putting into your query right so  without the semantic router we're just  putting this query in okay then we go  through our semantic router layer and it  augments our query with this so then if  we go with just the plane query put that  in we get it's generally recommended to  allow at least 48 Hours of rest and so  on and so on it's not specific to the  current time with the semantic router  powered augmentation we get this why not  train again at exactly 802 tomorrow  that's the time that I ask this question  uh but like the day before that way you  give your body a good rest unless you're  into those 20 24hour gym life goals  which is a bit cringey but anyway so you  see that through the semantic router  we're allow able to suggest to our agent  to take or or to get this additional  information like we have done here or to  suggest to the agent to act in a  particular way and then we have these  other ones you know I can uh do do  training sessions without the without  the augmentation there's nothing  relevant here it's generally recommended  actually\", metadata={'source': 'ro312jDqAh0', 'title': 'NEW AI Framework - Steerable Chatbots with Semantic Router', 'description': 'Unknown', 'view_count': 21867, 'thumbnail_url': 'https://i.ytimg.com/vi/ro312jDqAh0/hq720.jpg?v=65914ff4', 'publish_date': '2024-01-02 00:00:00', 'length': 874, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"here it's generally recommended  actually where is it I'm here to provide  guidance and support not personal  training sessions with the augmentation  why of course we offer these premium  training sessions at just $700 per hour  which is what I told it to to say now  that's an example of what semantic  router can do it's just one example  there are many different things that you  can do with this what I've just shown  you there is using these routes to  remind the agent of particular  information or to you know call a  function but we can also use it to  protect against certain queries we can  use it to basically do function calling  without the super slow agent processing  time that function calling requires and  we can also use this and this is one of  the things I use it for a lot as another  approach to rag you know in the past  I've spoken about there's naive rag  which way you're pering search every  query you have the agent based rag which  is slower but it can usually do a bit  more it's more powerful but then we also  have this which is kind of like the  semantic router rag or semantic rag but  it takes both it can be very powerful  like your agent but it can also be very  fast like your naive rag so it really  gets the best of both and it's generally  my preferred way of doing it so that is  the semantic router as I said now I and  my team have been implementing this  across many projects so we you know  we've been implementing it seeing what  works seeing what doesn't work and  fine-tuning it based on that and I think  what we have here is the first version  okay 100% this is still very early  version but it works incredibly well  it's truly getting us that final 20% of  the AI behaviors that we need in order  to make our agents something that we can  actually go ahead and use in production  which is very cool to see and we want  other people to be able to use this as  well which is why you're seeing this  right now I personally I'm very excited  about releasing this so I hope that this  is exciting for at least a few of you I  hope some of you get to use it and you  know please let me know what you think  if you're interested in contributing  it's all open source so you can and I'll  be doing a few more videos for sure on  how we use this how to make the most of  the semantic router and especially the  other features that I haven't spoken  about yet such as Dynamic routing the  hybrid layer  those are all very exciting things and  we have many more exciting things coming  as well so I hope all of this has been  exciting and interesting but for now I  will leave it there so thank you very  much for watching and I will see you  again in the next one  bye\", metadata={'source': 'ro312jDqAh0', 'title': 'NEW AI Framework - Steerable Chatbots with Semantic Router', 'description': 'Unknown', 'view_count': 21867, 'thumbnail_url': 'https://i.ytimg.com/vi/ro312jDqAh0/hq720.jpg?v=65914ff4', 'publish_date': '2024-01-02 00:00:00', 'length': 874, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"Tor augmented generation or rag has  become a little bit of a overloaded term  it promises quite a lot but when we  actually start implementing it  especially when we're new to doing this  stuff the results are sometimes amazing  but more often than not kind of not as  good as what we were expecting and that  is because rag as with most tools is  very easy to get started with but then  it's very hard to actually get good  implementing the truth is that there is  a lot more to rag than just putting  documents into a vector database and  then retrieving documents from that  Vector database and putting them into an  llm in order to make the most out of rag  you you have to do a lot of other things  as well so that's why we're starting  this series on how to do rag better in  this first video we're going to be  looking at how to do reranking which is  probably the easiest and fast way to  make a rag pipeline better now I'm going  to be talking throughout this entire  series within the context of rag and LMS  but in reality this can be applied to  retrieval as a whole if you have a  semantic Search application or maybe  even recommendation systems you can  actually apply not all but a lot of what  we're going to be talking about  throughout the series including  reranking which we'll go through today  so before jumping into the solution of  reranking I'm talk a little bit about  the problem that we face with just  retrieval as a whole and then specific  two LMS so to begin with retrieval to  ensure fast search times we use  something called Vector search that is  we transform our text into vectors place  them all into a vector space and then  compare their proximity to what we call  a query Vector which is just a vector  version of some sort of query and see  which ones are the closest together and  we return them now for Vector search of  work we we need vectors which are  essentially just compressed  representations of semantic meaning  behind that text because we're  compressing that information to a single  Vector we will naturally lose some  information but that is the cost of  vector search and for the most part it's  you know it's definitely worth paying  Vector search can give us very good  results but what I tend to find with  Vector search and rag with llms is that  okay I get some good results at the top  but there's actually another result in  let's say position 17 for example that  actually provides some very relevant  context for the question that I have  asked so in this example let's say let's  say this is position 17 down here we  have that relevant item but what we  would typically do when we're doing rag  with llms is we're returning like the  top three items so we're missing out on  these other relevant records down here  so you know what can we do the I mean  the simplest is simply to just return  everything and send all of these into  our llm right so over here we have our  llm now that's okay but llms have  limited context windows so we're going  to end up like filling that context  window very quickly if we just start  returning everything so we want to  return all of this so we want to return  a lot of Records so that we have high  retrieval home but then we want to limit  the number of Records we actually send  to our llm and that's where reranking  would come in so by adding a ranker we  can still use all of those records right  we still get to return all of these from  our retrieval component but then the  records that we actually sent to our LM  are just these here right these top  three and the rerer has gone ahead and  handled the reordering of our records to  get the most relevant items at the top  so we can then send all of that to our  llm now the question here is is a ranker  really going to help us here can we not  just use a better retrieval  model and yes we can use a better  retrieval model and that's something  we'll be talking about in a future video  but there is a very good reason as to  why a ranker can generally perform  better than a encoder model or retrieval  model so let's talk about that very  quickly this is what an encoder model is  doing so this is encoder SL retriever so  this is like your order  002 okay now what it's doing is we have  a Transformer model okay so and these  are the same Transformer model the  reason that I've got two of them on the  screen right now is because you use your  first iteration or inference step of the  transform model to create your embedding  for document  a right and from that you get your  vector a so that is the compressed  information that we can then take across  to our Vector database which would kind  of be like this point here all right  that's in our in our Vector space and  then in another inference step we're  going to do the same for document B we  get Vector B and there we go we we have  that in our Vector search and we can  then compare the proximity of those two  records to get the similarity all right  the metric that we'd be using here like  the the computation would be either dot  product or or cosine in the case of  02 now you have to consider that the  computational complexity of something  like cosine similarity is much simpler  than one of these Transformer inference  steps right so the reason that we use  this encoder architecture is that we can  do all of the Transformer inferences at  the start right when we're building our  index that takes a long time because  Transformers are big heavy things they  take a lot of computation whereas the  cosine similarity Step at the end which  we can run at you know the time when our  user is making a query is very fast so  it's kind of like we're doing the heavy  part of the computation to compare  documents at the very start of building  the index and that means we can do very  quick simple computations at user query  time and that is different to what we do  reranking so here this Transformer is  our  ranker and at query time right so let's  say document a here maybe that's our  query and document B is you know one of  documents in the database where saying  to the Transformer okay how similar are  these two  items so to compare the similarity in  this case we are running an entire  Transformer inference step so this  because we're doing everything in a  single Transformer step we're not losing  as much information as we are with this  one where we're compressing everything  into vectors that means that  theoretically we lose less information  so we can get a more accurate similarity  score here but at the same time it's way  slower so it's kind of like a you know  one on one side  you have fast and you know relatively  accurate and then on this side you have  slow but super accurate so the idea with  the sort of reranking approach to  retrieval is that we use our retrieval  and codep to basically filter down the  total number of documents to just you  know in this example let's say there's  like 25 documents there 25 documents is  not too much so feeding them into our  ranker is actually going to be very fast  whereas if we fed all documents into our  ranker we'd be  waiting I don't know like a really long  time which we don't want to do so  instead we filter down the encoder feed  them into the ranker and then we'll get  like three amazing results super quickly  so that is how the reranking approach  Works let's see how we'd actually  Implement that uh in Python okay so  we're going to be working through this  notebook  here we need hooking face data sets  that's going to be where we where we get  our data set from open AI for creating  our embeddings uh pine cone for soaring  those embeddings and C here uh for our  ranker we're going to start by  downloading our data set which is this  AI archive it's pre-run so I've already  chunked into like tokens of 300 I think  something like that and and it's  basically just a data set of archive  papers you can kind of see a few of them  here that are related to llms  essentially I gathered it by taking some  recent papers that are well known like  llama 2 paper gp4 paper gptq and and so  on and just extracting that extracting  what that was referencing and extracting  those papers and kind of just going in a  loop like through that so yeah we have a  fair few records in there it's not huge  but it's you know not small either so  41.5 th000 trunks but each chunk is you  know roughly this  size okay so I'm just going to reformat  the data into the format we need this is  basically like pine cone format you have  ID text which we're going to convert  into embeddings and metadata we're not  going to use metadata in this example  but it can be useful and maybe it's  something that will we'll look at in a  future video in this series as well so  we need to Define our embedding function  so we need to Define that encoder model  that we're going to be using for that  I'm going to be using opening eye it's  just it's easy 02 fairly good  performance although there are better  models and that's something we will also  be talking about in the future so I'm  going to just run that and I will need  to enter my openai API key to get that  you need to head on over to  platform.  open.com and get your API key I'm going  to enter mine in here and yeah so with  that we should be able to like  initialize our embedding model which we  are doing here I'm not going to go  through like all these functions because  I've done it like a million times  before I think people are probably  getting bored of um that part of these  videos so I'm just going to run through  those it's really very  quickly I'm going to get my pine cone  credentials again app. Pine cone. for  those and I will run  that enter my API key first and then I  want my Pyon environment which I find  next to my API key in the console so  mine was this yours will probably be  like gcp stter or something along those  lines Okay cool so here I'm going to  create an index if it doesn't\", metadata={'source': 'Uh9bYiVrW_s', 'title': 'RAG But Better: Rerankers with Cohere AI', 'description': 'Unknown', 'view_count': 34369, 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'publish_date': '2023-10-18 00:00:00', 'length': 1422, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"here I'm going to  create an index if it doesn't already  exist my index does actually already  exist and I'm not going to recreate it  because it takes a little bit of time or  at least it did the other day when  creating this so you can see that I  already have like the the 41,000 records  in there you you know if you're looking  at that you should probably see nothing  in your in yours unless you've just run  this or you're connecting to an existing  index okay so this is a code that I use  to create my index right it's it's  pretty straightforward the one thing  that is maybe a little more complicated  but it's not that complicated is we're  actually creating the embeddings here so  I think I defined an embedding function  up here actually and I ended up not  using it for some  reason just ignore that so in here this  is where we're doing our embeddings but  we're wrapping it within an exponential  backoff function to avoid rate Lim  errors which I was hitting a lot the  other day so  essentially it's going to try and embed  uh if it gets a rate limit error it's  going to wait okay and it's going to  keep doing that for maximum of five  retries hopefully you shouldn't be  hitting five retries if so there's  probably something wrong so yeah um you  should be okay there but if you are  hitting those rate limit errors you you  might be waiting a little bit of time  for this to finish if not it should  finish quite quickly I was hitting tons  of R limit errors the other day and I  ended up this took like 40 minutes I  think so yeah just be aware of that um  it's going to depend on the rate limits  you have set on your openingi account  now we want to test retrieval without  coh reranking model first so I'm going  to ask this question so get dos yeah I'm  just querying again not going to go  through everything I'm just going to  return for now the top three  records okay okay so my question is can  you explain why we would want to do  reinforcement learning with human  feedback that's what this is here it's  like a Training Method that is kind of  like why chat GPT was so good when it  was released so I kind of want okay why  why would I want to do that I think the  first answer here and there's some like  the this scraping that I did is not  perfect so I apologize for that but for  the most part I think can read it so  it's powerful strategy for find to aniz  langage models enabling significant  improvements in their performance  iteratively aligning the models  responses and more closely human  expectations and preferences  okay it can help fix issues with  factuality toxity and  helpfulness that cannot be remed by  simply scaling up  LMS okay so I think that is that's a  good answer like number one there and  then let's have a look at the second one  uh increasingly popular technique for  reducing harmful behaviors okay amount  of can significantly change metrics  doesn't necessarily tell me you know any  benefits there okay so the only the only  relevant bit of information in this  second sentence is increasingly popular  technique for reducing harmful behaviors  okay so just one little bit there and  then number three I think like I I don't  see anything in this that tells me why I  should use R  lhf it's tell me about rhf but isn't  telling me why I'd actually want to use  it so you know these results could be  better right so number one good number  two you know it's kind of relevant  number three not so much so can we can  we get better than that uh yes we can we  just need to use reranking so I'm going  to come down to here and we're going to  to initialize our reranking model so for  that we need another API key which is  coher API key you know this is this  should be free like the pine cone and  coh here ones will be free the open AI  one I think you need to pay a little bit  so yeah just be aware of that but again  we'll be like I said later on in these  in this series we'll be talking about  other alternatives to open AI for  embedding models which may actually be a  fair bit better so I'm going to go to to  this website here dashboard coh here.com  API Keys you will probably need to sign  up make an account and you know do all  of  that and then you will get to your here  dashboard new trial key I'm going to  call it something I don't know  demo generate Tri Crate Key okay and I'm  going to put it into  here cool so we now want to rerank stuff  let's TR try so I'm going to I'm just  going to rerun the last results CU I  only got three here I'm going to rerun  it with  25 so yeah we have many more now and I'm  just going to rank those 25 and I want  to see you know what was reranked just  want to compare those results so when we  rerank stuff we're going to return this  go here responses rerank result object  and we can access the text from those  like this okay so you can see see we  kind of get this output there and the  way that I've set up the like the the  docks I'm sorry docks object that I  return from the last item here like you  can see it's dictionary where the text  maps to the position the reason I've  done that is so that I can just very  quickly uh see what the reordered  position after reranking is so you can  see that okay it's kept you know the  zero position like the the top result  but then it swapped out to two and or  one and two for these two items here  okay so I'm going to Define this  function here it's basically just going  to do everything we've just gone through  it's going to query uh get those results  it's going to then rerank everything and  it's just going to compare uh the  results for us so I'm going to set a top  K of 25 so returning 25 records from our  retrieval step and then we're just going  to retrieve return top three from our  reranking step so I'm going to compare  paare that query so the r lhf query okay  so zero has remained the same one has  been s for 23 and two has been s for  14 so this won't show us the the first  results here because they haven't  changed so we're looking at these  results first so the original is what I  what we went through before where it has  like the the one like kind of useful bit  of information increasingly popular  technique for reducing harmful behaviors  in large language models and then the  rest wasn't really that relevant to our  specific question which is basically why  would I want to use RL HF now having a  look at  23 we've shown it's possible to use RL  HF to train llms that act as help one  hob assistance okay so that's useful  okay that's why we might want to use it  RL HF training also improves honesty  okay that's another reason to use it in  other works associated with aligning LMS  rhf improves helpfulness and  harmlessness by huge margin okay another  reason why we might want to use it so  okay Three Good Reasons already our  alignment interventions actually enhance  the capabilities of large models again  yes I think that's another reason  combined with training for specialized  skills without degradation in alignment  or performance another reason why we  should use it right so this here is  talking about RF like to previous like  number two ranked context but it's way  more relevant to our specific question  which is you know that's why we use  reranking  models now let's have another look so  this is uh yeah this one there was  nothing relevant right so this is the  original for our specific question there  wasn't anything relevant in here the  reranked one has this just one thing  here is like the the LMS are actually  reading all of this text which is kind  of impressive I I rarely struggle to but  anyway so the model outputs are output  safe with sponsors I think that's  assuming it's talking about rlf is a  good it's helpful we switch entirely to  RL HF to teach the model how to write  more Nuance responses okay so that's a  good reason comprehensive tuning with RF  has added the benefit that it may make  the model more robust a jailbreak  attempt another benefit we can do it to  RF by first collecting human preferences  it's not relevant an dat write a prompt  they believe can elicit safe behavior  and then compare multiple model  responses to the prompts selecting the  responses that safest according to a set  of guidelines we use the human  preference data to train a safety reward  model okay so I think the relevant bits  here are make the model more robust to  jailbreak attempts and teach the model  how to write more nuanced responses so  those two are good uh the rest of it  isn't as relevant but it's far more  relevant than this one where you know it  didn't tell us any benefits to using  RF cool now let's try one more so what  is your red teaming it's like a it's  like a safety or security testing thing  that they apply to llms now it's like  stress testing for llms you can see that  okay it hasn't changed the top one again  and I think the responses here were  generally you know not quite as  obviously better with reranking but  still slightly better uh what I will do  is just kind of let you read those so  you know you have this one here you can  you can pause and read through if you  want uh and also this one as well so  again you can pause and read through if  you like I'm not going to go through all  those again so that is reranking I think  it's pretty clear  uh it it can help a lot at least I have  found it just you know I don't have any  specific metrics on how much it helps  but just from using it in you know  actual use cases it it helps quite a bit  so I hope this is something that you can  also use uh to sort of improve your  retrieval pipelines particularly when  you're using Rag and sending everything  to LMS but you should also test it and  make sure it is actually helping so for  example if you're using a maybe you're  using kind of like an older reranking  model the chances are it won't actually  be quite as good as some of the more  recent and better encoder models so\", metadata={'source': 'Uh9bYiVrW_s', 'title': 'RAG But Better: Rerankers with Cohere AI', 'description': 'Unknown', 'view_count': 34369, 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'publish_date': '2023-10-18 00:00:00', 'length': 1422, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"of the more  recent and better encoder models so you  could actually degrade performance if  you if you do that so you always want to  make sure that you're using kind of like  state-of-the-art rankers alongside State  of-the-art encoders and you should see  an impact kind of similar to what we saw  here with the RL HF question but anyway  as I mentioned this is like the first  method I would use when trying to  optimize an existing retrieval Pipeline  and as you can see super easy to  implement it's you know you don't really  need to modify other parts of the  pipeline you just need to put this into  the middle so I'll leave it there for  now I hope this walk through has been  useful and interesting thank you very  much for watching and I will see you  again in the next one  bye\", metadata={'source': 'Uh9bYiVrW_s', 'title': 'RAG But Better: Rerankers with Cohere AI', 'description': 'Unknown', 'view_count': 34369, 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'publish_date': '2023-10-18 00:00:00', 'length': 1422, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"all right let's get started this is the  opening eye prompt engineering page  before we can start building with it  though we need to set things some things  up so I've created a f a fresh uh  virtual environment what I'm going to do  is I'm going to set up a l  serve uh uh template a l or application  with this the reason that I'm going to  be using lay serve is it'll make it easy  to deploy this once I finish creating  so the first thing that I'm going to do  is bootstrap a link serve project I'm  going to do that with the Lang chain CLI  I'm installing it here after it finishes  installing I'm going to create a new app  I will call  it open AI  prompter I am not going to add a package  because I'm going to be creating my  own I can then go inside it and if I  open it up the main thing that I'm  interested in is this Lang serve server  right here which wraps around fast API  and will make it really easy to deploy  my Lang chain  chain the last thing I'm going to do for  setup is set up laying Smith so laying  Smith is in uh private beta if you don't  have access to it and you're seeing this  and you want to follow along shoot me a  DM on LinkedIn or Twitter and I will uh  get you access to it um what it will  make it really easy to do is debug as we  go along and this will involve a lot of  prompter engineering and so we'll see  how that becomes very very helpful um  someone in a previous video commented  that I should go over how to set this up  more so you can go to your projects  let's even create a new project um let's  call this uh open AI  prompter um that's all we can do let's  then go to setup and then we can just  export some API Keys  here um so you can just copy this  you can paste it here you then need to  add in your API key you can do that let  me let me move my face over here and  let's move this over here as  well you can do that by going to API  keys and all right good it's not showing  mine so you can create an API key and  then put it there um I've already did  this ahead of time um so mine's all set  up but if you needed to do that this is  how you do that all right so let's go  back to this prompt engineering guide  the first thing we're going to do we're  just going to copy it all because we are  going to be using this in our prompt to  help us write better  prompts let's paste it into a  file and then I'm going to I'm going to  create my chain in a Jupiter notebook  and then I'm going to put it into uh and  then I'm going to put it into this app  and so the reason that I'm doing this  there's uh there's there's one real  reason and one fake reason the one real  reason is that a lot this will it' be a  pretty iterative process I'm going to do  some prompt engineering some of that  I'll do in Lang Smith but some of that  I'll I'll do in uh The Notebook as well  and we'll see why um and so having a  notebook like environment is really  really helpful because it's an iterative  environment and I can really easy  iterate the other reason I want to do  this is I want to show how it's really  easy to create a chain and then just  export it from a notebook most of the  time it's it's pretty simple so let's  save  this and now I've got this jup notebook  let's load um what I just put in  there all right so let's do  that let's import uh we can print out  the head I guess just  to see it let's import some stuff from L  chain that we're going to want so from L  chain  core. prompts  import template from L core do output  parsers string output parser from L  chain  Community do chat models import chat  open AI so I'm importing three important  things this is going pretty simple chain  that I'm writing we'll see maybe we'll  get more complex but it should just be  pretty simple because I'm just going to  use a model that has a long context  window and can work with all these  instructions so I'm importing a prompt  template this will help me uh structure  the inputs to the model I'm importing an  output parser that'll basically just  convert it from a message format which  is how the newer models respond the chat  messes respond with uh message but I I  really want to set it's string so I'm  getting that and then I'm importing chat  opening app which is the L chain wrapper  around the open AI models which are the  ones that I'll be  using now is the fun part where I'm  going to write my template for creating  or my template that's going to help me  create a chain that can take in an  objective and write a good  prompt so let's add this  there  um and okay so so this is actually an  interesting point that I'll get to later  but basically what I'm doing right here  is I'm going to have these instructions  as a variable that I'll pass into the  prompt and I'll I'll go into that more  later but for now let's just assume  that's what we're going to do and then  let's add just some delimiters here and  then let's say  based on the above  instructions help me write a  good  prompt I want a  prompt  that okay so what I actually want to  write is not actually a prompt but I  want to write a  prompt  template because I think most people  write prompt templates because they want  to write prompt templates that then they  can use in their code or at least most  linkchain users want to write prompt  templates help me write a good prompt  template  this template should be a python s  string  it can take in any number of  variables depending on my  objective return your answer in the  following  format this is my  objective this seems like a decent first  pass I don't know we'll see um and then  we're going to create the chain okay so  as I mentioned we have this thing here  um which I'm going to pass in the  instructions to let's see what would  happen if we didn't do that and if we  just did  like this which you could also easily do  so if I did this I would have prompt  equals prompt template from template  template  chain um equals prompt  Shadow  AI string up the  parser chain. invoke and now I'm going  to pass in objective what should my  objective  be  um  answer B answer a question based on a  based on context  provided and only on that context  so this is a pretty typical rag prompt  or retrieval augmented generation prompt  you want the the language model to  respond to a user question based on the  context  provided um so let's do this and I get  ke because basically what's happening is  in these instructions there's other um  there's other curly brackets which  aren't variables that I want to format  they're just part of the text and so  this often happens when you're working  with code or something like that and  we've got a a lot of questions about how  to deal with it so my preferred way to  deal with it is to do what I'd done  earlier treat the the text that has  these unwanted curly brackets as input  variables and then okay so so one way to  do this is you may be thinking okay so  that means that I now have to provide  text as text here and this is a little  bit annoying because now you have two  input variables it's not the end of the  world but it's a little bit annoying and  if if this chain is part of a larger  chain then then it starts to get more  and more complicated more and more  annoying um  and ah I  had that should be  taex this is  erroring because the model's not long  enough I could do some fancy rag um it's  just like barely over the context window  and I'm I'm going to want to use this  model anyways because this task is kind  of hard so I want a good model to do it  um so this will just get me around that  and then let me also set temperature  equals to zero while I'm at it so I  could do it this way I could pass in the  text here um and this would work let's  see what answer it gives um for the next  time I'll use streaming um so we can  start seeing what's going on earlier um  all right it's writing long thing while  while it's  working okay so let's um this is a  little  unreadable um because it's not super  readable what I'm going to do  is all right so this is Lang Smith this  is me debugging I want to see the output  um this is a all right so this is better  output um I get uh oh interesting  okay so all right so it's giving me some  instructions  on how to use  it  um I actually don't know if this is  correct because of these curly brackets  but anyways  um what we can do is uh uh sorry back to  this what we can do there's a lot I want  to show off but first things we can do  is partial The Prompt so we can now  do this and that means we no longer have  to pass this in so we're basically just  partialing the prompt we're passing in  some input variables ahead of time  before we even construct the train and  then because we know those are always  going to be fixed and they're and they  shouldn't be variables so we're going to  pass them in and then we can do this I'm  going to now change this to streaming  for Chunk in or more accurately for  token in chain.  stream print  token um all right I need to  add something like  this all right  so there we  go it's not  exactly um what I really want I really  want like just one I really just want  this  thing um templ should be a  python see if this  helps  so it's not helping a ton I can go back  in here and I can I can start doing some  prompt engineering here if I want so one  thing that I can do it's not really that  helpful here because this is a really  simple um chain but if I wanted to I  could open up the playground and it's  also not helpful here because my prompt  is really really long and this isn't the  grass oh I can okay that's slightly  better so now I have this um and um  let's let's just do  like bunch of new that thing based on  the above instructions help me write a  good prompt template this prompt  template should  be a string that can be formatted as if  a python fstring I can take in any  number of variables depending on my  objective  um I can now  run I can now run  this  okay okay that's pretty  good\", metadata={'source': 'mmBo8nlu2j0', 'title': 'Auto-Prompt Builder (with Hosted LangServe)', 'description': 'Unknown', 'view_count': 5458, 'thumbnail_url': 'https://i.ytimg.com/vi/mmBo8nlu2j0/hq720.jpg', 'publish_date': '2024-01-05 00:00:00', 'length': 1386, 'author': 'LangChain'}),\n",
       " Document(page_content=\"can now run  this  okay okay that's pretty  good  all right so that's pretty good I'm I'm  I'm relatively happy with  this so um let's  uh take  this from the playground let's bring it  back in the  notebook  24 being a bit slow okay so it's  generally doing pretty  good I'm relatively happy with this I  might do a little bit more prompt  engineering by the time this gets out  but I'm relatively happy with this for  now so what I'm going to do is I'm going  to move it into Lang serve now I'm going  to deploy it with Lang  serve so I am going  to create a new file in here I am going  to basically copy this chain  over that's done  I'm going to go into the  server um so I'm going to edit this add  the chain you want to add um from app.  chain  import  chain  chain  path  prompter cool um I'm going to go back  here and  let's try it out and see what happens L  chain  serve doesn't find the file that is  right so L chain  notebooks open AI prom what did I name  it prompting oh right let specify  that let's try serving it  again and I'm going to poetry add open  AI because I'm going to deploy this  project later um and this will add it to  the Pi Project file so it's going to  save  that um let's try it  again  cool so now if I go  to Local Host  8,000 prompter playground  I get  this let's try it out I take  this add it  here all right so I get back some  streaming um I get back some nice  stuff okay so there's definitely some  prompt engineering that I want to do um  it's doing this um Let me let me pause  the video a bit do some prompt  engineering and then resume it in a  second all right so I'm back with a  little bit of a better prompt um and I  will uh uh I I'll include links to all  the code so I won't read out here  basically now if I go here and I type in  an  objective this gets me um this gets me a  pretty solid prompt that I'd use for for  retrievable augmented  generation so so that's basically how we  get to something that's deployed with  Ling serf um this is still all local and  so for the final part of this I'll walk  through how to do this um on laying  Smith so that you can share it with  other  people as you  see so I have my my personal tenant my  personal tenant doesn't have access to  uh laying serve deployments yet on Lang  Smith so this is uh this is an alpha  feature that we're still testing um it's  it's it's only available to a few people  um if you are interested in being an  alpha tester please let us know um or  please let me know and uh we are not  letting a ton of people off because we  want we do want to make some  improvements um but as we let more and  more people off um you know we'll we'll  will uh let me know and and you can be  one of  them in order to show you briefly what  it looks  like I am uh going to switch to the Lang  chain account I'm going to go to  deployments um and you'll see once it  loads that I have a bunch of deployments  here these deployments are all connected  to GitHub um so one thing that I'm going  to do after this is I'm going to pause  set up a GitHub um uh repo  with this code so that I can easily  deploy  it um so let me pause and do that and  then I'll come back and walk through a  new deployment and see what  happens all right so we're back so I'm  going to create a new deployment I'm  going to click here I'm going to  choose this repo that I created open AI  Auto prompter name it uh open AI  prompting  helper um I'm going to add some  environment variables um namely I'm  going to add open AI  API key um I'm going to make it a secret  G to pause while I put this  in all right it's in um I automatically  get a tracing project for this so that's  a big benefit of deploying on Lang Smith  is it automatically connects to  everything else in Lang Smith um right  now that's tracing we'll make that  testing um and and prompts and other  things in the short  term um I can now submit  this and it will spin up a  deployment um and so here is this uh uh  deployment as you can see it's uh it'll  take a little bit so I'm going to pause  and come back when it's finished  deploying all right we're back our  deployment succeeded we can now open the  playground for this  deployment so we can go  here task um what what was the task we  had way back in the  day so let's just use the same task  here we can see it  streaming  awesome so we get this result um one  thing we can do is we can give thumbs up  or thumbs down I like this one so I'm  going to click thumbs up if I go back  here I can see it's only one Trace count  but I can see the traces for this so  here this is the most recent Trace I can  click into it this is a tracing from  this is the same tracing that's in uh uh  langing Smith as you've been using it so  that's one of the big benefits as it  comes with all this stuff it comes with  feedback automatically hooked up so we  have a score here um and  yeah that's uh basically it for the  video now hopefully this was pretty  helpful in terms of getting  um a pretty simple chain it's you know  it's just a a prompt a model and an Alpa  parer up and running using lell uh  getting around some uh prompt uh  annoyances with all the formatting of  the curly brackets shwing how to set up  langing Smith showing how to set up a  laying serve project and then showing uh  the new laying serve deployment feature  as well hopefully you guys enjoyed this  let me know in the comments have a good  one\", metadata={'source': 'mmBo8nlu2j0', 'title': 'Auto-Prompt Builder (with Hosted LangServe)', 'description': 'Unknown', 'view_count': 5458, 'thumbnail_url': 'https://i.ytimg.com/vi/mmBo8nlu2j0/hq720.jpg', 'publish_date': '2024-01-05 00:00:00', 'length': 1386, 'author': 'LangChain'}),\n",
       " Document(page_content=\"today we're going to be talking about  Lang chain expression language which is  a pretty interesting idea that  essentially allows us to write very  minimalist code to build chains within  line chain and for sure I think we'll  see from this video we can use a lot of  L chains more advanced features like  parallel execution async and streaming  very easily using the expression  language rather than just the more  typical approach to build Lang chain  chains and in my opinion it's worth  trying just for that I think we'll see  that just using this you can build stuff  very quickly that's not to say it  doesn't have its cons but we'll dive  into those later so let's just begin  with what this expression language  actually is so there's a page here in  the line train dos talking about this  expression language right so it's  LC for  short and yeah they just explain a few  things you know we streaming acing parel  execution so on and so on right but  let's just jump into this notebook and  we'll see more of how this actually  works so there will be a link to this  notebook as they usually is at the top  of the video right  now and I've WR all this in collab so  you can do the same it's pretty  straightforward we have a few  prerequisites we're going to be using  line chain of  course we're going to be using anthropic  the new Claude 2.1 model for our llm  we're going to be using cave the  embeddings and we're going to be using a  dock array just so I can give you an  example of parallel retrieval later on  which is super interesting now the main  things I think we would want to use the  Expression language for is these three  items here so we have super fast  development of chains we have those  Advanced features streaming acing  parallel execution just work out of the  box with these super fast and easy to  set up and there's also easy integration  with the other Lang chain products so  Lang Smith and Lang serve if you are  using those now let's take a look at  what it actually looks like so to get  started with this we're going to need a  anthropic API key and you can get that  by going to console anthropic tocom  you'd come into here hopefully you have  an account already and you can click get  API keys and you're just going to get  your API keys from there if you don't  have an anthropic account I think  there's still a like a very minor weight  list so one I just recommend you sign up  and you you'll get access pretty soon  but so that you're not waiting you can  also just use open AI so you would just  swap chat anthropic here with chat  openai and swap anthropic API key for  openai API key and if you do do that you  will also want to drop just drop these  two arguments it'll make things easier  so looking at this let's see we'll put  our API key in here and once we have  that we now have these three components  we have a prompt a Model A chat model  and a output passer okay now in typical  L chain we would chain these together  using the llm chain okay so you can see  llm chain your prompt the L and the  output passer  okay what I'm going to do is take this  prompt where're asking to give me a  small report about a particular topic  okay so the the input to that is going  to be topic and you can see that here so  we have topic artificial  intelligence and it's obviously just  going to Output a small report on that  okay so let's run that and see what we  get so it's running uh we create our  chain running chain. run and we'll just  print that output and we'll get this  small like rort thing on on AI okay so  all looks pretty good now how would we  do that with the expression language  well we use this this pipe operator and  I'm going to go into detail as to how  this actually functions because I think  that's understanding how this pipe  operator  functions allows us to just understand  what is actually happening here okay so  that we can actually understand this  abstraction rather than just blindly  using it  so we string things together right so we  have our prompt followed by the model  followed by output parer and rather than  putting them into an llm chain or some  other chain we just string them together  with this pipe operator so I mean it's  like for sure if I look at this it's  kind of it's simpler than this right if  you compare those two  it's I would say also more flexible  because we can just string things  together but it's you know I think it's  it's not so pythonic as to what we're  used to whether or not that is a good or  bad thing I'm undecided on like I really  I like the minimalist approach here it  looks  great but it it's maybe hard to  understand like if you if you don't  understand the syntax and you on python  very well this is going to be pretty  confusing anyway let's run that so we  create our chain using this new this  expression language syntax and then we  just rather than running run we run  invoke and we pass a dictionary of input  variables into  there so we run this and yeah it's going  to do the exact same thing we or very  similar output to what we saw before  okay so it gives us little report again  okay looks cool so these two things  this and this doing the exact same thing  just different syntax now I think when  you see that syntax of the pipe operator  for the first time at least for me I was  quite confused and I think most people  would be confused the way that it works  is pretty simple at least the idea  behind how it works can be explained  very easily what we see  on the left of each pipe operator the  output from that gets passed to what is  on the right of the pipe  operator okay and then the output from  this is passed into this so it's it's  literally piping things from the left of  the pipe operators all the way through  to the right of the pipe  operators that's that's all it's really  doing now how that pipe operator  actually works is  more not necessarily complicated it's  probably a little bit hacky in my  opinion but it's it's kind of  interesting so this pipe operator when  we apply it to an object in Python what  it actually looks for within that object  is this or method here right so if I  come down to here we have this kind of  confusing class called runnable but  let's break it down a little bit okay so  I'm going to do class and we're going to  call it what still going to call it  runable now when we initialize this  class we run I'll see the init method  here and within that we're going to pass  a function right because the way that  we're going to implement this is we're  going to give a function into this class  and we're going to use this class to  transform this function into something  that we can use this pipe operator on so  we want to save that function within our  runable class or object and then the  next thing you see this is the part that  makes the the pipe operator work okay so  when a pipe operator is applied to an  object it's going to look for the  objects all method now the or method  that needs to contain another function  that we call other here now the way that  you can think of this the funk and the  other arguments here is that funk is  kind of what is on the left of our pipe  and other is what is on the right of our  pipe okay so what we do is we create  this chain function here which is going  to consume a set arguments and keyword  arguments so we can call it chain Funk  as we do there our arguments and we have  our keyword arguments now the reason  that we set up with args and keyword  arguments like this is because we don't  know the names of the parameters that I  going to be input into our function  right so by doing this we can you know  those parameter names can vary we can  have more or less and this chain  function will be able to handle those so  we would do return other so our  basically this function here that  consumes the output from our function  okay and again that function is going to  take those ARs and keyword  arguments okay so from that we would  then return the the runnable  here so this is going to be our like  runnable version of that  chain  function so basically by doing that  we're putting the uh this ability to run  chains within each one of the functions  that we pass through this actual chain  okay so we can do multiple of these so  we could have you know other  two other three so on and so on now the  final thing that we need to have here is  a method that allows us to call and and  begin this chain now I'm going to  implement it with this we will see that  line chain actually uses I think they  use  invoke so rather than call they would  have invoke here and that starts to ch  but I'm I'm just going to do call  because I think it's simpler so that is  our runnable function we can run that  and I also have it here maybe I'll just  run this one and what we want to do is  use this Runner board to kind of wrap  around different functions that we would  like to run with this pipe operator  approach to do that we're going to  Define two very simple functions here  one is add five one is multiply by two  okay so let's run those and I'm going to  wrap those with this runnable object  that we've created and then using this  approach right so we have uh we have the  chain we're going to do add five and  then rather than using the PIP operator  I'm going to use the the all method  directly and then within that all method  I'm going to pass our multiply by two  runnable okay so we have those and then  we can just call our train so three to  it and we get the value 16 which is  that's correct so we do 3 +  5 take both those gives us  eight and multiply those by two okay so  it's correct it's run in the correct  order now we can use this syntax or now  that we use this or method we can also  use the syntax that we see here with the  pipe operator so let's try  that okay you can you see that we we now  have this so yeah that's that's pretty  interesting so we can you know we can  build our own\", metadata={'source': 'O0dUOtOIrfs', 'title': 'LangChain Expression Language (LCEL) Explained!', 'description': 'Unknown', 'view_count': 7101, 'thumbnail_url': 'https://i.ytimg.com/vi/O0dUOtOIrfs/hq720.jpg', 'publish_date': '2023-12-07 00:00:00', 'length': 1538, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"so we can you know we can  build our own pipe operator functions  using using this and this is what line  chain is doing okay so when we see this  line chain expression language this is  what we're actually looking at which is  an interesting way of putting things  together now that's how it works let's  have a look at how we actually use the  Expression language itself so we saw  already  we can use the or operators or the pipe  operators now let's put it together in  an actual use case so I'm going to be  using the coher embedding model you know  if you you can also use open a eyes  embedding model it's up to you but to  get that API key I don't think there's a  weight list for coh here so you can you  should be able to jump straight into it  you can go to dashboard. here.com you'd  go to API keys and from the API Keys  page you can you can create either a  trial key or production key and you just  use that so I'm going to add mine in  here and I'm going to be using the cair  embedding model so the the newest one  from there which is very high  performance embedding model I'm going to  be using that to create two kind of like  document stores that we have here okay  so we have you know they're very small  it's just for an example we have one  where we have half the information in  Vector sore document sore a and half the  information in saw or do saw B you'll  see why soon but for now what we're  going to do is just use the first one  okay so we're going to use  a right so it contains information about  me when my birthday is the one contains  the year of my  birthday so let's  try  putting information into the Vex store  or retrieving information my vase store  and then feeding that alongside the  original query into a chain using the  expression language now when we do this  there's one important thing that we need  to be aware  of which is when we use this  syntax just using this syntax and  nothing else we we have like one input  and one output to each of these items  right each of these components so how  you know how does that work when we have  you know we have a context that we need  to use here and also a question that we  need to feed into our prompt and the way  that we do that is by using this  runnable parallel object so I've  imported those here we have runable  parallel and runable pass through the  runable parallel which we have here  first it allows us to run multiple  chains or components in parallel and  also extract multiple values from them  right so here we're going to run  retriever a and then for this question  we're using this runnable pass through  item what runnable pass through does is  whatever was input into the retrieval or  the runable parallel object it's just  going to return that okay so it's  literally a pass through for  values that you pass into here so let's  run all of that okay so we have our  retriever a here that we're using we  have our prompt template so on and so on  right we have our retrieval that happens  first so we have a query when was when  was I born we're going to invoke that  and this value is being passed into our  retriever it's doing a search getting  the context it's also being passed  through here and going straight through  to our prompt  okay so then our prompt gets formatted  with the question we have when was James  born with the context we have the record  we will have the records from here okay  so V saw a so my birthday the actual  date now what we will get here is  unfortunately I do not have enough  context to definitively State when James  was born and it tells me what it found  it found this little bit of information  so it knows that my birthday is z but it  does not specify the year that I was  born okay so it can't actually fully  answer the question but we can see that  this chain is working it's going to do  retrieval comparing soon our prompt  model Alpha Passa whatever else it's  going through everything now the cool  thing with runnable parallel you might  have guessed with what we have here is  that it can run many things in parallel  not just a retriever and you know  passing through a question we can  actually run multiple Retrievers in  parallel or we can run multiple  different components in parallel at the  same time and this is one of the things  that is very cool about the expression  language is that it you know we we set  these things up in parallel and like  runnable parallel here is just going to  do them in parallel right it's going to  run those in parallel we don't have to  deal with you know building or writing  any of that code ourselves which is I  think pretty cool so let's come down to  here what I'm going to do is now that  we're going to be retrieving information  from two places I'm going to create a  context a and a context B we're going to  run that or we're going to initialize  the The Prompt then our runnable  parallel now we need to modify a little  bit we need to add so we have retriever  a we're now mapping that to context a  and we have retriever B which we're  going to map over to context B and then  as before we have our question which is  the runable pass through now the chain  itself is exactly the same we still just  have one like retrieval component there  now because you know both our retrievals  are being run in parallel within that  abstraction so we're going to run that  and now I'm going to say the same the  same question when was I born okay so  now it it knows based on the context  provider James was born in  1994 okay stated in the second document  with the page content James born in  1994 and maybe if I want to kind of say  okay give me the date as well i' say um  what date exactly which spawn and we  actually get this which is odd because  so it it says unfortunately the given  context does not provide definitive  information to answer the question what  dat exactly was James born but then then  it actually it gives us here so we have  I don't know that there's a little bit  of a lack of reasoning ability with  Claude in this case clearly so my  birthday is 7th December and I was born  in 1994 I don't know why it's kind of  surprising to me that I didn't get that  but interesting but at least we can see  that our chain is working correctly we  can see that it's pulling in information  from both our retrievers there which is  cool and we're almost done with what I  think are the essentials of the  expression language there's just one  more thing that I think is super  important and it's basically line chains  abstraction of doing what I showed you  earlier where we created our own sort of  runnable class and fed functions into it  to create these you know things that we  can run with the pipe operator so to do  that in line chain they have these  runable lambdas okay and this is why  earlier on I called that class A  runnable because here they they call  them runnable lambdas so we have our our  add five and our multiply by two I'm  going to just come up here and show you  what we we had earlier so yeah we have  these two functions let's take those  okay we can see runable it's what we  were doing before so that we could use  this let's do it again  here all right so we have our add five  and I'll multiply by two  let's run this this time we're doing  runnables but we're just doing them  through Line  train so our train is going to be at  five multiply by two as we did before  and as I mentioned you know line chain  we have to use infol rather than just  calling the object directly so we run  that and yes as before we get 16 so yeah  we can wrap our own functions using Lang  chains runable Lambda here now when  would we use that I mean there there are  definitely different scenarios why we  might want to use that but let me just  show you something here which you know  kind of bothers me a little bit and it's  a good example where we might want to  use this either use this or we'd  probably want to adjust the output parer  as well so we have let's run both of  these what we see when we run this is  one there's some leading white space  here that we could do removing but it  also starts each answer with this here's  a short fact about artificial  intelligence and then we have two double  new line characters maybe I don't want  that and I just want it to get straight  to the fact so what I can do is use this  runnable Lambda abstraction to to do  that right so I'm going to define a  function which is going to look within  this string for a double new line within  the string if that is in there we're  going to split by double new lines and  we're going to take everything that  occurs after the double new lines now in  the case that maybe there are multiple  double new lines we're taking everything  you know one from one to the end of the  list that we would get from this and  then we're joining everything back here  okay so we're basically just dropping  that first one the first part here so  let's run that I'm going to wrap that  within a runable line  and then I'm going to put all those  things together and I'm going to add the  get fact runable to the end of my  chain now let's invoke again and see  what we get okay so there's no weird  sarting text here and yeah we see with  both of those it know it works so our a  little runnable Lambda here works well  okay so that is really everything I  wanted to cover with the expression  language you know I think there's  there's other things that we can talk  about and more to cover but this is I  think pretty much everything you need to  really get started with it and just  understand what this abstraction is  actually doing which like I said at the  start it's important to understand  because then at least we know what we're  doing rather than just kind of you  putting in these pipe operators and kind  of thinking they should work when maybe  we're doing something that doesn't make  sense so I hope this has been useful for  understanding\", metadata={'source': 'O0dUOtOIrfs', 'title': 'LangChain Expression Language (LCEL) Explained!', 'description': 'Unknown', 'view_count': 7101, 'thumbnail_url': 'https://i.ytimg.com/vi/O0dUOtOIrfs/hq720.jpg', 'publish_date': '2023-12-07 00:00:00', 'length': 1538, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"so I hope this has been useful for  understanding the expression language  you know there's pros and there's cons  to using this now on the pros obviously  there's the Min andless style of the  code which is kind of nice it's very  clean and the out of the box support for  different features like streaming and  the parallel execution that we saw but  there are also some cons and you know  there's plenty of people that are less  fond of the expression language as you  know it's a big change it's to be  expected now the things that people  point to when they're like this doesn't  make sense is that it makes things more  abstract L chain is a you know  abstractions already so we're kind of  adding another abstraction to the  abstractions and that the syntax is it's  definitely not common syntax of python  and that kind of goes against the Zen of  python which is that kind of shouldn't  make special cases for things and of  course it's a new syntax it's especially  when you first look at it I think once  you've explored it a little bit it makes  sense but when you first get started  with it it's it's definitely confusing  so in my honest opinion I think both of  those viewpoints are entirely  valid there's pros and cons for for sure  but I like it I I think it it's  definitely worth learning and  experimenting with and it can definitely  speed things up when particularly when  you're  prototyping and maybe in production code  you know it's going to depend on what  you're wanting to what you're wanting to  do there anyway that's it for this video  I hope all this has been useful in  understanding the expression language so  thank you very much for watching and I  will see you again in the next one  bye\", metadata={'source': 'O0dUOtOIrfs', 'title': 'LangChain Expression Language (LCEL) Explained!', 'description': 'Unknown', 'view_count': 7101, 'thumbnail_url': 'https://i.ytimg.com/vi/O0dUOtOIrfs/hq720.jpg', 'publish_date': '2023-12-07 00:00:00', 'length': 1538, 'author': 'James Briggs'}),\n",
       " Document(page_content=\"one of the most effective strategies to  improve the performance of your language  model applications is to split your  large data into smaller chunks the goal  is to give the language model only the  information that it needs for your task  and nothing more this practice is the  Art and Science of text splitting it is  one of the first and most foundational  decisions a language model practitioner  will need to make text splitting takes a  minute to learn but in this video you're  going to learn the five levels of text  splitting that squeeze out more  performance from your language model  applications using the same data that  you already have now there's something  for everyone in this video for the  beginners we're going to start from the  very Basics and for the advanced STS I'm  going to give you plenty that you're  going to want to argue with me on but  either way I guarantee you're going to  learn something along the way this is  going to be a longer video and we're  going to cover a lot but that's on  purpose I want to take our time and I  guarantee that if you make it to the end  you're going to have a solid grasp on  chunking Theory strategies and resources  to go learn more for those that are just  joining us my name is Greg and I'm  exploring the AI space through the lens  of business value you see models are  cool stats are cool but I want to find  out how businesses will actually be  taking advantage of AI and language  models this video will be split up into  six different sections first we're going  to talk about Theory we'll talk about  what splitting and chunking are why we  need them and why they're important I  even made a cool tool called chunk  vi.com to help us visualize along the  way then we're going to jump into the  five levels of text splitting for each  level we're going to progressively get  more complex and introduce topics along  the way for you to consider when you're  building your own language model  applications for level one we're going  to talk about character splitting this  is when you split your documents by a  static character limit for level two  we're going to talk about recursive  character text splitting this is when  you start with your long document and  then recursively go through it and split  it by a different list of separators for  level three we're going to talk about  document specific text splitting so if  you have python Docs or JavaScript docs  or maybe PDFs with images we're going to  include multimodal in this level as well  for level four this is where it gets  interesting we're going to talk about  semantic splitting so the first three  levels were all naive ways of splitting  these levels focused on the physical  positioning and structure of the text  chunks these first three levels it's a  bit like sorting a library based off the  book sizes and shelf space rather than  the actual content of the books but in  level four here we're not just going to  look at where the text sits or its  structure instead we're going to start  to delve into the what and the why of  the text the actual meaning and context  of these chunks it's like understanding  and categorizing the books by their  genre and themes instead and then with  level five we're going to talk about a  gentic splitting so we're going to look  at an experimental method where you  actually build an agent-like system  that's going to review our text and  split it for us and then to finish it  off we're going to end with some dessert  a bonus level that shows the advanced  tactics that start to creep a little  Beyond Tech splitting but are going to  be important for your overall knowledge  about how to do retrieval in general my  goal isn't to prescribe the best or most  powerful method you'll see why that's  actually not possible my goal is to  expose you to the different strategies  and considerations of splitting your own  data so you're able to make a more  informed decision when you're building  this is part of a larger series on  retrieval I ofo and if you want to check  out more or get the code for this  content head over to fullstack  retrieval.com and I can go send to  lastly I do a lot of workshops with  individuals and teams if you your team  or your company want to chat live or do  a custom Workshop just feel free to  reach out so without further Ado let's  jump into it first we're going to start  off with a theory behind text splitting  what is it and why do we even need to do  it in the first place you see well  applications are better when you give it  your own data or maybe your user's data  but you can't pass unlimited data to  your language model and there's two main  reasons for this number one applications  have a context limit this is an upper  bound on the amount of data that you can  actually give to a language model you  can see the context windows on open AI  websites for their own models and number  two language models do better when you  increase the signal to noise ratio let's  see what Anton co-founder of chroma has  to say about this distracting  information in the model's context  window does tend to measurably destroy  the performance of the overall  application so instead of giving your  language model the kitchen sink and  hoping the language model can figure it  out you want to prune the fluff from  your data whenever possible Now text  splitting or chunking is the process of  splitting your data into smaller pieces  so you can make it optimal for your task  and your language model now I really  want to emphasize this point the whole  goal of splitting your text is to best  prepare it for the task that you  actually have at hand so rather than  starting with hey how should I chunk my  data your question should really be  what's the optimal way for me to pass  the data that my language model needs  for my task our goal is not to chunk  just for chunking sake our goal is to  get the data in a format where it can be  retrieved for Value later so let's talk  about retrieval in general so in the  bigger picture the act of gathering the  right information for your language  models is called retrieval this is the  orchestration of tools and techniques to  surface up what your language model  actually needs to complete its task  let's take a look at where chunking fits  into the retrieval process so here we're  taking a look at the full stack  retrieval process we have everything  from your raw data sources to your  response here if you want an overview  about this entire process head over to  fullstack retrieval.com where I do a  separate tutorial on this now the  important part is we're all going to  have our raw data sources down at the  bottom here and they eventually need to  make it into our knowledge base right  however we can't just put our raw data  sources we're going to need to chunk  them which is what this video is about  now right when you do your data loading  this is where your chunking strategy is  going to come into play how you choose  to split up your documents is a very  important decision as you go through  this you'll see that there isn't one  right way to do your chunking strategy  or really your retrieval strategy for  that matter for example take a look at  this tweet from Robert hir for those  that need a translation Robert is  basically saying that he employs many  alternative strategies across his  retrieval stack what works for him may  not work for you the last thing I'll  comment on is the topic of evaluations  evaluations are super important when  you're developing your language model  applications you won't know if your  performance is improving without  rigorous testing one of the most popular  retrieval evaluation Frameworks out  there is ragas I encourage you to go  check it out I won't be covering those  today because it's more of a retrieval  topic rather than this narrow Niche that  we're going to be covering today plus  they're very domain specific and  application specific if you want my  taken evals please head over to  fullstack retrieval.com and you'll get a  notice when I start to cover it all  right now that is enough talking for now  I finally want to get into some code  let's move on to level one character  split all right so level one charact  character splitting before we jump into  that I want to talk about the chunking  commandment your goal is not to chunk  for chunking sake your goal is to get  our data in a format where it can be  retrieved for Value later I'm placing so  much emphasis on this point because it  doesn't matter what your chunking  strategy is if it doesn't serve your  Downstream task keep that in mind as we  keep going here so level one character  splitting this is the most basic form of  splitting and this is when you're going  to chunk up your text by a fixed static  character length let's talk about what  that means first the pros it's extremely  simple and easy the cons it's very rigid  and doesn't take into account the  structure of your text and to be honest  I don't know anybody that does this in  production I don't let me know if you do  cuz I'm curious the two concepts I want  to talk about for this one we're going  to see what chunk size is and we're  going to talk about what chunk overlap  is but let's use examples to explain  those for our text this is the text I  would like to Chunk Up it is an example  text for this exercise cool we got that  one so before we talk about packages  that help us do this automatically I  want to show you how we do this manually  first just so you can appreciate the  nuances for how cool some of the stuff  is so in order to create our chunks I'm  going to first create an empty list of  chunks now my chunk size is going to be  35 this stands for 35 characters so I'm  going to count 35 characters in count  that as chunk one next 35 characters is  chunk two I'm going to run through this  I'm going to create a range and the  range length is going to be the\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"a range and the  range length is going to be the length  of my text that I have up above here and  then um the iteration step or the the  step we're going to take is the chunk  size so we're going to skip ahead every  35 characters I'm going to get the chunk  I'm going to unpend it and let's see  what our chunks are this is the text I  would like to CH Unk up it is an example  text for this exercise first off  congratulations you just did your first  chunking exercise do you feel like a  language model practitioner yet I sure  do let's keep on going here there's a  couple problems with this this is the  text I would like to Chu well it's stuck  in the middle middle of the word that's  no good um how do how are we supposed to  know how long this 35 character length  we could change this to 40 and then all  of a sudden this goes a little bit  further but dang again we have it one  more time that's giving us a hard time  that's not too good um so yes it's quick  yes it's simple but uh we need to fix  this problem now before we move on to  level two I want to talk about Lang  Chain's character splitter so their  character splitter that's going to do  the exact same thing for us but it's  going to be through a lang chain  oneliner and the way that you do that is  you're going to initialize a character  text splitter you're going to tell it  how much of a chunk size you want you're  going to tell it how much of a chunk  overlap you want we'll talk about that  in a second and when you do a blank or  an empty string as a separator we'll  talk about that in a second too that  means it's just going to split by  character and Lang Chain by default they  will strip the white space and so they  will remove the uh the spaces on the end  of your chunks I don't want them to do  that quite yet so I'm going to say false  so this is we just made our character  splitter now we're actually going to go  and split the documents and so I'm going  to say dot create documents and this  create documents function it expects a  list and because our string up above is  just a plain old string I need to wrap  it in a list right here let's go do that  text splitter so now what we get  returned is we get three chunks however  they look a little bit different than  plain strings the reason why is because  they're actually a document object now  in Lang chain a document object is uh  well an object that holds a string but  it can also hold metadata which is  important for us to understand when we  start doing more Advanced Techniques so  don't get scared documents uh still have  our string and they're held within page  content this is a text I would like to  chump it's same thing that we had up  above cool that makes sense so let's  talk about overlaps and separators here  so I'm going to make the character  splitter again it's going to be 35  characters but this time we're going to  have a chunk overlap of four now what  this means is that the tail end of Chunk  number one is going to overlap a little  bit with the head or the beginning of  Chunk number two and the overlap is  going to be 4 characters so the last  four characters of Chunk one will be the  same four characters of Chunk two let's  see what this looks like here and again  I'm just going to make these and what we  have is this is the chunk or this is the  text that I would like to we still have  the first same split the chunk size is  the same but check this out now the  first four characters of Chunk number  two are the same four characters as  chunk number one because we have that  chunk overlap all right now when I was  getting ready for this exercise I  thought I remember a tool that actually  visually showed you different chunking  Tech techniques with uh highlights of  the different chunks but I couldn't find  it so I ended up making a tool and that  tool is called chunk viz.com and this is  just a quick snippet of it but I want to  show you this while we're on the topic  so chunk number one was this first  beginning part and then we have the  overlap and then we have chunk number  two and then the overlap and then chunk  number three but if you want to try this  out for yourself it's kind of cool you  can go to chunk fis.com and then what  you'll get here is you'll get a tool  where you can input um different text  and uh play with your different chunk  sizes that we get get out so let's go  back up and let's grab the text that we  had I'm going to bring this over here  I'm just going to replace this and so  you can see here that right now our  chunk size is one with no overlap that  doesn't make any sense although it's  visually cool it won't do any good for  us because we have 83 chunks here what  are you going to do with 83 one  character chunks I don't know I'm not  going to do much with there but as you  start to increase this number you can  see that these different chunk sizes are  going to start to get bigger so we're  going to take this all the way up and as  we go through I'm going to put it at  what we had it before which is 35 so  this is a text that I would like to and  you see it ends right in the middle of  the CH just like we had beforehand let  me zoom in just a little bit more here  it ends just in the CH that we had  beforehand now if I start to introduce  overlap you can see that now we have a  little bit of an overlapping section so  we had chunk oversiz or chunk overlap  before this is the text that I would  like to so it chunk one still ends here  but now there's the overlap that comes  with it so what's cool is you can switch  this around to yourself you see above a  certain mark it ends up being I want to  get rid of this over  over a certain Mark well this chunk size  is bigger than the document that we have  so it's just going to encapsulate  everything but you can go to chunk  vi.com and go play around with this  we'll take a look at one more of these  sections uh in a minute here cool so  let's go down there those are characters  those are separators fabulous and so the  next thing I want to talk about is  separator so beforehand we just had a  blank string as a separator which means  you're going to split by character right  however if we specify another separator  in this case I'm going to do CH well  let's see what that says here this is  the text that I would like to and you  see here that the CH is missing and the  space is missing because I removed the  uh strip whites space and so it's  Default true so that's gone and you see  that this word is supposed to be chunk  up but it's not anymore because we  removed the CH it's not too helpful when  we do ch um you could do the letter e if  you wanted and it's going to be a little  bit different either way unless you know  exactly what you're doing I wouldn't  suggest messing around with the  separator to try to get better results  here all right so that's the Lang chain  side of the house the next next one I  want to show you is the Llama index side  of the house so they have what they call  a sentence splitter but also I'm going  to use their simple directory reader  because this time instead of just using  a a static string that I put in the code  we're actually going to load some essays  from a directory so I'm going to make  the sentence splitter and this time I'm  going to have a chunk size of 200 and a  chunk overlap of 15 and then I'm going  to load up some essays so my input files  I'm just going to load up one essay this  data is also in the repo so if you go  and clone this repo you can also get  this data pretty easily this is going to  be a Paul Graham essay and this is going  to be his MIT essay so we can go check  out Paul Graham's MIT essay and we can  see what it is right here you can go and  read it I have it loaded up for you so  let's go ahead and load this now we have  our documents but this document is just  going to be let's let me show you here  let's check  out let's check how long this document  is and it's just one big long document  because the entire essay was loaded into  this variable here however we want to  chunk it up and the way we're going to  chunk it up is with our split splitter  we're going to say get nodes from  documents now you may be asking hey Greg  wait what's a node well llama index is  nomenclature for a chunk or a subsection  of a doc is going to be nodes and so  that's what we're going to get here same  thing so now that we have our nodes I  want to take a look at one so as you can  see this node is quite long based off of  the amount of text that's in here but  there's some really cool information  that comes out of the box so first of  all this node has an ID and we can see  here that it's a text node because they  delineate from other nodes so we have a  node ID so we can go and uh use it later  and deal with its uniqueness and then we  also have some metadata so we can tell  where it came from we can have last  modified date Etc but then one of the  other parts I like a lot is going to be  around node relationships so here we  have a relationships key and we can take  a look at other relationships this node  has so node relationship we can see the  source node that it came from but then  also we can take a look at the next node  so what node actually comes next and  this is really helpful for when you  start doing some traversing across your  documents but either way I won't go too  far into that one well congratulations  we just finished level number one let's  head off to level number two recursive  character text splitting so you'll  notice that in our previous level one we  split by a static number of Chunk sizes  each time so 35 characters by 35  characters however there's other  chunking mechanisms that will actually  look at the physical structure of your  text and it will infer what type of  Chunk sizes you should have so instead  of specifying by 35 characters you can  then specify well give me every new line  or give me every\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"well give me every new line  or give me every double new line and  that's what recursive character text  splitter does so what it's going to do  is it's actually going to have a series  of separators and it's going to  recursively go through these documents  and it's going to start at its first  separator and it's going to first Chunk  Up by every new double new line that you  have here for any chunks that are still  too large after that first iteration  it's going to go to its next uh  separator and that's going to be just  new lines and then it's going to go to  spaces and then it's going to go to  characters so now I don't need to  specify 35 characters or 200 all I can  do is I can just pass it my text and it  will infer what the structure should be  now the cool part about this one is if  you think about how you write text  you're probably going to separate your  ideas by paragraphs and those paragraphs  are separated by double new lines this  method takes advantage of that fact and  so we can start to be smart about which  separators that we use to take advantage  of how humans naturally write a text all  right so let's go ahead and let's check  this out we're going to do Lang chain  text splitter we're going to do the  recursive character text splitter now  again I'm going to take some text but  this one's going to be a little bit  longer that we have here right and so  let's do that text and then let's put it  through our recursive character text  splitter this is going to be a 65 uh  character limit we're going to pass it  through and then all of a sudden you can  see that we get a whole bunch of  different chunks here now I'm not even  sure how many chunks let's see how many  we actually have we have six 16  different chunks all right so with that  one of the most important things I  didn't understand about the for end of  sentence end of sentence day you can see  here that what's cool is that we're  ending on Words quite often and that's  because words have spaces in between  them and that is one of the separators  that we try out so this is cool now  we're not splitting in between words  anymore however we are starting to split  in between sentences that's not so good  that's not so fun um one of the ways  that we can combat that is we can  increase the chunk size because our  hypothesis is that if we inre increase  the chunk size we can start to take  advantage of the paragraph splits a  little bit more all right so now what  I'm going to do is I'm going to increase  the chunk size to 450 still chunk  overlap of zero and let's see what we  have here one of the most important  things I didn't understand about the  world with without child was the degree  to which performances are super linear  cool so there's a period here's a period  and here's the end of it now what's  interesting is all three of those let me  scroll down to this viz again this is  the same exact uh string that we have  those are all three different paragraph  breaks hm that's pretty interesting so  one of the important things to note here  is look how these these paragraphs are  different lengths if I use level one in  the 35 character split or any character  split I'd start to cut in the middle of  them but now I can get these paragraphs  grouped together and the hypothesis  behind this method is that these  paragraphs will hold semantically  similar information that should be held  together so the recursive character text  this is pretty awesome let's go take a  look at what this looks like at chunk  vi.com I'm just going to go ahead and  copy this go back to chunk fis.com let  me put this text in there and you can  see if we did the character splitter of  35 uh text we have 26 different chunks  we're chunking all over the place this  doesn't make any sense but what I'm  going to do is I'm going to actually  scroll down I'm going to select the  recursive character text splitter and  now I still have a chunk size of 35 but  we're going to increase this one so the  first thing I want to show you is as I  start to increase this notice how I go  between 35 and 36 the split the first  chunk here doesn't switch sizes that's  because it's looking for the space to  actually split on and because there's a  space here it snaps to the nearest word  this is why it's so cool so I'm going to  increase this let's see when it finally  does split and there it goes it just  jumped up to a degree but either way let  me save you for this here I'm going to  select the chunk size I think we wanted  maybe like 450 I forget what it was let  me zoom out just a little bit let's go  four maybe four I mean either way look  it now we're splitting all these three  different paragraphs and we can even  increase the size and it doesn't really  do much for us but all of a sudden if I  go too big well it's going to chunk the  first two paragraphs together because  that's um this 493 is around the size of  this these two combined anyway you can  go and split this again you can get so  big that it finally takes over the third  paragraph all right so that's it for  level two congratulations recursive  character text splitter if I'm starting  a project this is my go-to splitter that  I use each time the ROI for your energy  to split up your docks is pretty awesome  it's a oneliner it goes really quick  there's no extra processing that's  needed so if you're looking for a go-to  place to start I recommend with level  two the recursive character text  splitter let's move on to level three  document specific splitting so up until  now we've been splitting just regular  Old Pros we've had some static strings  and we have had some Paul gram essays  but what if you have markdown what if  you have python docs what if you have uh  JavaScript docs there's probably a  better way to split on those because we  can infer more about the document  structure from special characters within  those documents because when you have  code you start to have some code  formatters and we can take advantage of  those  all right so the first I want to look at  here is for markdown so we're still  going to have something that's like the  recursive character text splitter  however we have a lot more separators  now the reason why this is so cool is  because let's take a look at this first  separator it's a new line and then it's  going to be a pound symbol which  indicates a heading within markdown and  this Rex here means one pound symbol  between one and six times so it's a new  line followed by a header H1 through H6  why would we do this well headers  usually denot what what you're going to  be talking about so this is a cool way  to try to group similar items together  so these are the Lang chain Splitters if  you have your own different package you  might see other Splitters but if you  want to see the Lang chain side of the  house you can head over to their GitHub  and you can go find on Lang chain Libs  Lang chain Lang chain texts spitter dopy  you can see that they have a markdown  language and here are the Splitters that  they actually end up using all right so  let's go ahead and load up our Lang  chain markdown splitter I'm going to do  a chunk size of 40 which is again this  is really really small my first go-to  for chunk sizes is going to be anywhere  between 2,000 4,000 5,000 6,000  different chunks and as contact lengths  for language models get better and their  performance with large context gets even  better well you're going to start to  increase this a whole lot because it can  infer what you want there all right so  let's go ahead and do that and then so  here's some uh markdown text fun in  California uh H2 driving blah blah blah  blah blah blah cool and let's split  these up and so we can see here uh the  first document is fun and California  driving and what's cool is that it split  it on these headers and so split on  header here etc etc split on header here  which is nice that's markdown you can do  this also for python but instead of  using the markdown Splitters you're  going to want to have your own python  Splitters so again Lang chain is going  to split on classes uh functions  indenting functions so these might be  methods within your class or you might  have double new lines new lines spaces  and characters they have a python code  text splitter let's go ahead and run  this let's see what we got chunk size of  100  and we scroll and we can see that we  have uh this whole class is enveloped in  within one document which is cool  because that's what we'd want but then  we have uh P1 equals John equals person  we have this stuff and we have the  ranges right here um I put this over on  the uh chunk uh chunk fizz.com and you  can go ahead and you can throw this in  there you can go to python Splitters and  let's see how how long this that was 100  let's go ahead and bump this up to  100 we have it at 100 right here and  Lang chain I couldn't figure out how to  undo the strip whites space with them  within the JavaScript version to make  this tool which is why I'm a little  hesitant to show it but either way you  can see that we're splitting right there  that's what we'd want nice all right  same thing we have uh JavaScript uh we  also have just a bunch of different um  separators that we have here this is  very similar but in this case we're  going to uh do our recursive character  text splitter again but now we're going  to specify which language we want it to  split by so here's our text recursive  character text splitter. from language  this time and we're going to do a  language which is going to be language.  JS chunk size 65 let's go through there  let's do it and then all of a sudden we  split up our JavaScript code as well  cool those are all strings those are all  pretty easy because they might just be  in txt files which is simple enough for  us to work with however what if you have  PDFs everyone loves talking about PDFs  they especially love\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"loves talking about PDFs  they especially love talking about  pulling tables from PDFs this is because  there's a lot of old school industries  that still put information inside a PDF  so when it comes to chunking you're not  only just going to split text but you  just want to pull out all the different  elements within your documents and some  of those might actually be tables it  might be pictures it might be graphs  let's take a look at how we do this here  so the way I'm going to do this is I'm  going to load up a PDF right here and  this is just going to be a Salesforce  Financial PDF I went over and I just  pulled one of the random PDFs that we  have here so I can have a table all  right um so we have this and the way I'm  going to do it is I'm actually going to  do it via unstructured now unstructured  is another Library we can check go check  them out they're at unstructured doio we  get your data llm ready so they have  some really cool parsers that you can  use which is going to be advantageous  for when you start to get more  complicated data types or your data gets  a whole lot more Messier so for example  if you had a million PDFs that you  somehow needed to get into a structured  form unstructured would be who you win  with that not a sponsored video by them  at all all right so we're going to load  up two uh Elements by them it's going to  be partition PDF and then elements to  Json we're going to get our PDF and if  we take a look at this Salesforce PDF  here you can see that we have a few kind  of just like notes or whatever then we  have some paragraphs but then we have  this table and this is where I'm going  to start to um Place more emphasis on  and then we're going to do uh partition  PDF we're going to give it the file name  and then we're going to give it some  unstructured helpers this is just a  little bit of config for them so if we  take a look here let's load it up let's  see what elements I actually found and  so I found a whole bunch of them here it  looks like we just have some narrative  text this is going to be your regular  text but then we have this table and  that's actually what I want to double  click on and so what I'm going to do is  I'm just going to go grab the fourth  from the last element which is going to  be this table right here 1 2 3 4 and  let's look at what the HTML looks like  so here we have the HTML and now you  might be saying well Greg how come the  HTML is important why would you just  want to read it like a table well tables  are easy for us to read They're not so  easy for the language model to read  however the language model has been  trained on HTML tables not only HTML but  also markdown but in this case I want to  pull out the HTML table because the  language model is going to be able to  make more sense of this than I can so  when I pass my data to the language  model I'm going to pass the HTML or you  can pass markdown whatever works for you  and if you want to see what this  actually looks like you can head over to  an HTML viewer and you can see what the  um unstructured actually pulled out  which is pretty cool nice so that's how  you do tables within PDFs but now let's  say you have images within PDFs or maybe  you have images elsewhere how you going  to take advantage of those  how are you going to extract those well  let's take a look at how you do that and  I'm going to use unstructured once more  this time I'm going to do uh their  partition PDF which is the same as last  time and here I have a fine-tuning uh  visual fine tuning paper you can go and  check this out let's go look at the  archive one I'm going to download the  PDF just so you can see it because  there's this wacky photo up at the front  all right so here's the same one I'm  going to load up that page that I had  and then I'm going to get the uh  partition PDF ready for me and this time  I'm going to do extract images in PDF  equals true so now it's going to extract  all the parts for me which is the  chunking process but it's going to treat  the images separately which is nice also  iner table structure blah blah blah  let's go from there image output  directory path this is in this repo uh  so you don't have to reload the images  if you don't want to but either way I'm  going to load those up and I don't want  to make you wait this does take just a  little bit of time so let me come back  to you 1 minute later awesome so that  just finished up loading for us let's  take a look here so I know it's kind of  small on the screen but I they found  looks like 15 or so different images or  16 different images on here and that was  all extracted from the PDF that I  supplied now the interesting thing about  this is how are we going to make those  images useful right we would need to  take an embedding of them because we're  probably going to do semantic search  later however embedding models usually  don't cross paths between text and  images meaning there's embedding models  for images and there's embedding models  for text but generally their Vector  length isn't going to line up and if you  don't have the same model for each one  doing similarity search between the two  may give you a hard time yes I know for  all the perfectionists out there there  is something called the clip model which  is going to do embeddings for both  images and text so you can take  advantage of them however the tech is  still not quite there and I haven't  found the same performance with clip  that I have with other ones so I'm going  to show you a different method here I I  will note this to say that in the future  and when you're watching this there may  be really good models that do both of  these if it's true then I would take  advantage of those uh instead of the  method I show you but either way let's  take a look here so what I want to do  actually is I want to generate a text  summary of each image and then I'm going  to do an embedding of that text summary  so now what I can do is I'm going to go  to semantic search maybe the text  summary will get returned for me if so  then I can pass that image to a  multimodal llm or I can just use the  text summary on its own to answer my  question or do my task cool the way  we're going to do this is we're actually  going to use langen and here we are  we're going to load up uh chat open Ai  and we're going to use the gp4 vision  preview model all right so I'm going to  load that up and I made a quick function  here this is just going to convert the  physical file on my local machine to Bas  64 which we uh then can go and pass to  the language model so string to base 64  now we have this image string let's go  take a look at what this looks like just  looks like a bunch of gobbly go which  that doesn't mean much to me but it will  mean something to open Ai and I'm glad  that it does all right let's close this  let's get that out of there so what I'm  going to do is I'm going to use the GPT  for vision one again and we're GNA  construct a human message this just  means it acts as if it's coming from the  human content type please give me a  summary of the image provided be  descriptive and then we're going to pass  it an image URL and here we are the URL  is we're going to pass in our image U  base 64 we had there let's go ahead and  pass that over and let's see what openi  thinks the image actually is I haven't  shown you what the image is but let's  look at the summary the image shows a  baking tray with pieces of food like a  cookies or some baked goods arranged  Loosely to resemble the continents on  earth as seen from space hm what but do  you know there you go yeah that makes  sense so now when I do my retrieval  process I can either just use this text  in lie of the picture if I don't want to  work with a multimodel llm or I can do  semantic search have this summary get  returned and then pass this image over  to the language model the llm all right  so that seems about right so what I've  done in level three here is emphasizing  that your chunking strategy really  depends on your data types so in this  case I was pretty explicit about that  and I showed you what it would mean for  Python and JavaScript and if you have  images but in your industry in your  vertical you may have different data  formats and you'll want to pick a  chunking strategy that is going to adapt  to those data formats because remember  the ultimate goal is that you want to  group similar items together so that you  can get them ready and prepared for your  language model task in the end now  you'll see there that I even made an  assumption that you want to group  similar items together I'm just saying  that because generally you're doing  question and answer and generally you  want to combine similar items together  for context to answer a question however  if you're not doing that maybe for some  reason you want to combine opposite  items together in in which case your  trunking strategy be a lot different I  don't know of anybody who actually do  that but let's get back to the tutorial  here all right so now we're moving on to  level four semantic chunking now the  interesting part about levels 1 through  three here is we all took physical  positioning into account doesn't it seem  kind of weird that we would split up a  document with the intention of grouping  similar items together we just assume  that paragraphs have similar information  in there what if they don't what if we  have really messy information and doing  recursive character text splitting  doesn't really do anything for us you  know I saw this tweet from lonus we can  go and take a look at this one he  says weird idea chunk size when doing  this when doing retrieval augments  generation is an knowing hyper pram and  feels naive to turn it into a global  constant value I totally agree now he  recommends could we train an end to-end  chunking model I didn't want to go quite  that far because I think\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"didn't want to go quite  that far because I think there's a  little easier step that we could try  beforehand and I wanted to do an  exploration but now what I'm going to do  is I'm going to do an embedding based  chunking method it's a little bit more  expensive and it's definitely more work  and it's definitely slower than what we  talked about for the first three but it  starts to take the meaning and the  content of the text into account to make  our chunks the analogy I looked up  beforehand is imagine the first three  levels that's like having a bunch of  books and putting them on a bookshelf  depending on their size right and the  bookshelf size but what if you want to  group the books together by genre or by  theme or by author well then you  actually need to know what the books are  about and that's what we're going to try  doing level four here all right so when  I thought about level four what I wanted  to do was obviously semantic chunking  and I chose an embedding based way to do  this so what I wanted to do was I wanted  to take embeddings at certain positions  of our document and then I wanted to  compare those embeddings together right  so if two embeddings are close to each  other distance-wise  well maybe they're talking about the  same thing that's the assumption that  we're going to make if they're further  from each other that means that they're  maybe not talking about the same thing  right so what I imagine is I we'd have a  big long essay and then with those I'd  take an embedding of every single  sentence that we have right and then I  want to compare those embeddings  together now the comparing the  embeddings that's going to be the  important part and where all the magic  is going to be for this and I did two  different methods that I wanted to share  with you the first one is I did  hierarchical  that's a mouthful hierarchical  clustering with positional reward so my  first thought is well you know let's  just do a clustering algorithm and let's  see which embeddings are clustered  together and then let's assume that  those are the chunks that we're going to  have but one thing I wanted to do was  take into account short sentences that  appear after a long sentence you know  just like that I wanted the you know to  be included with that long sentence  because it's likely needs to be relevant  with it and so I added a little bit of a  positional reward so hierarchal FAL  clustering generally is just going to be  based off of distance but I added a a  little uh extra sauce to it and did some  positional reward all right this one was  okay but it was kind of messy to uh to  work with and it wasn't as logical as I  wanted it to be and I couldn't really  tune this um intuitively like I wanted  so I wanted to find something just a  little bit easier as an exploration for  me so the what I did was is the next  method was to find break points between  sequential sentences so I got embedding  number one of sentence number one and I  compared that to sentence number two's  embedding and I measured the distance  between them and then I got two compared  it to three and then three compared it  to four and so on and so forth um I do a  visual with this and so I guarantee it's  going to make more sense in a second  we're going to use Paul Graham's essay  and what I'm going to do is I'm first  going to split all of my different  sentences and I'm going to do that just  via some rejects with a period a  question mark or an explanation point  there's likely a lot of better ways to  do this don't come at me with that but  either way we have  3177 different sentences in this Paul  gram essay all right so what I want to  do is I want to start adding more  information to each one of these  sentences so it's like I have a lang  chain document but I'm just going to do  my own to show you how we're going to do  this so instead of having a list of  sentences I want to have a list of  dictionaries of which the sentence is  placed in it I'm going to add in the  index just CU it's fun why not and let's  take a look at these first three after I  do that now I have a list of  dictionaries and one of the keys is  sentence and now we have these different  sentences up here CU if I were to go  here let me just show you what this  looks like the single sentence list I  want to do this with this the first  three again it's just a list of strings  now these list of strings are list of  dictionaries all right cool well now  what I want to do is I actually want to  do some combining of the sentences like  I said if I just did sentence one  compared to sentence two compared to  sentence three it was a little noisy it  was kind of all over the place and it  didn't tell me much I thought you know  what if I combined the sentences so  there's a little less movement from each  one cuz now what I want to do instead of  comparing one to two comparing to three  comparing to four Etc I'm going to  compare the embedding of sentence 1 2  and 3 combined with sentence 2 3 and  four combined then compare that with  sentence 3 four and five combined so  it's a little bit more of a group I did  a just a small little function you could  take advantage of here I have a buffer  size of one means one sentence before  and one sentence afterwards you can do  whatever you want go and switch around  with this and go play with it I won't go  through this code but I've commented it  so you can follow along if you want now  let's take a look at what that does so  here we have our original sentence but  now we have our combined sentence all  right and this combined sentence is  going to be what comes uh before and  after it because this is the first one  there's nothing before it's only after  so um get funded by Y combinator is the  sentence of number two nice so we have a  combined sentence here which is want to  start or startup that's what sentence  number one is and then uh something in  the grad school and that's what sentence  number uh three is cool now that we have  those what I want to do is I want to get  an embedding of the grouped sentences of  this combined sentence key so I'm going  to use open Ai embeddings and let's go  through this and I'm going to get all  the embeddings um which is basically get  the combined sentence for X in each one  of those sentences and this is going to  be we're going to go grab all those  which is really nice we have our  embeddings now I need to put those  embeddings with its proper list all  right so sentence uh with the now I'm  going to make a new key the combined  sentence embeddings and I'm just going  to go through and add those and let's go  take a look at what that looks like now  CU of course this is fun and I like  doing this in an iterative nature so we  can take one step together at a time all  right so sentence want to start a  startup here's our combined sentence  embedding so now we have this embedding  for what's up here all right cool um  well now what I want to do is I want to  add one more metric to it and I know we  keep on going here but hopefully you're  still following along I want to add the  distance between the first uh sentence  and the second group of sentences I want  to add that to the first sentence so I  can see how big is the jump with the  next one all right so what we're going  to do is we're going to get the  embedding of the current thing we're  going to get the embedding of the second  thing the second group that it comes  with we're going to get the distance  we're going to append the distances  because we're going to do something with  this later but then we're going to start  we're going to get a distance to next so  how far is the distance between the  current embedding with the next one  let's go ahead and let's run this and so  now we just got that and this is added  to our sentences but we have our  distances here too so let's just take a  look at the first three distances  awesome so this is 08 so this means that  sentence number group number one is 08  distance away from group number two and  group number two is0 2 distance away  from group number three hm that's kind  of interesting why is group one further  away from group two than group two is  further away from group three I don't  know but we're going to do something  with this in a second let me show you  these sentences look like one more time  just because we're doing this  iteratively oh boy I added a whole bunch  here there's too many I should have did  just did the first three okay um we'll  go through this let's scroll all the way  down to the bottom we got a long ways to  go okay now we finally have it distan to  next because this is the first one you  can see that it's 08 that's what we just  saw above cool let's close that up uh we  have our distances here but now we're  all data people we're all having fun we  all want to see some visuals I want to  see some visuals let's do that any data  scientists out there will laugh at this  because I've typed this more in my life  than I think I've ever should import  matplot live. pyplot as PLT that is just  absolutely muscle me memory for me right  now all right so now we plot our  distances hm cool so this is our  distance now it looks kind of random  doesn't it well I mean a little bit you  can see here that it looks like there's  a little bit of some e and flows this  one there's a little bit more distance  in between so what this would mean in  English is that for some reason the  chunks here are more dissimilar from  each other than uh further down than  chunks that are grouped together but  either way what's interesting to me is  that we have some outliers up at the top  here you can see here there's these  points up the very top and that tells me  hm maybe there's good break points there  because two groups are so dissimilar  that they should actually be chunked up  and they shouldn't actually be together  because if there's a long distance\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"be together  because if there's a long distance in  their embedding space maybe they're not  talking about the same thing all right  so I want to show you this one more time  but let's iteratively build another  visualization to further emphasize the  point that I'm trying to make here all  right first thing I'm going to do is I'm  just going to plot the distances let's  go down that's the exact same that we  think that we had beforehand all right  the next thing I want to do I just want  to do a little bit of formatting don't  hate me for this first thing I'm going  to do is I'm going to do a y upper bound  this means the bound of the upper y  limit because you as you can see right  here there's not enough cushion up here  for me it's visually too off I want to  fix this all right and then we're going  to do a y limit of from zero to the Y  upper bound which means how long is your  y AIS and then we're going to have an X  limb of how long you want this to be on  the X limit because you see here there's  these buffers in between I don't want  that all right let's get rid of that and  anyway we go through this we have more  space up the top we got rid of the sides  all right what's next let's see what we  have here I don't even know what's going  on here I wrote this code I I still  don't I'm just kidding I do um  breakpoint percentile threshold so what  we need to do is we need to somehow  identify which outliers do we want to  split on now there's a million ways you  could go about doing this and I'm really  excited to hear alternative methods for  you um that you may have in the back of  your mind for me what I ended up doing  was I just did a percentile base I  wanted to identify the outliers so using  the different points as a distribution I  wanted to find out hey which points are  in the top 5% because if it's in the top  5% well those are probably going to be  outliers for us so I only want to take  these um upper these upper distances  right here and the way I'm going to do  that is I'm going to specify the  percentile I want to take and then I'm  going to use numpy and I'm going to go  mp. percentile I'm going to pass it my  distribution of distances and I'm going  to get the break point percentile  threshold it's going to be 95 and then  what I want to do is I want to draw a  line on the graph showing where that  threshold is so I'm going to draw a a  horizontal line right across and the Y  is going to be the breakpoint distance  threshold this will be a number that  says hey what is the 95th percentile of  all these all right let's take a look at  what that looks like and here we go so  now anything above this line is going to  be in the 95th percentile these will be  my outliers where I'm going to  eventually make my chunks so everything  that's in right here up until this one  point that'll be chunk one then we have  chunk two then we have chunk three chunk  four chunk five blah blah blah and going  from there because again one one last  time I know I keep on repeating this but  I really want to hammer this home the  hypothesis is that if there's a big  break point then a chunk should be uh  split up at that point and so this is  where we're end going to end up doing  that all right then what we're going to  do is we're going to see how many  distances are actually above this one  and so I want to get the number of  distances that we have meaning the  number of break points the number of  things that are going to be above that  threshold and then I'm going to do PLT .  text this is just a fancy way to put  some text on your visualization let me  go ahead and do that and so we can see  that we have 17 chunks I put that down  in the corner right there H that's kind  of interesting all right um then what  we're going to do is we're going to get  the indices of which points actually are  above the breakpoint meaning which are  the actual outliers because this break  uh this break breakpoint distance  threshold this is just a single static  number but I need to get a list of  numbers to find out where these break  points and these chunks actually need to  be met so for I IND distances um if x is  above the breakpoint distance threshold  so I'll get a BN a bunch of indices  there that doesn't do anything different  for us but then what we're going to do  actually let me just do this because I  think this actually would be helpful I'm  going to look at these indices now what  we have here these are 17 different  chunks we can look at this it says 16  but that's cuz there's an extra one  added to the front there should be a  zero right here um or this could be the  317 at the end so what this means is  between uh between sentence zero and  sentence 23 we want to make our split  and make our break all right because at  this at number 23 it says Hey here the  distance to the next one was quite big  so we want to include number 23 on this  one okay either way let's go from there  let's go do some more uh let's do some  fancy colors on here so what I want to  do is I want to add some colors I just  set my own custom ones right here then  what I'm going to do is I'm going to go  do uh a vertical span meaning you're  gonna have a vertical shading in the  background of your um the background of  your graph here and you're going to do a  start index and an end index the reason  why I do a for Loop here is because you  add them one at a time but the start  index and the end n index will be what  is in our indices above thresholds  anyway's go through there and you can  see here I cheated a little bit let's  take away this text we can see that we  have our different chunks right here so  now we have chunk zero chunk one chunk  two blah blah blah and go go all the way  through there but of course we want some  text on here to really make it more  explicit and you can see here chunk blah  blah blah blah blah uh this last one was  giving me a hard time so I actually had  to do just just a little bit of custom  code for that one uh that splits it up  uh that's just a little bit of a  Band-Aid don't don't add me for that one  either I was too lazy to figure that one  out Okay cool so we go through there so  here's all of our different chunks now  this wouldn't be a good chart unless we  put on some graphics or some titles as  well so now we'll do a title A Y Lael  and an X  label and then there we go hm that's  kind of cool uh Paul Graham essay chunks  based off embedding  breakpoints H that's pretty interesting  but uh a good visualization doesn't do  anything for us we can't really pass  this to the language model and it's not  going to know what to do with it so what  we're going to do is we're going to  actually get the sentences and actually  combine them so here's a bunch of code  here um I won't go through it too much  but the the tldr is that you're going to  append all these different pieces in  your chunks so like I said beforehand  you're going to go from chunk zero to  chunk 23 and you're going to combine  those and that'll be your first chunk  let's go through there then let's take a  look at what this actually looks like  let's go through this and so we have the  uh chunk number zero about a month of  need phing cycle we had something called  a prototype day you might think they  wouldn't need any more motivation cool  they're working on their cool new idea  they have funding for an immediate  future and they're playing the long game  with the only two outcomes wealth or  failure hm you think motivation might be  enough so the hypothesis here is that  these two are actually getting split up  at a spot where there's a big breakpoint  with it so it's kind of interesting to  see where the uh semantic splitting  actually happened there you go and now I  want to reemphasize that this isn't  perfect of course but I think this is an  interesting step towards doing chunking  because if I were to think if I'm going  to hypothesize out in the future what is  chunking going to be like uh well as  compute gets better as language models  get better there's no way we're going to  do physical based chunking anymore  unless the um structure of our documents  is uh we can make those big assumptions  on it we're probably going to do a smart  chunking and I think this is a really  cool um way to go towards that all right  so that's level four again experimental  please let me know what you think please  give me other ideas for how we could  make this a little bit better and this  is going to be a little plug if you want  to see more of these experimental  methods that I do I shared this out on  Twitter so um other people got to see  this early now let's move on to level  five a gentic chunking so if we went off  the deep end with level four um we're  going into the ocean here we're going  into the Mariana Trench and we're going  to the very bottom and we're going to do  some cool things so my motivation for  this side is I asked myself hey Greg  what if a human were to do chunking how  would I do chunking in the first place  and I thought well I would go get myself  a piece of scratch paper cuz I can't do  all that in my head I'd start at the top  of the essay and assume the first part  will be in a chunk well because the  first little bit it needs to go  somewhere we don't have any chunks yet  so of course it's going to go in a first  chunk then I would keep going down the  essay and evaluate if a new sentence or  piece of the essay should be a part of  the first chunk if not then create a new  one then keep doing that all the way  down until the S on the essay until we  got to the end and I thought to myself  wait wait a minute this is pseudo code  we can make something like an agent to  do this for us now I don't like the word  agent quite yet because we're not quite  there yet a lot of people like using I  think there's a lot of marketing around  it so yes I call this agentic chunking  but I'm going to call this an\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"agentic chunking  but I'm going to call this an agent like  system the way that I like to Define  what an agent is is there's some  decision making that needs to go on in  there and you use the language model to  make decisions which you're going to  have an unbounded path but the language  model will help you guide that via your  uh decision-making that you do and so so  I thought man this is pretty cool I'm  going to try this out all right so now  let's go into level five and talk about  what I found here so one first design  decision that I need to make is how do I  uh want to give different pieces of my  essay to the language model and I  thought man well there's still that  problem with the short sentences you  know so around this time there was a  cool paper that came out all about  propositions what is a proposition well  a proposition is a sentence that can  stand on its own so there's another  agent-like system it's kind of just a  prompt we'll go over that in a second  here but it's going to take a sentence  and it's going to pull out propositions  which are little itty bitty Legos that  can stand on their own let's talk about  what that means Greg went to the park he  likes walking if you were to pass he  likes walking as a chunk to the language  model language model is going to be like  WTF who is he however if we change this  into propositions it will make less  sense from a reader's perspective  meaning it doesn't look great for us to  read but it makes a lot more sense for a  language model Greg went to the park  Greg likes walking that's pretty  interesting if you want to take a look  at more proposition work and I might do  a whole another video on this let me  know if you want me to because I think  this is really cool uh Lang chain came  out with proposition based retrieval so  a new paper by Tom Chen um I haven't met  Tom Chen yet but Tom I'm a big fan of  your work if you want to chat I'm very  down at talk in the image that they  showed prior to the restoration work  performed between blah blah blah blah  blah blah you have this big long  paragraph and then it's going to split  up into different propositions and then  use that for your retrieval because  these sentences can stand a bit more on  their own let's go ahead ahead and do it  I'm importing a whole bunch of Lang  chain stuff here I'm not going to go  over each one except for the for the  cool parts now one of the cool Parts  here is going to be this Lang chain  import Hub hey Greg what's the Lang  chain Hub I should probably do a whole  another video on this either but they  have this thing or Lang chain came out  with this thing called The Lang Hub this  is within their Lang Smith Suite but a  lang Hub is just going to be where they  share prompts around so this person I  don't know who this is um looks like  it's been viewed a lot YouTube  transcript to article  act as a uh expert copywriter  specializing blah blah blah so Lang  chain will help host prompt templates  that you can use for your own so it's an  easy way to just go share prompt  templates so here we have this whole  entire prompt and here we have a token  where you can go and grab it and then if  you wanted to grab this prompt yourself  then you can go and just grab it like  this the uh advantage of this is you can  share prompts a whole lot easier but  then two if you want prompts to be  updated with the latest and greatest  thinking well you can just keep on  pulling from there which is nice  so I'm going to pull uh hub.  whf proposal indexing all right so let's  go you view the proposition prompt here  uh wfh proposal indexing here's a  citation that comes with it and here's  the chat prompt template I won't go  through all this but the interesting  part is split the compound sentence into  simple sentences maintain the original  phrasing from the input whenever  possible and then they actually give one  example here so they have an input one  here and then they have an output about  what they want the language model to  Output hm interesting I just went and  copied this code I put that in right  here I'm also going to uh get my  language model going we're going to use  GPT for a preview because I want the  long context and good processing power  all right so within this object is going  to be the prompt then I'm going to  create a runnable which combines The  Prompt and the language model and this  is via the Lang chain expression  language and with that language you can  do just this pipe operator and you can  combine those right there and then the  next thing I'm going to do I tried doing  some uh extraction of these propositions  via the uh recommended way but I found  it was giving me a hard time so I just  made my own extractor from it and the  way I'm going to do that is I'm going to  just do the pantic extraction method so  I'm going to create a pantic class here  and then I'm going to do extraction  chain create extraction chain with p uh  pantic and I'm going to pass it in the  sentences pass in the language model  cool we have that and then I'm going to  create a small little function which is  this the get propositions which is going  to be hey go and get this Pro go and get  the list of propositions from the thing  that I give you because it takes a  runnable and then it's going to do the  extraction  and it's going to return the  propositions for us all right so now I'm  going to do Paul Grahams superlinear  essay cool we have that I'm going to  split the essay into paragraphs now this  is a a design Choice hey Greg aren't we  doing just chunking again not really  because this is like a very loose chunk  that doesn't really matter I could pass  it a sentence at a time I could pass it  to I could pass it paragraphs see how  many paragraphs we have we have 53  different paragraphs nice let's take a  look at one of the paragraphs just  because I like um being super explicit  here  uh let's do number two nice one of the  most important things I didn't know etc  etc we can look at number five cool  those are just paragraphs for us all  right then what we're going to do is  we're going to go get our propositions  and so we're going to do essay  propositions and so I'm just going to go  through this go through each paragraph  I'm just going to do the first five  because there's kind of kind of a lot of  data here I'm going to get the  propositions let me come back to you  when this is done one minute later  awesome so we have our propositions  let's take a look at a few here again  because we're doing this iteratively  with each other all right I'm going to  take a look at a  few you have 26 propositions cool so our  five paragraphs resulted in 26  propositions the month is October the  year is 2023 at the time past I did not  understand something about the world  cool so now we're starting to pull out  individual facts about what the um what  the paragraph is about lovely so now  what I want to do is I want to use an  agent-like system that is going to go  through each one of these iteratively  and decide hey should this be a part of  a chunk that we already have or should  it not there there's no package that I  saw that did this for us yet because  this is a quite experimental method but  what I did is I ended up making a a  gentic chunker this isn't a package yet  so you can't go uh import this anywhere  but I'll show you the code that's  powering this awesome so here's the code  on how this works now I'm not going to  go through this in detail because that's  not the uh point of this tutorial but  I'm going to go through the highle  pieces here the way that it works is  you're going to have uh AC equals a  gench chunker okay cool and then you're  going to have your list of propositions  this could be sentences but propositions  will be best cuz that's how I designed  it to work and then what you're going to  do is you're going to add your  propositions to the class and then what  it's going to do is it's going to start  to form chunks for you then we're going  to pretty print the chunks all right so  let's go through and see how this works  the real magic happens in the add  propositions so if we go up to the top  here so we're going to add proposition  now if it's your first Chunk meaning  your first proposition your first chunk  then you're not going to have any chunks  and chunks is going to be a property of  this class if chunk size equals zero  meaning you don't have any yet then  create a new chunk hm totally makes  sense well what do you want the chunk to  be about well let's go find out where  create new chunk is and on create new  chunk what we're going to do is we're  going to create a chunk ID which is just  going to be a random uh uu ID or a  subset of one then we're going to get a  chunk summary and a chunk title the  reason why we do this is because when we  add a future thing we need to know well  what are our chunks already about right  because that'll tell us whether or not  we need to add it so we're going to have  a summary which is going to have a lot  of good detailed information and we're  going to have a title right so get new  chunk summary all that this is doing is  looking at the propositions that are  currently in the chunk and then it's  generating a summary about what that  chunk is about and then the chunk title  this is just a few words that kind of  give you a quick glance on what it  actually is now there is a parameter you  can set when you do this do you want to  update your summary and title because as  I was going through this um as I added  say proposition number one it was about  one thing but then once you added  proposition number two and three and  four to the chunk you may need to update  the summary or update the chunk because  now the chunk is kind of just slightly  different right it's kind of as if you  had a uh centroid and it's moving just  slightly but you want to capture that  Essence all right and\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"you want to capture that  Essence all right and then what we do is  with each one of the chunks we have a  chunk ID we have a proposition or we  have a list of propositions a title a  summary and then the chunk index is just  um when was this chunk made its number  cool uh let's go back to where we were  um okay so that's if if you add uh your  first proposition to your empty list of  chunks and then you're going to return  here meaning you're going to stop you're  not going to go any further because well  you've already added it to a chunk it's  all good cool but let's say that wasn't  the case well let's go find a relevant  chunk this part I thought was actually  kind of cool too in this relevant chunk  you're going to have a proposition now  you want a proposition to go in and you  want a chunk to come out right and so we  have a a simple little prompt here  determine whether or not the proposition  should belong to any of the existing  chunks and then I do an example here I  have some other good stuff but what I'm  going to do here is I'm G to pass it a  uh string of what our current chunks  actually look like and those current  chunks are going to be groups of three  things the ID of the chunk the name of  the chunk and the summary of the chunk  because what I want it to do is if it  deems that yes this proposition should  be part of a chunk well then what I want  you to do is I want you to Output the  chunk uh ID for me and this is how I can  then extract which chunk it should  actually be a part of all right so we're  going to go through there we got all of  that um and let's just say that a chunk  ID uh does come out meaning it should be  added to another chunk well what I'm  going to do is I'm just going to add  that proposition to the chunk nice so  yes I found a chunk it should be a part  of cool I'm going to add this  proposition to it then when you add this  proposition to it this is where I was  talking about beforehand um if you want  to generate new metadata then it will go  and generate a new summary and title if  you don't want to do it then it doesn't  have to so let's say you didn't find a  new chunk ID meaning you uh wanted to  add a proposition it didn't find a new  chunk so you need to actually make a new  one uh no chunks are found create a new  chunk and that's the same thing that we  had up above and you go from there so  that's really the meat of the entire  thing and if we're going to go back to  our repo here let's go do this for our  essay that we had all right so I'm going  to uh do the agentic chunker I'm going  to say from AC or uh make AC and then  what I want to do is I want to add each  one of the first 20 propositions but you  know we only had 23 so let me just this  whole thing here and then what I did was  is I have a lot of print logging if you  don't want that cuz you think it's  annoying just go print logging equals  false but let's step through this so now  it's adding our first chunk which is the  month is October no chunks duh because  you don't have any so it's created a new  chunk which is  51322 it's called date and times yeah  cool makes sense that's probably where  I'd want this to go the year is 2023  chunk found date and times oh yeah duh  cuz what it did is it's saw there's a  chunk there's a date and times chunk I'm  going to add this one to it too too okay  cool I was a child at some past time no  chunks are found because there's only  one at this time and it doesn't think  it's part of that one and so created a  new chunk personal history H nice okay  um at the past time I did not understand  something important about the world it's  adding it to personal history cool makes  sense the important thing I did not  understand is that the degree in which  returns for performance are super linear  it didn't find any chunks and so it  doesn't think it's part of date and  times or personal history it made a new  one called  return performance and returns  relationship cool teachers and coaches  implicitly told us returns were linear  uh chunk found it's adding it to the  returns chunk nice teachers and coaches  meant well no chunks blah blah blah and  we go all the way through here and so  it's kind of interesting but what I want  to show you is an an instance where a  chunk name was updated so adding um you  get what you you get out what you put in  Was Heard A Thousand Times by the  speaker it's adding it to the Mis  conceptions in per performance of  returns and relationships wait where did  that title come from oh wait it's the  same chunk ID but the name has been  updated because you're actually updating  the chunk as uh the chunk TM and the  chunk title as you go along here so as  we go through here we're going all the  way through it did this a whole bunch of  times I want to see what comes out the  other end so you can pretty print the  chunks cool so it looks like we have  five chunks chunk number zero has this  chunk ID um this chunk contains  information about the specific dates and  cool and here are the propositions that  were added to that chunk nice so this is  the content of our chunk that we want to  actually pull in this chunk contains  Reflections on someone's childhood blah  blah blah okay cool oh dang oh this is a  big one so now we have a bunch of  statements that were pulled from the  essay that are all about superlinear  returns across different fields H all  right cool and let's look at this last  one or second to last one teachers and  coaches K cool consequences of inferior  product quality on business viability  and customer base nice all right so  that's kind of interesting now we're  starting to group similar items together  so if we have a question that pops up  about product quality and business  viability well here's the chunk that you  got to look at now is this perfect well  not quite yet because you could get some  complicated questions where you may want  multiple things for different chunks but  I think this is a really interesting  Direction on how we'd start to move  towards there and if we actually want to  get these chunks because we want to go  do something interesting with them maybe  proper index them then you can go and  get those and get the list of strings so  there you go that's a gentic chunking  now again it's slow and it's expensive  but if you bet that language models will  speed up and they'll get cheaper which  I'm guessing they will then this start  this type of method starts to come into  play finally what I want to do is I want  to congratulate you on finishing the  five levels of text chunking and text  splitting we are almost done but I  wanted to throw in a bonus level in  there right so this bonus level is going  to be through alternative  representations now much like before  like our chunking commandment we need to  think about how we're going to get our  data ready for our language model right  chunking is only one part of that story  it's how you're actually going to split  your texts but the next step that you're  going to take in front of that is you're  going to get embeddings of your text and  you're going to go throw those in your  knowledge base right well there's  different ways you can get embeddings of  your text and there's different  questions that'll come up like should  you get embeddings of your raw text or  should you get embeddings of an  alternative representation of your raw  text that's what we're going to talk  about this bonus level let's go through  this very briefly  the first thing that we're going to talk  about is multiv Vector indexing this is  when you do a semantic search but  instead of doing semantic search over  the embeddings of your raw text you get  it off of something else like a summary  of your raw text or hypothetical  questions of your raw text all right  let's do this first one which is  summaries and how we'd actually do that  so we're going to get our super linear  essay one more time I'm going to blow  through this part since we already  talked about it but what I'm going to do  is I'm going to get my six chunks from  our document and I'm going to say  summarize the following  documents and and that's going to be a  chain that we have there and then we're  going to do this but we're going to do  this in batch which is one of the other  nice things about Lang chain uh  expression language and now we have all  of our summaries and here's the first  one so this is a summary of our first  chunk that we have right now what I want  to do is I want to get an embedding of  this summary instead of the embedding of  the chunk like I I would have with the  old method and the way that we're going  to do this is we're going to get our  Vector store ready we're going to use  chroma today and we're going to get just  a doc store ready which is just going to  be the inmemory bite store that link has  and then we're going to pass in a multiv  vector retriever this is a cool Lang  chain abstraction where you can um  basically go semantic search off one  thing but then return another thing for  your final language model all right  we're going to go through this there's a  whole tutorial that they have on this in  fact I have a whole tutorial on this as  well at fullstack retrieval.com so I'm  going quick through it but that's CU I  want to show you a whole bunch here  we're going to get our summary docs  we're going to add those and we're going  to uh change those into proper Lang  chain documents instead of the actual  strings that they were and then here's  where the cool part happens we're going  to add them to the vector store which is  also going to get the embeddings for us  we're going to add the summary docs and  then we're going to add our normal docks  to our normal Dock Store and our  retriever nice um cool and then what we  could do here is we could go to our  Retriever and we could go get the  relevant documents and so now what I'm  going to do is with my\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"and so now what I'm  going to do is with my query this is  going to go match on the summary  embedding rather than the raw document  embedding but when these docs are passed  back to us these are the raw documents  so there's a little happening behind the  hood here um but again I encourage you  to go to fullstack retrieval.com and go  check out theor tutorial on this all  right there's one method for us the  other method you could do is well  instead of summaries I want hypothetical  questions this one's really nice if you  anticipate a Q&A bot that you're making  because then you can start to anticipate  well which questions are people going to  ask the language model will generate  questions for you and there's a your  hypothesis is that there's a high higher  likelihood that um these questions will  end up matching for you and you'll get  better uh document matching cool another  one you can do is you can do a parent  document retriever so now this case the  hypothesis with this method is that if  you subset your document even more  you'll get a better semantic search  however in order to answer the question  of whatever hypothetical question you  could have you actually want what's  around that small document so yes that  small document will will do good  semantic search but you actually want  the buffer around it so what comes  before and after it and another way to  say that is you want the parent document  that that small chunk actually comes  from and or in order to show you this  one I'm going to do a um I'm going to do  a tutorial from llama index now I also  have another tutorial on this on full  stacker tre.com if you want to go check  that out so with L uh with llama index  we're going to do their hierarchical  node parser and what you're going to do  is you're going to give a list of Chunk  sizes so they're going to get chunks  that are two uh 20 48 512 and 128 here  and let's go do our Paul Graham essay  let's see how many nodes we actually  have this is 119 nodes that's cuz 128 is  pretty small for us here and if we take  a look at one of these relationships  here we and this is one of the smaller  ones because this is something at the  end and that's the 128 you can see that  these chunks are quite small but and we  are just looking at the relationships  here you can get a source a previous a  next and then the important part here is  the parent so if you were to do this via  the Llama index way yes you matched on  the  128 chunk because that has good semantic  search but you actually got the 512 or  the 2048 right I won't show you how to  go all the way through that because  again that's a whole separate tutorial  then the last thing that I'll show here  is around the graph structure so  sometimes you're going to go over your  raw text and instead of chunking your  raw text you actually want to extract a  graph structure from that text because  there's a lot of entities within your uh  within your text I'm going to do that  via the diffbot and this is the diffbot  Transformer we're going to go I'm going  to say say Greg lives in New York Greg  is friends with Bobby San Francisco is a  great City but New York is Amazing Greg  lives in New York and let's see what  actually pops out of over  here so now we have this so now we have  actually have a graph document and one  of our nodes is node ID Greg type person  properties K Greg we have another Bobby  node which is cool we have another node  here which is an entity location it's  entity New York we have another uh  relationship node so this kind of be  like an edge and this is a social  relationship between Greg and Bobby so I  won't go through all these but either  way you can start to see how now you can  use a graph structure to answer  questions about uh a person or your  specific text but that's a little  outside the chunking side either way  that is actually the rest of the  tutorial and I want to congratulate you  on what is quite a long video I'm not  sure how long this is probably one of my  longer ones that we have yet either way  I'm excited that you're here my name is  Greg camrad and I am on a mission to  figure out how Ai and business business  are going to overlap the vend diagrams  with each other this has been a lot of  fun thank you for joining me we will see  you later\", metadata={'source': '8OJC21T2SL4', 'title': 'The 5 Levels Of Text Splitting For Retrieval', 'description': 'Unknown', 'view_count': 10834, 'thumbnail_url': 'https://i.ytimg.com/vi/8OJC21T2SL4/hq720.jpg', 'publish_date': '2024-01-08 00:00:00', 'length': 4139, 'author': 'Greg Kamradt (Data Indy)'}),\n",
       " Document(page_content=\"all right so today we are going to  implement a new template um and we're  going to implement it for this new paper  that just came out um skeleton of  thought large language models can do  parallel decoding so I saw this on  Twitter I forgot where I saw it but I  thought it was really cool um and it  plays to some strengths of Lang chain  and of Lang chain expression language  namely it's going to use multiple llm  calls it's going to break things down  into small chunks and then a bunch of  those LM calls are going to be conducted  in parallel um and so with Lang chain  expression language we can do that  fairly easily so we're going to create a  template um for those and for Miner what  templates are those are are really easy  way to get started with any application  um and so we have a bunch of predefined  templates here um but we're going to  create a new one um and so we should  have instructions on how to add um yeah  all right  so let's create a new template I'm if  you look here I'm in my workplace Lang  chain templates directory um L chain  template new um skeleton of thought and  something there we  go all right uh L all right  um so I now have this uh uh example  template that was just set  up I can go inside it um let's find it  where are we skeleton of thought um  let's get add everything in here get  check out the Harrison skeleton  of thought all right just checking out a  clean Branch um we've added a bunch of  stuff in here we can see that we have a  really simple read me um so I need to  add a description here I'll do that  later environment variables I'll do that  later um and then a bunch of predefined  uh documentation  there got some p project. tunels with  some dependencies we'll use open AI for  this maybe we'll go into how to do this  with a different language model should  be pretty easy and then the main part  here is just chain we have a really  simple dummy chain here but of course  what we're going to do is we're going to  implement this new paper skeleton of  thought so what is this  paper I'll post a link to this in the  YouTube description as well but  basically the idea is it's a really cool  idea if we take a look at  here if you get a uh question like what  are the most effective strategies for  conflict resolution in the workplace a  normal llm response would just be  generating this answer one by one what  skeleton of thought does is it generates  a skeleton really short bullet points  and then it expands each bullet point  and the reason reason I like this is it  um it speeds things up so it's really  really fast which is good um and then  also this this kind of speaks a little  bit to kind of using the language model  to plan and then execute on things so  here each execution is just expanding  with another language model call but you  could easily imagine that each execution  could be another you know it could it  could it could write a whole paragraph  it could write a whole research report  for each bullet point um it could  actually take actions and do them um  importantly like here the the reason  that this is helpful and that you can do  this as opposed to like a react style  agent or something is that all of these  are kind of independent so you can do  one without doing two and you can do two  without doing three or two without doing  one um and that makes it really easy to  paralyze them which leads to the speed  up so if you're doing things where it  relies on the results of previous steps  that may not be as good of fit for  this so let's see see okay so this is  prompt one this is prompt two that's  great it looks like they're already  in  Okay Okay so we've got one and this is  being used to generate the  skeleton so  let's copy  this  um skeleton  generator template  there's some weird like new line stuff  here  um I'll just do  that  now that seems fine and then  um  skeleton generator  prompt and then we're just going to do  from template from this  template um  let's  um we're going to import this because  now we're going to create our chain our  Chain's going to be really simple the  first chain so we're going to create our  first chain which is generating the  skeleton so it's going to be this prompt  and we're going to pass this in  toi and then we're just going to parse  out the string we're probably going to  actually need to do other things we're  going to need to parse out the bullet  points because we're going to want to  work with those but for now we can just  do this um and then let's test it out  um let's see do they have a good example  in the  all right so this is the example they  given the paper we'll use  that  um some weird stuff happens when copying  things  over okay let's try that out um let's  print out this just to make sure we're  also logging things to  lsmith um so uh what lsmith is is the  debugging logging um uh tool that we've  built on the side um and so what I've  done before is I've basically copy  pasted I'm using my Dev account but um  we we you should use the the regular  link Smith account it's smithl chain.com  um if you you can sign up for Access  there it is on a wait list if if you  don't have access shoot me a DM on on  Twitter or LinkedIn um and we can give  you access pretty  easily um once you do that I've just  exported these three variables you want  to pop in your API key there um I've  already done that but you you should do  that and then what that does is it will  Trace everything that we do so here  let's do python skeleton of thought  chain let's run this and so it should  print it out here okay so this uh uh  spits out the  um skeleton 10 bullet points okay that's  a lot we can also see that it should  show up um here um so here we have this  runable sequence we can see it's really  simple it's just a prompt template I'll  out preparer what I really like about um  Ling Smith is it will let you open  things up in a playground so here if we  wanted to play around with the prompt  for whatever reason um like let's say we  wanted to try to change this to just  have I don't know two to three points  then we do that and we can rerun it and  here we can see that just generates two  points so this is really useful for  debugging things especially when they're  in Chains It's also useful for seeing  what's going on under the hood which  right now doesn't really matter because  it is quite simple it's just this but  when it comes time to add in the next  point the point expanding stage it will  come in quite  handy so let's take in this point  expanding  stage and let's do Point  expander template  um all right we're going to add this  in  um  okay so now this takes in a few  different  variables Let's uh let's make this point  expander  prompt chat promp template from template  put expander template so this this takes  in question skeleton which is what we  just generated um Point index and then  point skeleton  okay so what we're going to want to do  is we're going to want to take let's see  if there's any good diagrams for  here I guess the main diagram wow okay  there's a big appendix for this what's  in this  appendex oh okay so they run it on a a  lot of  different they have some yeah okay so  they have a bunch of different prompts  for done different models which is  really cool um we'll probably just do a  basic one uh one shot  prompt  um oh that's cool so they do  some that's cool so they do they have  some like routing between it looks like  just a regular response and one that  needs a skeleton so they're determining  whether this is even needed um anyways  long appendex might read that in more  detail later um but the main idea is  that what we're going to want to do is  we're going to want to take each of  these bullet points and then expand it  so in our prompt template we have  question skeleton Point index Point  skeleton so it looks like  this is the question this is the  skeleton the point skeleton would be  here Point index would be one point  skeleton would be there I think they  actually have  code for there yeah there they go or  there we  go they do have some code let's see if  we can find  where they have prompts in there oh it's  probably in here skeleton of thought  um what I'm looking for is a parser  that'll turn this into a  list but we can probably do  that okay this isn't really that  helpful this isn't really that  helpful  at all okay so what we're going to want  to do is we're going to want to write  a some snippet that is going to  construct a list of things that we're  going to  expand so it's going to create a list of  things from this output we're ALS we're  going to want to have each element in  that list should be a dictionary with  four things the question the skeleton  the skeleton point and then the uh the  skeleton  index so what that's going to look like  is first from Lang chain.  schema do runnable import runnable pass  through all right so we're going to  create let's  um let's rename this just skeleton  generator chain  um okay so we're going to create our  final chain or a final chain as run pass  through. assign um we already have  question that comes in so now what we  really want is skeleton and we're going  to pass skeleton as just skeleton  generator chain so this is going to add  a new variable called skeleton and it's  going to be the result of calling this  great  um  then what we're going to want to do is  we're going to want to turn this into so  this will be a single dictionary with  question and skeleton we're going to  want to turn this into a list of things  and then we're going to have  our Point  Xander actually yeah let's try  X expander  chain Point expander prompt chat  chat opening  ey string out parser cool let's try that  um Let's ignore this for a moment let's  try let's so let's try out this so let's  just pretend that would got to the point  where we had the question we had this as  skeleton um oh that's not  that skeleton let's\", metadata={'source': 'wLRHwKuKvOE', 'title': 'Skeleton-of-Thought: Building a New Template from Scratch', 'description': 'Unknown', 'view_count': 5332, 'thumbnail_url': 'https://i.ytimg.com/vi/wLRHwKuKvOE/hq720.jpg', 'publish_date': '2023-11-27 00:00:00', 'length': 1990, 'author': 'LangChain'}),\n",
       " Document(page_content=\"skeleton um oh that's not  that skeleton let's pretend we had this  as skeleton  um  Point index as  one  point  skeleton let's do  this  cool  weird all  right so for whatever reason the  variables had some spaces in them I  don't like  that um did it not  save  um  ah there we  go okay so it expanded the first  one that doesn't actually expand it that  much  whatever let's um we we fix that with  some prompt engineering later on if we  really want to so we've got this point  expander thing working well now what we  really want  is we want to write a python  function  that  parses  a  single  number list into a list of dictionaries  with  each element in the  list um add two  keys a index key for the index in the  number list and a point py for the  content let inlo they look something  like  this and so here we had prompted it with  one so I'm going to add that in there  and I I'll show how we can do something  to just get that so so when it generated  it it didn't have the one in front of it  but I'm adding it um to start oh  interesting it looks like it's using  Code interpreter  okay  cool  um we will wait for this to  finish in the meantime we can see what  was going on with the point expander  solution um so this was the point  expander um we can see that this is what  the um prompt ended up looking like by  the time it got into the uh  to  the uh to the to the llm so this is the  fully formatted thing um we're actually  missing a one zero one point something  there I think so okay cool that that's  interesting to know um we'll have to so  basically this doesn't generate the one  at the start because if you notice we  have it here so what I'm actually going  to do is I'm going to add something here  that's just Lambda X  um one so that's just adding it to the  response that'll make it just a little  bit more convenient to pass  around go back to open  AI that looks reasonable to  me interesting that this chat GPT is  getting updated every day it seems all  right let's copy  this  okay  so um this par is the list so what I  also want to do is  um  create  list  elements so so I want something that  takes in the output of this which is a  dictionary that has question and  skeleton and then creates things that  can be passed into the point expander  which is question skeleton Point index  Point skeleton  so I'm going to pretend that's input I'm  going to um do skeleton equals  input  skeleton then I'm going to do number  list equals par number list  skeleton um I'm going to  change  these  to match those  things and then for  L list  L  skeleton  skeleton why is that getting highlighted  I have  name  r vers  skeleton weird um  L  question  question input  question  return numbered list cool so now I can  add this in here and I can do create  list  elements  cool um then I can do ex Point expanded  chain should be expander let's fix  that um  expander  expander  expander expander thing  um math then this is going to get back a  list of expanded  things and what I want to do is  basically uh pass those in uh into or  yeah I want I want to combine those into  a final answer um so let's just write  something like def  Final  Answer  expanded  points Final  Answer equals that or actually um yeah  we don't even need this we can just do  Lambda X  um join X that should return just a  string because it's getting a list of  strings  back let's try some  stuff now delete everything except  question  um yeah let's run it and see what  happens  oh okay that was pretty fast so that was  faster than I expected so it generates  all of this  um generates all of this uh by uh well  let's see what's going on under the  hood this is how it generates all of  this and if we expand this okay there we  go a lot more LM calls so all these  yellow blocks here are LM calls so it  actually made a lot of LM calls under  the hood it was just done very fast if  you look at them in sequence this is the  first one and it generates the skeleton  and then this is uh mapping over each of  them this is expanding the first one um  and uh basically so this is expanding  the first one of identifying the root  cause this is expanding the second one  of encouraging open communication blah  blah blah we get to The Final Answer um  where and then this is the final answer  which it takes in all of these and gets  back this output if we wanted to change  this output in some way um let's uh uh  let's maybe let let's let's show how we  can do that if we wanted to like format  this a little bit nicer right now this  is just joining them with that um as you  can maybe guess we could just write a  simple function like def get Final  Answer um which we started to do before  but I wanted to make sure it was working  turns out it was working it's quite  simple actually um final answer string  use that um actually let's here's a  comprehensive  answer um for L and expanded list um and  and you know what let's add like that  and  enumerate expanded list  um equals  I  do  L turn Final Answer  string drop that in  there run it again so now we should get  a slightly nicer formatted um final  answer so yeah here and it's zero index  you know we can we can fix that by going  like this but the point is we get a  nicely formatted um thing here's a final  answer we go back here we can see a new  thing popped up input is this question  output is this here's a comprehensive  answer um okay so it looks like um okay  so this is actually helpful  so some of them so it looks like it  largely okay so this is actually pretty  cool so what we can see is going on is  the first one it just starts with is and  that's because we have this  prompt where we ask it to basically  continue from the um first one so what  we actually want to do is we actually  want  to add a new element into this  thing which is going to  be um we can call this  expanded answers  um and then we can pass it here now this  is uh not going to get in the expand so  so what we're so basically what's  happening  is our expanded answers are continuing  from here so what we actually want to do  is basically take um the uh take this  thing and  then take this thing and then add in um  the add in this part to  there so what that's going to look like  now is we're going to have something  that has question skeleton and then  expanded  answers um  and  um or  actually what we can do actually I know  what we're going to do we're going to  reverse  this  that's fine what we're going to actually  do is we're going to change the point  expander chain  um and we're going to do a  runnable runnable pass through. assign  here we're going to have  continuation there and then we're going  to do Lambda  x x X  um  uh it's what are we calling it we're  calling it The Point  skeleton  plus the  continuation okay so let's run this see  if this works and then I'll explain what  it does if it does  work okay so now we can see that each  bullet point is more of a uh fully  featured or of an actual sentence we've  still got some weird things going on  there likely so let's let we let's debug  this a little bit more  um so here we have this map now each of  these is now basically it's generating  this answer  um it's generating this answer and then  it's appending it to this um and so the  weird thing that's happening is that  there isn't a space but there should be  a space the reason there isn't a space  is  because why isn't there space well let's  see oh I mean there's just there's just  no space there um okay so what we can  just do is in case there is a space  we'll strip then we'll  in space um and then there so if we run  this  now okay that looks good there's proper  spacing um they're all uh full sentences  and it looks like that should  work so um and if we look  at what's going on under the hood we  have the full thing here um we can  expand it out we can see all the calls  that are getting called we can jump into  any place um debug it hop into a  playground try it out with different  language models as well one of the cool  things that we did um where there we go  one of the cool things that we did is we  actually worked with fireworks and  Google to have some free models here  let's see how  llama L of 2 13B  does  um h  all right this  uh all right so  llama llama 13B is not amazing at this  where is what about mistr Moll is a  pretty good  model all right I'm probably not using  us the right uh tokens to uh prompt myal  correctly so let's stop doing that um  but basically yeah this is this we've  we've added skeleton of thought as a  template to laying chain um oh one thing  that we can do  now is we can actually see this in  action um so if  we let's just delete this let's check P  project. toml skeleton thought chain  yep cool that's all right um what we can  do is I think we can just do L chain  serve from  here could not import model module  defined in P project. toml so I think we  need do p install  D in here to install this  module now if we do L chain  serve okay so I'm on  yeah so I'm on pantic two um we really  should make this pantic one by default  um but now if I go to this SL  playground okay so what I need to do is  it can't because I'm using this thing it  can't automatically infer what the  um inputs are correctly um so I'm going  to  from base  model  class input base  model  with types input  type chain  input  um I think  that's do I I don't know if I have to  type the output or not um cool okay so  we get this  um what the most effective strategy  contact for what most  strategies for conflict resolution in  the  workpl  um yeah so we can see the intermediate  steps jump up a lot we get back our  answer um this actually also  automatically logs things to link Smith  so this is uh this is the one that we  just ran very  recently um but now we have a playground  to play around with this as well so if  we wanted to share this with uh anyone  we can uh spin up a little playground  this is just\", metadata={'source': 'wLRHwKuKvOE', 'title': 'Skeleton-of-Thought: Building a New Template from Scratch', 'description': 'Unknown', 'view_count': 5332, 'thumbnail_url': 'https://i.ytimg.com/vi/wLRHwKuKvOE/hq720.jpg', 'publish_date': '2023-11-27 00:00:00', 'length': 1990, 'author': 'LangChain'}),\n",
       " Document(page_content=\"can uh spin up a little playground  this is just served by fast API and do  it this  way okay I think that's really all I  have  now thank  you\", metadata={'source': 'wLRHwKuKvOE', 'title': 'Skeleton-of-Thought: Building a New Template from Scratch', 'description': 'Unknown', 'view_count': 5332, 'thumbnail_url': 'https://i.ytimg.com/vi/wLRHwKuKvOE/hq720.jpg', 'publish_date': '2023-11-27 00:00:00', 'length': 1990, 'author': 'LangChain'}),\n",
       " Document(page_content='RAG |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookRAGOn this pageRAGLet s look at adding in a retrieval step to a prompt and LLM, which adds up to a  retrieval-augmented generation  chain%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())chain.invoke(\"where did harrison work?\")\\'Harrison worked at Kensho.\\'template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a Kensho.\\'Conversational Retrieval Chain We can easily add in conversation history. This primarily means adding in chat_message_historyfrom langchain.schema import format_documentfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_stringfrom langchain_core.runnables import RunnableParallelfrom langchain.prompts.prompt import PromptTemplate_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.Chat History:{chat_history}Follow Up Input: {question}Standalone question:\"\"\"CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"ANSWER_PROMPT = ChatPromptTemplate.from_template(template)DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")def _combine_documents(    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\\\n\\\\n\"):    doc_strings = [format_document(doc, document_prompt) for doc in docs]    return document_separator.join(doc_strings)_inputs = RunnableParallel(    standalone_question=RunnablePassthrough.assign(        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])    )    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),)_context = {    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,    \"question\": lambda x: x[\"standalone_question\"],}conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()conversational_qa_chain.invoke(    {        \"question\": \"where did harrison work?\",        \"chat_history\": [],    })AIMessage(content=\\'Harrison was employed at Kensho.\\')conversational_qa_chain.invoke(    {        \"question\": \"where did he work?\",        \"chat_history\": [            HumanMessage(content=\"Who wrote this notebook?\"),            AIMessage(content=\"Harrison\"),        ],    })AIMessage(content=\\'Harrison worked at Kensho.\\')With Memory and returning source documents This shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.from operator import itemgetterfrom langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(    return_messages=True, output_key=\"answer\", input_key=\"question\")# First we add a step to load memory# This adds a \"memory\" key to the input objectloaded_memory = RunnablePassthrough.assign(    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),)# Now we calculate the standalone questionstandalone_question = {    \"standalone_question\": {        \"question\": lambda x: x[\"question\"],        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),    }    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),}# Now we retrieve the documentsretrieved_documents = {    \"docs\": itemgetter(\"standalone_question\") | retriever,    \"question\": lambda x: x[\"standalone_question\"],}# Now we construct the inputs for the final promptfinal_inputs = {    \"context\": lambda x: _combine_documents(x[\"docs\"]),    \"question\": itemgetter(\"question\"),}# And finally, we do the part that returns the answersanswer = {    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),    \"docs\": itemgetter(\"docs\"),}# And now we put it all together!final_chain = loaded_memory | standalone_question | retrieved_documents | answerinputs = {\"question\": \"where did harrison work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison was employed at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}# Note that the memory does not save automatically# This will be improved in the future# For now you need to save it yourselfmemory.save_context(inputs, {\"answer\": result[\"answer\"].content})memory.load_memory_variables({}){\\'history\\': [HumanMessage(content=\\'where did harrison work?\\'),  AIMessage(content=\\'Harrison was employed at Kensho.\\')]}inputs = {\"question\": \"but where did he really work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison actually worked at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}PreviousPrompt + LLMNextMultiple chainsConversational Retrieval ChainWith Memory and returning source documentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en'}),\n",
       " Document(page_content='Chroma |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreComponentsLLMsChat modelsDocument loadersDocument transformersText embedding modelsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAstra DBAtlasAwaDBAzure Cosmos DBAzure AI SearchBagelDBBaidu Cloud ElasticSearch VectorSearchBigQuery Vector SearchChromaClarifaiClickHouseDashVectorDatabricks Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissFaiss (Async)Google Vertex AI Vector SearchSAP HANA Cloud Vector EngineHippoHologresJaguar Vector DatabaseKDB.AILanceDBLanternLLMRailsMarqoMeilisearchMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVecto.rsPGVectorPineconeQdrantRedisRocksetScaNNSemaDBSingleStoreDBscikit-learnSQLite-VSSStarRocksSupabase (Postgres)SurrealDBTairTencent Cloud VectorDBTigrisTileDBTimescale Vector (Postgres)TypesenseUSearchValdVearchVectaraVespaviking DBWeaviateXataYellowbrickZepZillizRetrieversToolsAgents and toolkitsMemoryCallbacksChat loadersAdaptersStoresComponentsVector storesChromaOn this pageChromaChroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.Install Chroma with:pip install chromadbChroma runs in various modes. See below for examples of each integrated with LangChain. - in-memory - in a python script or jupyter notebook - in-memory with persistance - in a script or notebook and save/load to disk - in a docker container - as a server running your local machine or in the cloudLike any other database, you can: - .add - .get - .update - .upsert - .delete - .peek - and .query runs the similarity search.View full docs at docs. To access these methods directly, you can do ._collection.method()Basic Example In this basic example, we take the most recent State of the Union Address, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.# importfrom langchain.text_splitter import CharacterTextSplitterfrom langchain_community.document_loaders import TextLoaderfrom langchain_community.embeddings.sentence_transformer import (    SentenceTransformerEmbeddings,)from langchain_community.vectorstores import Chroma# load the document and split it into chunksloader = TextLoader(\"../../modules/state_of_the_union.txt\")documents = loader.load()# split it into chunkstext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)# create the open-source embedding functionembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")# load it into Chromadb = Chroma.from_documents(docs, embedding_function)# query itquery = \"What did the president say about Ketanji Brown Jackson\"docs = db.similarity_search(query)# print resultsprint(docs[0].page_content)/Users/jeff/.pyenv/versions/3.10.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html  from .autonotebook import tqdm as notebook_tqdmTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Basic Example (including saving to disk) Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.Caution: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other s work. As a best practice, only have one client per path running at any given time.# save to diskdb2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")docs = db2.similarity_search(query)# load from diskdb3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)docs = db3.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Passing a Chroma Client into Langchain You can also create a Chroma Client and pass it to LangChain. This is particularly useful if you want easier access to the underlying database.You can also specify the collection name that you want LangChain to use.import chromadbpersistent_client = chromadb.PersistentClient()collection = persistent_client.get_or_create_collection(\"collection_name\")collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])langchain_chroma = Chroma(    client=persistent_client,    collection_name=\"collection_name\",    embedding_function=embedding_function,)print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Add of existing embedding ID: 2Add of existing embedding ID: 3Add of existing embedding ID: 1Insert of existing embedding ID: 1Add of existing embedding ID: 2Insert of existing embedding ID: 2Add of existing embedding ID: 3Insert of existing embedding ID: 3There are 3 in the collectionBasic Example (using the Docker Container) You can also run the Chroma Server in a Docker container separately, create a Client to connect to it, and then pass that to LangChain.Chroma has the ability to handle multiple Collections of documents, but the LangChain interface expects one, so we need to specify the collection name. The default collection name used by LangChain is  langchain .Here is how to clone, build, and run the Docker Image:git clone git@github.com:chroma-core/chroma.gitEdit the docker-compose.yml file and add ALLOW_RESET=TRUE under environment    ...    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config log_config.yml    environment:      - IS_PERSISTENT=TRUE      - ALLOW_RESET=TRUE    ports:      - 8000:8000    ...Then run docker-compose up -d --build# create the chroma clientimport uuidimport chromadbfrom chromadb.config import Settingsclient = chromadb.HttpClient(settings=Settings(allow_reset=True))client.reset()  # resets the databasecollection = client.create_collection(\"my_collection\")for doc in docs:    collection.add(        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content    )# tell LangChain to use our client and collection namedb4 = Chroma(    client=client,    collection_name=\"my_collection\",    embedding_function=embedding_function,)query = \"What did the president say about Ketanji Brown Jackson\"docs = db4.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Update and Delete While building toward a real application, you want to go beyond adding data, and also update and delete data.Chroma has users provide ids to simplify the bookkeeping here. ids can be the name of the file, or a combined has like filename_paragraphNumber, etc.Chroma supports all these operations - though some of them are still being integrated all the way through the LangChain interface. Additional workflow improvements will be added soon.Here is a basic example showing how to do various operations:# create simple idsids = [str(i) for i in range(1, len(docs) + 1)]# add dataexample_db = Chroma.from_documents(docs, embedding_function, ids=ids)docs = example_db.similarity_search(query)print(docs[0].metadata)# update the metadata for a documentdocs[0].metadata = {    \"source\": \"../../modules/state_of_the_union.txt\",    \"new_value\": \"hello world\",}example_db.update_document(ids[0],', metadata={'source': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'title': 'Chroma | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chroma is a AI-native', 'language': 'en'}),\n",
       " Document(page_content='\"hello world\",}example_db.update_document(ids[0], docs[0])print(example_db._collection.get(ids=[ids[0]]))# delete the last documentprint(\"count before\", example_db._collection.count())example_db._collection.delete(ids=[ids[-1]])print(\"count after\", example_db._collection.count()){\\'source\\': \\'../../../state_of_the_union.txt\\'}{\\'ids\\': [\\'1\\'], \\'embeddings\\': None, \\'metadatas\\': [{\\'new_value\\': \\'hello world\\', \\'source\\': \\'../../../state_of_the_union.txt\\'}], \\'documents\\': [\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\']}count before 46count after 45Use OpenAI Embeddings Many people like to use OpenAIEmbeddings, here is how to set that up.# get a token: https://platform.openai.com/account/api-keysfrom getpass import getpassfrom langchain_openai import OpenAIEmbeddingsOPENAI_API_KEY = getpass()import osos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEYembeddings = OpenAIEmbeddings()new_client = chromadb.EphemeralClient()openai_lc_client = Chroma.from_documents(    docs, embeddings, client=new_client, collection_name=\"openai_collection\")query = \"What did the president say about Ketanji Brown Jackson\"docs = openai_lc_client.similarity_search(query)print(docs[0].page_content)Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.Other Information Similarity search with score The returned distance score is cosine distance. Therefore, a lower score is better.docs = db.similarity_search_with_score(query)docs[0](Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'}), 1.1972057819366455)Retriever options This section goes over different options for how to use Chroma as a retriever.MMR In addition to using similarity search in the retriever object, you can also use mmr.retriever = db.as_retriever(search_type=\"mmr\")retriever.get_relevant_documents(query)[0]Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation s top legal minds, who will continue Justice Breyer s legacy of excellence.\\', metadata={\\'source\\': \\'../../../state_of_the_union.txt\\'})Filtering on metadata It can be helpful to narrow down the collection before working with it.For example, collections can be filtered on metadata using the get method.# filter collection for updated sourceexample_db.get(where={\"source\": \"some_other_source\"}){\\'ids\\': [], \\'embeddings\\': None, \\'metadatas\\': [], \\'documents\\': []}PreviousBigQuery Vector SearchNextClarifaiBasic ExampleBasic Example (including saving to disk)Passing a Chroma Client into LangchainBasic Example (using the Docker Container)Update and DeleteUse OpenAI EmbeddingsOther InformationSimilarity search with scoreRetriever optionsFiltering on metadataCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/integrations/vectorstores/chroma', 'title': 'Chroma | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chroma is a AI-native', 'language': 'en'}),\n",
       " Document(page_content=\"Summarization |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchUse casesQ&A with RAGSQLTool useInteracting with APIsChatbotsExtractionSummarizationTaggingWeb scrapingCode understandingSynthetic data generationGraph queryingUse casesSummarizationOn this pageSummarizationOpen In ColabUse case Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.LLMs are a great tool for this given their proficiency in understanding and synthesizing text.In this walkthrough we ll go over how to perform document summarization using LLMs.Overview A central question for building a summarizer is how to pass your documents into the LLM s context window. Two common approaches for this are:Stuff: Simply  stuff  all your documents into a single prompt. This is the simplest approach (see here for more on the StuffDocumentsChains, which is used for this method).Map-reduce: Summarize each document on it s own in a  map  step and then  reduce  the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).Quickstart To give you a sneak preview, either pipeline can be wrapped in a single object: load_summarize_chain.Suppose we want to summarize a blog post. We can create this in a few lines of code.First set environment variables and install packages:%pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8)Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0)Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4)Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299)Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0)Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1)Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5)Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3)Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12)Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2)Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1)Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2)Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4)Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1)Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1)Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0)Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1)Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3)Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9)Collecting tqdm (from openai)  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)       57.6/57.6 kB 2.7 MB/s eta 0:00:00Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0)Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0)Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1)Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20)Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1)Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3)Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9)Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33)Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42)Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5)Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3)Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0)Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4)Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0)Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.3)Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16)Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)Requirement already satisfied: h11>=0.8 in\", metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content='(8.1.7)Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2)Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)Installing collected packages: tqdm  Attempting uninstall: tqdm    Found existing installation: tqdm 4.64.1    Uninstalling tqdm-4.64.1:      Successfully uninstalled tqdm-4.64.1ERROR: pip\\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible.Successfully installed tqdm-4.66.1We can use chain_type=\"stuff\", especially if using larger context window models such as:16k token OpenAI gpt-3.5-turbo-1106100k token Anthropic Claude-2We can also supply chain_type=\"map_reduce\" or chain_type=\"refine\" (read more here).from langchain.chains.summarize import load_summarize_chainfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import ChatOpenAIloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")chain = load_summarize_chain(llm, chain_type=\"stuff\")chain.run(docs)\\'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.\\'Option 1. Stuff When we use load_summarize_chain with chain_type=\"stuff\", we will use the StuffDocumentsChain.The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM:from langchain.chains.combine_documents.stuff import StuffDocumentsChainfrom langchain.chains.llm import LLMChainfrom langchain.prompts import PromptTemplate# Define promptprompt_template = \"\"\"Write a concise summary of the following:\"{text}\"CONCISE SUMMARY:\"\"\"prompt = PromptTemplate.from_template(prompt_template)# Define LLM chainllm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")llm_chain = LLMChain(llm=llm, prompt=prompt)# Define StuffDocumentsChainstuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")docs = loader.load()print(stuff_chain.run(docs))The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems.Great! We can see that we reproduce the earlier result using the load_summarize_chain.Go deeper You can easily customize the prompt.You can easily try different LLMs, (e.g., Claude) via the llm parameter.Option 2. Map-Reduce Let s unpack the map reduce approach. For this, we ll first map each document to an individual summary using an LLMChain. Then we ll use a ReduceDocumentsChain to combine those summaries into a single global summary.First, we specify the LLMChain to use for mapping each document to an individual summary:from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChainfrom langchain.text_splitter import CharacterTextSplitterllm = ChatOpenAI(temperature=0)# Mapmap_template = \"\"\"The following is a set of documents{docs}Based on this list of docs, please identify the main themes Helpful Answer:\"\"\"map_prompt = PromptTemplate.from_template(map_template)map_chain = LLMChain(llm=llm, prompt=map_prompt)We can also use the Prompt Hub to store and fetch prompts.This will work with your LangSmith API key.For example, see the map prompt here.from langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")map_chain = LLMChain(llm=llm, prompt=map_prompt)The ReduceDocumentsChain handles taking the document mapping results and reducing them into a single output. It wraps a generic CombineDocumentsChain (like StuffDocumentsChain) but adds the ability to collapse documents before passing it to the CombineDocumentsChain if their cumulative size exceeds token_max. In this example, we can actually re-use our chain for combining our docs to also collapse our docs.So if the cumulative number of tokens in our mapped documents exceeds 4000 tokens, then we ll recursively pass in the documents in batches of \\\\< 4000 tokens to our StuffDocumentsChain to create batched summaries. And once those batched summaries are cumulatively less than 4000 tokens, we ll pass them all one last time to the StuffDocumentsChain to create the final summary.# Reducereduce_template = \"\"\"The following is set of summaries:{docs}Take these and distill it into a final, consolidated summary of the main themes. Helpful Answer:\"\"\"reduce_prompt = PromptTemplate.from_template(reduce_template)# Note we can also get this from the prompt hub, as noted abovereduce_prompt = hub.pull(\"rlm/map-prompt\")reduce_promptChatPromptTemplate(input_variables=[\\'docs\\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'docs\\'], template=\\'The following is a set of documents:\\\\n{docs}\\\\nBased on this list of docs, please identify the main themes \\\\nHelpful Answer:\\'))])# Run chainreduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)# Takes a list of documents, combines them into a single string, and passes this to an LLMChaincombine_documents_chain = StuffDocumentsChain(    llm_chain=reduce_chain, document_variable_name=\"docs\")# Combines and iteratively reduces the mapped documentsreduce_documents_chain = ReduceDocumentsChain(    # This is final chain that is called.    combine_documents_chain=combine_documents_chain,    # If documents exceed context for `StuffDocumentsChain`    collapse_documents_chain=combine_documents_chain,    # The maximum number of tokens to group documents into.    token_max=4000,)Combining our map and reduce chains into one:# Combining documents by mapping a chain over them, then combining resultsmap_reduce_chain = MapReduceDocumentsChain(    # Map chain    llm_chain=map_chain,    # Reduce chain    reduce_documents_chain=reduce_documents_chain,    # The variable name in the llm_chain to put the documents in    document_variable_name=\"docs\",    # Return the results of the map steps in the output    return_intermediate_steps=False,)text_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)Created a chunk of size 1003, which is longer than the specified 1000print(map_reduce_chain.run(split_docs))Based on the list of documents provided, the main themes can be identified as follows:1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver.2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent\\'s capabilities.3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results.4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods.5. Tool use: The documents highlight the agent\\'s ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information.6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of', metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content='of the capabilities and potential applications of such agents.7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents.8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources.Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges.Go deeper CustomizationAs shown above, you can customize the LLMs and prompts for map and reduce stages.Real-world use-caseSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!  The blog post and associated repo also introduce clustering as a means of summarization.This opens up a third path beyond the stuff or map-reduce approaches that is worth considering.Option 3. Refine Refine is similar to map-reduce:The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.This can be easily run with the chain_type=\"refine\" specified.chain = load_summarize_chain(llm, chain_type=\"refine\")chain.run(split_docs)\\'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\\'It s also possible to supply a prompt and return intermediate steps.prompt_template = \"\"\"Write a concise summary of the following:{text}CONCISE SUMMARY:\"\"\"prompt = PromptTemplate.from_template(prompt_template)refine_template = (    \"Your job is to produce a final summary\\\\n\"    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"    \"We have the opportunity to refine the existing summary\"    \"(only if needed) with some more context below.\\\\n\"    \"------------\\\\n\"    \"{text}\\\\n\"    \"------------\\\\n\"    \"Given the new context, refine the original summary in Italian\"    \"If the context isn\\'t useful, return the original summary.\")refine_prompt = PromptTemplate.from_template(refine_template)chain = load_summarize_chain(    llm=llm,    chain_type=\"refine\",    question_prompt=prompt,    refine_prompt=refine_prompt,    return_intermediate_steps=True,    input_key=\"input_documents\",    output_key=\"output_text\",)result = chain({\"input_documents\": split_docs}, return_only_outputs=True)print(result[\"output_text\"])Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit  di contesto che limita l\\'inclusione di informazioni storiche dettagliate e la difficolt  di pianificazione a lungo termine e decomposizione delle attivit . Inoltre, l\\'affidabilit  dell\\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti   incerta, poich  i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci , il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\\'ambito degli agenti autonomi.print(\"\\\\n\\\\n\".join(result[\"intermediate_steps\"][:3]))This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.Splitting and summarizing in a single chain For convenience, we can wrap both the text splitting of our long document and summarizing in a single AnalyzeDocumentsChain.from langchain.chains import AnalyzeDocumentChainsummarize_document_chain = AnalyzeDocumentChain(    combine_docs_chain=chain, text_splitter=text_splitter)summarize_document_chain.run(docs[0].page_content)ValueError: `run` not supported when there is not exactly one output key. Got [\\'output_text\\', \\'intermediate_steps\\'].PreviousExtractionNextTaggingUse caseOverviewQuickstartOption 1. StuffGo deeperOption 2. Map-ReduceGo deeperOption 3. RefineSplitting and summarizing in a single chainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Open In Colab', 'language': 'en'}),\n",
       " Document(page_content=\"Chains |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphModulesChainsChainsChains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.LCEL is great for constructing your own chains, but it s also nice to have chains that you can use off-the-shelf. There are two types of off-the-shelf chains that LangChain supports:Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL.[Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are rather standalone classes.We are working creating methods that create LCEL versions of all chains. We are doing this for a few reasons.Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.These chains natively support streaming, async, and batch out of the box.These chains automatically get observability at each step.This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.LCEL Chains Below is a table of all LCEL chain constructors. In addition, we report on:Chain ConstructorThe constructor function for this chain. These are all methods that return LCEL runnables. We also link to the API documentation.Function CallingWhether this requires OpenAI function calling.Other ToolsWhat other tools (if any) are used in this chain.When to UseOur commentary on when to use this chain.Chain ConstructorFunction CallingOther ToolsWhen to Usecreate_stuff_documents_chainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.create_openai_fn_runnable If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it.create_structured_output_runnable If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.load_query_constructor_runnableCan be used to generates queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations.create_sql_query_chainSQL DatabaseIf you want to construct a query for a SQL database from natural language.create_history_aware_retrieverRetrieverThis chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.create_retrieval_chainRetrieverThis chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a responseLegacy Chains Below we report on the legacy chain types that exist. We will maintain support for these until we are able to create a LCEL alternative. We report on:ChainName of the chain, or name of the constructor method. If constructor method, this will return a Chain subclass.Function CallingWhether this requires OpenAI Function Calling.Other ToolsOther tools used in the chain.When to UseOur commentary on when to use.ChainFunction CallingOther ToolsWhen to UseAPIChainRequests WrapperThis chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respondOpenAPIEndpointChainOpenAPI SpecSimilar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpointsConversationalRetrievalChainRetrieverThis chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.StuffDocumentsChainThis chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.ReduceDocumentsChainThis chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. Useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do in parallel.MapReduceDocumentsChainThis chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.RefineDocumentsChainThis chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).MapRerankDocumentsChainThis calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).ConstitutionalChainThis chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this when you want to enforce that a chain s answer follows some principles.LLMChainElasticsearchDatabaseChainElasticSearch InstanceThis chain converts a natural language question to an ElasticSearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elastic Search databaseFlareChainThis implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.ArangoGraphQAChainArango GraphThis chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphCypherQAChainA graph that works with Cypher query languageThis chain constructs an Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.FalkorDBGraphQAChainFalkor DatabaseThis chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.HugeGraphQAChainHugeGraphThis chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.KuzuQAChainKuzu GraphThis chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NebulaGraphQAChainNebula GraphThis chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.NeptuneOpenCypherQAChainNeptune GraphThis chain constructs an Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.GraphSparqlChainGraph that works with SparQLThis chain constructs an SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.LLMMathThis chain converts a user question to a math problem and then executes it (using numexpr)LLMCheckerChainThis chain uses a second LLM call to varify its initial answer. Use this when you to have an extra layer of validation on the initial LLM call.LLMSummarizationCheckerThis chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost).create_citation_fuzzy_match_chain Uses OpenAI function calling to answer questions and cite its sources.create_extraction_chain Uses OpenAI Function calling to extract information from text.create_extraction_chain_pydantic Uses OpenAI function calling to extract information from text into a Pydantic model. Compared to create_extraction_chain this has a tighter integration with Pydantic.get_openapi_chain OpenAPI SpecUses OpenAI function calling to query an OpenAPI.create_qa_with_structure_chain Uses OpenAI function calling to do question answering over text and respond in a specific format.create_qa_with_sources_chain Uses OpenAI function calling to answer questions with citations.QAGenerationChainCreates both questions and answers from documents. Can be used to generate question/answer pairs for evaluation of retrieval projects.RetrievalQAWithSourcesChainRetrieverDoes question answering over\", metadata={'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en'}),\n",
       " Document(page_content='question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over load_qa_with_sources_chain when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in).load_qa_with_sources_chainRetrieverDoes question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).RetrievalQARetrieverThis chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a respoinse.MultiPromptChainThis chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.MultiRetrievalQAChainRetrieverThis chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.EmbeddingRouterChainThis chain uses embedding similarity to route incoming queries.LLMRouterChainThis chain uses an LLM to route between potential options.load_summarize_chainLLMRequestsChainThis chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more generalPreviousTools as OpenAI FunctionsNext[Beta] MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/modules/chains', 'title': 'Chains | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'Chains refer to sequences of calls - whether to an LLM, a tool, or a', 'language': 'en'}),\n",
       " Document(page_content='Prompt + LLM |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookPrompt + LLMOn this pagePrompt + LLMThe most common and valuable composition is taking:PromptTemplate / ChatPromptTemplate -> LLM / ChatModel -> OutputParserAlmost any other chains you build will use this building block.PromptTemplate + LLM The simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output.Note, you can mix and match PromptTemplate/ChatPromptTemplates and LLMs/ChatModels as you like here.%pip install  upgrade  quiet langchain langchain-openaifrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")model = ChatOpenAI()chain = prompt | modelchain.invoke({\"foo\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\", additional_kwargs={}, example=False)Often times we want to attach kwargs that ll be passed to each model call. Here are a few examples of that:Attaching Stop Sequences chain = prompt | model.bind(stop=[\"\\\\n\"])chain.invoke({\"foo\": \"bears\"})AIMessage(content=\\'Why did the bear never wear shoes?\\', additional_kwargs={}, example=False)Attaching Function Call information functions = [    {        \"name\": \"joke\",        \"description\": \"A joke\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},                \"punchline\": {                    \"type\": \"string\",                    \"description\": \"The punchline for the joke\",                },            },            \"required\": [\"setup\", \"punchline\"],        },    }]chain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)chain.invoke({\"foo\": \"bears\"}, config={})AIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'joke\\', \\'arguments\\': \\'{\\\\n  \"setup\": \"Why don\\\\\\'t bears wear shoes?\",\\\\n  \"punchline\": \"Because they have bear feet!\"\\\\n}\\'}}, example=False)PromptTemplate + LLM + OutputParser We can also add in an output parser to easily transform the raw LLM/ChatModel output into a more workable formatfrom langchain_core.output_parsers import StrOutputParserchain = prompt | model | StrOutputParser()Notice that this now returns a string - a much more workable format for downstream taskschain.invoke({\"foo\": \"bears\"})\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"Functions Output Parser When you specify the function to return, you may just want to parse that directlyfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParserchain = (    prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonOutputFunctionsParser())chain.invoke({\"foo\": \"bears\"}){\\'setup\\': \"Why don\\'t bears like fast food?\", \\'punchline\\': \"Because they can\\'t catch it!\"}from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = (    prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke({\"foo\": \"bears\"})\"Why don\\'t bears wear shoes?\"Simplifying input To make invocation even simpler, we can add a RunnableParallel to take care of creating the prompt input dict for us:from langchain_core.runnables import RunnableParallel, RunnablePassthroughmap_ = RunnableParallel(foo=RunnablePassthrough())chain = (    map_    | prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke(\"bears\")\"Why don\\'t bears wear shoes?\"Since we re composing our map with another Runnable, we can even use some syntactic sugar and just use a dict:chain = (    {\"foo\": RunnablePassthrough()}    | prompt    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)    | JsonKeyOutputFunctionsParser(key_name=\"setup\"))chain.invoke(\"bears\")\"Why don\\'t bears like fast food?\"PreviousCookbookNextRAGPromptTemplate + LLMAttaching Stop SequencesAttaching Function Call informationPromptTemplate + LLM + OutputParserFunctions Output ParserSimplifying inputCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser', 'title': 'Prompt + LLM | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'The most common and valuable composition is taking:', 'language': 'en'}),\n",
       " Document(page_content=\"Introduction |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphGet startedIntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by language models. It enables applications that:Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)This framework consists of several parts.LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.LangServe: A library for deploying LangChain chains as a REST API.LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.Together, these products simplify the entire application lifecycle:Develop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.Deploy: Turn any chain into an API with LangServe.LangChain Libraries The main value props of the LangChain packages are:Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or notOff-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasksOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.The LangChain libraries themselves are made up of several different packages.langchain-core: Base abstractions and LangChain Expression Language.langchain-community: Third party integrations.langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.Get started Here s how to install LangChain, set up your environment, and start building.We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.Read up on our Security best practices to make sure you're developing safely with LangChain.noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.LangChain Expression Language (LCEL) LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest  prompt + LLM  chain to the most complex chains.Overview: LCEL and its benefitsInterface: The standard interface for LCEL objectsHow-to: Key features of LCELCookbook: Example code for accomplishing common tasksModules LangChain provides standard, extendable interfaces and integrations for the following modules:Model I/O Interface with language modelsRetrieval Interface with application-specific dataAgents Let models choose which tools to use given high-level directivesExamples, ecosystem, and resources Use cases Walkthroughs and techniques for common end-to-end use cases, like:Document question answeringChatbotsAnalyzing structured dataand much more...Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.Guides Best practices for developing with LangChain.API reference Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.Developer's guide Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.Community Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM s.PreviousGet startedNextInstallationLangChain LibrariesGet startedLangChain Expression Language (LCEL)ModulesExamples, ecosystem, and resourcesUse casesIntegrationsGuidesAPI referenceDeveloper's guideCommunityCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.\", metadata={'source': 'https://python.langchain.com/docs/get_started/introduction', 'title': 'Introduction | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': 'LangChain is a framework for developing applications powered by language models. It enables applications that:', 'language': 'en'}),\n",
       " Document(page_content='Quickstart |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphGet startedQuickstartOn this pageQuickstartIn this quickstart we\\'ll show you how to:Get setup with LangChain, LangSmith and LangServeUse the most basic and common components of LangChain: prompt templates, models, and output parsersUse LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chainingBuild a simple application with LangChainTrace your application with LangSmithServe your application with LangServeThat\\'s a fair amount to cover! Let\\'s dive in.Setup Jupyter Notebook This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See here for instructions on how to install.Installation To install LangChain run:PipCondapip install langchainconda install langchain -c conda-forgeFor more details, see our Installation guide.LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:export LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"Building with LangChain LangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes an LLM to determine whether or not it needs to fetch data to answer questions. We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.LLM Chain For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.OpenAILocalFirst we\\'ll need to import the LangChain x OpenAI integration package.pip install langchain-openaiAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export OPENAI_API_KEY=\"...\"We can then initialize the model:from langchain_openai import ChatOpenAIllm = ChatOpenAI()If you\\'d prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:from langchain_openai import ChatOpenAIllm = ChatOpenAI(openai_api_key=\"...\")Ollama allows you to run open-source large language models, such as Llama 2, locally.First, follow these instructions to set up and run a local Ollama instance:DownloadFetch a model via ollama pull llama2Then, make sure the Ollama server is running. After that, you can do:from langchain_community.llms import Ollamallm = Ollama(model=\"llama2\")Once you\\'ve installed and initialized the LLM of your choice, we can try using it! Let\\'s ask it what LangSmith is - this is something that wasn\\'t present in the training data so it shouldn\\'t have a very good response.llm.invoke(\"how can langsmith help with testing?\")We can also guide it\\'s response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are world class technical documentation writer.\"),    (\"user\", \"{input}\")])We can now combine these into a simple LLM chain:chain = prompt | llm We can now invoke it and ask the same question. It still won\\'t know the answer, but it should respond in a more proper tone for a technical writer!chain.invoke({\"input\": \"how can langsmith help with testing?\"})The output of a ChatModel (and therefore, of this chain) is a message. However, it\\'s often much more convenient to work with strings. Let\\'s add a simple output parser to convert the chat message to a string.from langchain_core.output_parsers import StrOutputParseroutput_parser = StrOutputParser()We can now add this to the previous chain:chain = prompt | llm | output_parserWe can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).chain.invoke({\"input\": \"how can langsmith help with testing?\"})Diving Deeper We\\'ve now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.Retrieval Chain In order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. This requires installing BeautifulSoup:```shellpip install beautifulsoup4After that, we can import and use WebBaseLoader.from langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")docs = loader.load()Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.For embedding models, we once again provide examples for accessing via OpenAI or via local models.OpenAILocalMake sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()Make sure you have Ollama running (same set up as with the LLM).from langchain_community.embeddings import OllamaEmbeddingsembeddings = OllamaEmbeddings()Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity\\'s sake.First we need to install the required packages for that:pip install faiss-cpuThen we can build our index:from langchain_community.vectorstores import FAISSfrom langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)vector = FAISS.from_documents(documents, embeddings)Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.First, let\\'s set up the chain that takes a question and the retrieved documents and generates an answer.from langchain.chains.combine_documents import create_stuff_documents_chainprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:<context>{context}</context>Question: {input}\"\"\")document_chain = create_stuff_documents_chain(llm, prompt)If we wanted to, we could run this ourselves by passing in documents directly:from langchain_core.documents import Documentdocument_chain.invoke({    \"input\": \"how can langsmith help with testing?\",    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]})However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.from langchain.chains import create_retrieval_chainretriever = vector.as_retriever()retrieval_chain = create_retrieval_chain(retriever, document_chain)We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer keyresponse = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})print(response[\"answer\"])# LangSmith offers several features that can help with testing:...This answer should be much more accurate!Diving Deeper We\\'ve now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.Conversation Retrieval Chain The chain we\\'ve created so far can only answer single questions. One of the main', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'}),\n",
       " Document(page_content='can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?We can still use the create_retrieval_chain function, but we need to change two things:The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.The final LLM chain should likewise take the whole history into accountUpdating RetrievalIn order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.from langchain.chains import create_history_aware_retrieverfrom langchain_core.prompts import MessagesPlaceholder# First we need a prompt that we can pass into an LLM to generate this search queryprompt = ChatPromptTemplate.from_messages([    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")])retriever_chain = create_history_aware_retriever(llm, retriever, prompt)We can test this out by passing in an instance where the user is asking a follow up question.from langchain_core.messages import HumanMessage, AIMessagechat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retriever_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.prompt = ChatPromptTemplate.from_messages([    (\"system\", \"Answer the user\\'s questions based on the below context:\\\\n\\\\n{context}\"),    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),])document_chain = create_stuff_documents_chain(llm, prompt)retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)We can now test this out end-to-end:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retrieval_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})We can see that this gives a coherent answer - we\\'ve successfully turned our retrieval chain into a chatbot!Agent We\\'ve so far create examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access two tools:The retriever we just created. This will let it easily answer questions about LangSmithA search tool. This will let it easily answer questions that require up to date information.First, let\\'s set up a tool for the retriever we just created:from langchain.tools.retriever import create_retriever_toolretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)The search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:export TAVILY_API_KEY=...If you do not want to set up an API key, you can skip creating this tool.from langchain_community.tools.tavily_search import TavilySearchResultssearch = TavilySearchResults()We can now create a list of the tools we want to work with:tools = [retriever_tool, search]Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent\\'s Getting Started documentationInstall langchain hub firstpip install langchainhubNow we can use it to get a predefined promptfrom langchain_openai import ChatOpenAIfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutor# Get the prompt to use - you can modify this!prompt = hub.pull(\"hwchase17/openai-functions-agent\")llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})We can ask it about the weather:agent_executor.invoke({\"input\": \"what is the weather in SF?\"})We can have conversations with it:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]agent_executor.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})Diving Deeper We\\'ve now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.Serving with LangServe Now that we\\'ve built an application, we need to serve it. That\\'s where LangServe comes in. LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\\'ll show how you can deploy your app with LangServe.While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.Install with:pip install \"langserve[all]\"Server To create a server for our application we\\'ll make a serve.py file. This will contain our logic for serving our application. It consists of three things:The definition of our chain that we just built aboveOur FastAPI appA definition of a route from which to serve the chain, which is done with langserve.add_routes#!/usr/bin/env pythonfrom typing import Listfrom fastapi import FastAPIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.vectorstores import FAISSfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.tools.retriever import create_retriever_toolfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langchain_openai import ChatOpenAIfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutorfrom langchain.pydantic_v1 import BaseModel, Fieldfrom langchain_core.messages import BaseMessagefrom langserve import add_routes# 1. Load Retrieverloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")docs = loader.load()text_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)embeddings = OpenAIEmbeddings()vector = FAISS.from_documents(documents, embeddings)retriever = vector.as_retriever()# 2. Create Toolsretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)search = TavilySearchResults()tools = [retriever_tool, search]# 3. Create Agentprompt = hub.pull(\"hwchase17/openai-functions-agent\")llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)# 4. App definitionapp = FastAPI(  title=\"LangChain Server\",  version=\"1.0\",  description=\"A simple API server using LangChain\\'s Runnable interfaces\",)# 5. Adding chain route# We need to add these input/output schemas because the current AgentExecutor# is lacking in schemas.class Input(BaseModel):    input: str    chat_history: List[BaseMessage] = Field(        ...,        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},    )class Output(BaseModel):    output: stradd_routes(    app,    agent_executor.with_types(input_type=Input, output_type=Output),    path=\"/agent\",)if __name__ == \"__main__\":    import uvicorn    uvicorn.run(app, host=\"localhost\", port=8000)And that\\'s it! If we execute this file:python serve.pywe should see our chain being served at localhost:8000.Playground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.Client Now let\\'s set up a client for programmatically interacting with our service. We can easily do this with the [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain as if it were running client-side.from langserve import RemoteRunnableremote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")remote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})To learn more about the many other features of LangServe head here.Next steps We\\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey, we recommend you read the following (in order):All of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'}),\n",
       " Document(page_content='to better understand how to create custom chains.Model IO covers more details of prompts, LLMs, and output parsers.Retrieval covers more details of everything related to retrievalAgents covers details of everything related to agentsExplore common end-to-end use cases and template applicationsRead up on LangSmith, the platform for debugging, testing, monitoring and moreLearn more about serving your applications with LangServePreviousInstallationNextSecuritySetupJupyter NotebookInstallationLangSmithBuilding with LangChainLLM ChainDiving DeeperRetrieval ChainDiving DeeperConversation Retrieval ChainAgentDiving DeeperServing with LangServeServerPlaygroundClientNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'source': 'https://python.langchain.com/docs/get_started/quickstart', 'title': 'Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain', 'description': \"In this quickstart we'll show you how to:\", 'language': 'en'})]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "  docs,\n",
    "  embedding=OpenAIEmbeddings(),\n",
    "  persist_directory='./data'\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieverdb = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Tor augmented generation or rag has  become a little bit of a overloaded term  it promises quite a lot but when we  actually start implementing it  especially when we're new to doing this  stuff the results are sometimes amazing  but more often than not kind of not as  good as what we were expecting and that  is because rag as with most tools is  very easy to get started with but then  it's very hard to actually get good  implementing the truth is that there is  a lot more to rag than just putting  documents into a vector database and  then retrieving documents from that  Vector database and putting them into an  llm in order to make the most out of rag  you you have to do a lot of other things  as well so that's why we're starting  this series on how to do rag better in  this first video we're going to be  looking at how to do reranking which is  probably the easiest and fast way to  make a rag pipeline better now I'm going  to be talking throughout this entire  series within the context of rag and LMS  but in reality this can be applied to  retrieval as a whole if you have a  semantic Search application or maybe  even recommendation systems you can  actually apply not all but a lot of what  we're going to be talking about  throughout the series including  reranking which we'll go through today  so before jumping into the solution of  reranking I'm talk a little bit about  the problem that we face with just  retrieval as a whole and then specific  two LMS so to begin with retrieval to  ensure fast search times we use  something called Vector search that is  we transform our text into vectors place  them all into a vector space and then  compare their proximity to what we call  a query Vector which is just a vector  version of some sort of query and see  which ones are the closest together and  we return them now for Vector search of  work we we need vectors which are  essentially just compressed  representations of semantic meaning  behind that text because we're  compressing that information to a single  Vector we will naturally lose some  information but that is the cost of  vector search and for the most part it's  you know it's definitely worth paying  Vector search can give us very good  results but what I tend to find with  Vector search and rag with llms is that  okay I get some good results at the top  but there's actually another result in  let's say position 17 for example that  actually provides some very relevant  context for the question that I have  asked so in this example let's say let's  say this is position 17 down here we  have that relevant item but what we  would typically do when we're doing rag  with llms is we're returning like the  top three items so we're missing out on  these other relevant records down here  so you know what can we do the I mean  the simplest is simply to just return  everything and send all of these into  our llm right so over here we have our  llm now that's okay but llms have  limited context windows so we're going  to end up like filling that context  window very quickly if we just start  returning everything so we want to  return all of this so we want to return  a lot of Records so that we have high  retrieval home but then we want to limit  the number of Records we actually send  to our llm and that's where reranking  would come in so by adding a ranker we  can still use all of those records right  we still get to return all of these from  our retrieval component but then the  records that we actually sent to our LM  are just these here right these top  three and the rerer has gone ahead and  handled the reordering of our records to  get the most relevant items at the top  so we can then send all of that to our  llm now the question here is is a ranker  really going to help us here can we not  just use a better retrieval  model and yes we can use a better  retrieval model and that's something  we'll be talking about in a future video  but there is a very good reason as to  why a ranker can generally perform  better than a encoder model or retrieval  model so let's talk about that very  quickly this is what an encoder model is  doing so this is encoder SL retriever so  this is like your order  002 okay now what it's doing is we have  a Transformer model okay so and these  are the same Transformer model the  reason that I've got two of them on the  screen right now is because you use your  first iteration or inference step of the  transform model to create your embedding  for document  a right and from that you get your  vector a so that is the compressed  information that we can then take across  to our Vector database which would kind  of be like this point here all right  that's in our in our Vector space and  then in another inference step we're  going to do the same for document B we  get Vector B and there we go we we have  that in our Vector search and we can  then compare the proximity of those two  records to get the similarity all right  the metric that we'd be using here like  the the computation would be either dot  product or or cosine in the case of  02 now you have to consider that the  computational complexity of something  like cosine similarity is much simpler  than one of these Transformer inference  steps right so the reason that we use  this encoder architecture is that we can  do all of the Transformer inferences at  the start right when we're building our  index that takes a long time because  Transformers are big heavy things they  take a lot of computation whereas the  cosine similarity Step at the end which  we can run at you know the time when our  user is making a query is very fast so  it's kind of like we're doing the heavy  part of the computation to compare  documents at the very start of building  the index and that means we can do very  quick simple computations at user query  time and that is different to what we do  reranking so here this Transformer is  our  ranker and at query time right so let's  say document a here maybe that's our  query and document B is you know one of  documents in the database where saying  to the Transformer okay how similar are  these two  items so to compare the similarity in  this case we are running an entire  Transformer inference step so this  because we're doing everything in a  single Transformer step we're not losing  as much information as we are with this  one where we're compressing everything  into vectors that means that  theoretically we lose less information  so we can get a more accurate similarity  score here but at the same time it's way  slower so it's kind of like a you know  one on one side  you have fast and you know relatively  accurate and then on this side you have  slow but super accurate so the idea with  the sort of reranking approach to  retrieval is that we use our retrieval  and codep to basically filter down the  total number of documents to just you  know in this example let's say there's  like 25 documents there 25 documents is  not too much so feeding them into our  ranker is actually going to be very fast  whereas if we fed all documents into our  ranker we'd be  waiting I don't know like a really long  time which we don't want to do so  instead we filter down the encoder feed  them into the ranker and then we'll get  like three amazing results super quickly  so that is how the reranking approach  Works let's see how we'd actually  Implement that uh in Python okay so  we're going to be working through this  notebook  here we need hooking face data sets  that's going to be where we where we get  our data set from open AI for creating  our embeddings uh pine cone for soaring  those embeddings and C here uh for our  ranker we're going to start by  downloading our data set which is this  AI archive it's pre-run so I've already  chunked into like tokens of 300 I think  something like that and and it's  basically just a data set of archive  papers you can kind of see a few of them  here that are related to llms  essentially I gathered it by taking some  recent papers that are well known like  llama 2 paper gp4 paper gptq and and so  on and just extracting that extracting  what that was referencing and extracting  those papers and kind of just going in a  loop like through that so yeah we have a  fair few records in there it's not huge  but it's you know not small either so  41.5 th000 trunks but each chunk is you  know roughly this  size okay so I'm just going to reformat  the data into the format we need this is  basically like pine cone format you have  ID text which we're going to convert  into embeddings and metadata we're not  going to use metadata in this example  but it can be useful and maybe it's  something that will we'll look at in a  future video in this series as well so  we need to Define our embedding function  so we need to Define that encoder model  that we're going to be using for that  I'm going to be using opening eye it's  just it's easy 02 fairly good  performance although there are better  models and that's something we will also  be talking about in the future so I'm  going to just run that and I will need  to enter my openai API key to get that  you need to head on over to  platform.  open.com and get your API key I'm going  to enter mine in here and yeah so with  that we should be able to like  initialize our embedding model which we  are doing here I'm not going to go  through like all these functions because  I've done it like a million times  before I think people are probably  getting bored of um that part of these  videos so I'm just going to run through  those it's really very  quickly I'm going to get my pine cone  credentials again app. Pine cone. for  those and I will run  that enter my API key first and then I  want my Pyon environment which I find  next to my API key in the console so  mine was this yours will probably be  like gcp stter or something along those  lines Okay cool so here I'm going to  create an index if it doesn't\", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 1422, 'publish_date': '2023-10-18 00:00:00', 'source': 'Uh9bYiVrW_s', 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'title': 'RAG But Better: Rerankers with Cohere AI', 'view_count': 34369}),\n",
       " Document(page_content=\"here it's generally recommended  actually where is it I'm here to provide  guidance and support not personal  training sessions with the augmentation  why of course we offer these premium  training sessions at just $700 per hour  which is what I told it to to say now  that's an example of what semantic  router can do it's just one example  there are many different things that you  can do with this what I've just shown  you there is using these routes to  remind the agent of particular  information or to you know call a  function but we can also use it to  protect against certain queries we can  use it to basically do function calling  without the super slow agent processing  time that function calling requires and  we can also use this and this is one of  the things I use it for a lot as another  approach to rag you know in the past  I've spoken about there's naive rag  which way you're pering search every  query you have the agent based rag which  is slower but it can usually do a bit  more it's more powerful but then we also  have this which is kind of like the  semantic router rag or semantic rag but  it takes both it can be very powerful  like your agent but it can also be very  fast like your naive rag so it really  gets the best of both and it's generally  my preferred way of doing it so that is  the semantic router as I said now I and  my team have been implementing this  across many projects so we you know  we've been implementing it seeing what  works seeing what doesn't work and  fine-tuning it based on that and I think  what we have here is the first version  okay 100% this is still very early  version but it works incredibly well  it's truly getting us that final 20% of  the AI behaviors that we need in order  to make our agents something that we can  actually go ahead and use in production  which is very cool to see and we want  other people to be able to use this as  well which is why you're seeing this  right now I personally I'm very excited  about releasing this so I hope that this  is exciting for at least a few of you I  hope some of you get to use it and you  know please let me know what you think  if you're interested in contributing  it's all open source so you can and I'll  be doing a few more videos for sure on  how we use this how to make the most of  the semantic router and especially the  other features that I haven't spoken  about yet such as Dynamic routing the  hybrid layer  those are all very exciting things and  we have many more exciting things coming  as well so I hope all of this has been  exciting and interesting but for now I  will leave it there so thank you very  much for watching and I will see you  again in the next one  bye\", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 874, 'publish_date': '2024-01-02 00:00:00', 'source': 'ro312jDqAh0', 'thumbnail_url': 'https://i.ytimg.com/vi/ro312jDqAh0/hq720.jpg?v=65914ff4', 'title': 'NEW AI Framework - Steerable Chatbots with Semantic Router', 'view_count': 21867}),\n",
       " Document(page_content='RAG |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookRAGOn this pageRAGLet s look at adding in a retrieval step to a prompt and LLM, which adds up to a  retrieval-augmented generation  chain%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())chain.invoke(\"where did harrison work?\")\\'Harrison worked at Kensho.\\'template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a Kensho.\\'Conversational Retrieval Chain We can easily add in conversation history. This primarily means adding in chat_message_historyfrom langchain.schema import format_documentfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_stringfrom langchain_core.runnables import RunnableParallelfrom langchain.prompts.prompt import PromptTemplate_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.Chat History:{chat_history}Follow Up Input: {question}Standalone question:\"\"\"CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"ANSWER_PROMPT = ChatPromptTemplate.from_template(template)DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")def _combine_documents(    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\\\n\\\\n\"):    doc_strings = [format_document(doc, document_prompt) for doc in docs]    return document_separator.join(doc_strings)_inputs = RunnableParallel(    standalone_question=RunnablePassthrough.assign(        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])    )    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),)_context = {    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,    \"question\": lambda x: x[\"standalone_question\"],}conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()conversational_qa_chain.invoke(    {        \"question\": \"where did harrison work?\",        \"chat_history\": [],    })AIMessage(content=\\'Harrison was employed at Kensho.\\')conversational_qa_chain.invoke(    {        \"question\": \"where did he work?\",        \"chat_history\": [            HumanMessage(content=\"Who wrote this notebook?\"),            AIMessage(content=\"Harrison\"),        ],    })AIMessage(content=\\'Harrison worked at Kensho.\\')With Memory and returning source documents This shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.from operator import itemgetterfrom langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(    return_messages=True, output_key=\"answer\", input_key=\"question\")# First we add a step to load memory# This adds a \"memory\" key to the input objectloaded_memory = RunnablePassthrough.assign(    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),)# Now we calculate the standalone questionstandalone_question = {    \"standalone_question\": {        \"question\": lambda x: x[\"question\"],        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),    }    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),}# Now we retrieve the documentsretrieved_documents = {    \"docs\": itemgetter(\"standalone_question\") | retriever,    \"question\": lambda x: x[\"standalone_question\"],}# Now we construct the inputs for the final promptfinal_inputs = {    \"context\": lambda x: _combine_documents(x[\"docs\"]),    \"question\": itemgetter(\"question\"),}# And finally, we do the part that returns the answersanswer = {    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),    \"docs\": itemgetter(\"docs\"),}# And now we put it all together!final_chain = loaded_memory | standalone_question | retrieved_documents | answerinputs = {\"question\": \"where did harrison work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison was employed at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}# Note that the memory does not save automatically# This will be improved in the future# For now you need to save it yourselfmemory.save_context(inputs, {\"answer\": result[\"answer\"].content})memory.load_memory_variables({}){\\'history\\': [HumanMessage(content=\\'where did harrison work?\\'),  AIMessage(content=\\'Harrison was employed at Kensho.\\')]}inputs = {\"question\": \"but where did he really work?\"}result = final_chain.invoke(inputs)result{\\'answer\\': AIMessage(content=\\'Harrison actually worked at Kensho.\\'), \\'docs\\': [Document(page_content=\\'harrison worked at kensho\\')]}PreviousPrompt + LLMNextMultiple chainsConversational Retrieval ChainWith Memory and returning source documentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain'}),\n",
       " Document(page_content=\"of the more  recent and better encoder models so you  could actually degrade performance if  you if you do that so you always want to  make sure that you're using kind of like  state-of-the-art rankers alongside State  of-the-art encoders and you should see  an impact kind of similar to what we saw  here with the RL HF question but anyway  as I mentioned this is like the first  method I would use when trying to  optimize an existing retrieval Pipeline  and as you can see super easy to  implement it's you know you don't really  need to modify other parts of the  pipeline you just need to put this into  the middle so I'll leave it there for  now I hope this walk through has been  useful and interesting thank you very  much for watching and I will see you  again in the next one  bye\", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 1422, 'publish_date': '2023-10-18 00:00:00', 'source': 'Uh9bYiVrW_s', 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'title': 'RAG But Better: Rerankers with Cohere AI', 'view_count': 34369}),\n",
       " Document(page_content='of the capabilities and potential applications of such agents.7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents.8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources.Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges.Go deeper CustomizationAs shown above, you can customize the LLMs and prompts for map and reduce stages.Real-world use-caseSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!  The blog post and associated repo also introduce clustering as a means of summarization.This opens up a third path beyond the stuff or map-reduce approaches that is worth considering.Option 3. Refine Refine is similar to map-reduce:The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.This can be easily run with the chain_type=\"refine\" specified.chain = load_summarize_chain(llm, chain_type=\"refine\")chain.run(split_docs)\\'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\\'It s also possible to supply a prompt and return intermediate steps.prompt_template = \"\"\"Write a concise summary of the following:{text}CONCISE SUMMARY:\"\"\"prompt = PromptTemplate.from_template(prompt_template)refine_template = (    \"Your job is to produce a final summary\\\\n\"    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"    \"We have the opportunity to refine the existing summary\"    \"(only if needed) with some more context below.\\\\n\"    \"------------\\\\n\"    \"{text}\\\\n\"    \"------------\\\\n\"    \"Given the new context, refine the original summary in Italian\"    \"If the context isn\\'t useful, return the original summary.\")refine_prompt = PromptTemplate.from_template(refine_template)chain = load_summarize_chain(    llm=llm,    chain_type=\"refine\",    question_prompt=prompt,    refine_prompt=refine_prompt,    return_intermediate_steps=True,    input_key=\"input_documents\",    output_key=\"output_text\",)result = chain({\"input_documents\": split_docs}, return_only_outputs=True)print(result[\"output_text\"])Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit  di contesto che limita l\\'inclusione di informazioni storiche dettagliate e la difficolt  di pianificazione a lungo termine e decomposizione delle attivit . Inoltre, l\\'affidabilit  dell\\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti   incerta, poich  i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci , il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\\'ambito degli agenti autonomi.print(\"\\\\n\\\\n\".join(result[\"intermediate_steps\"][:3]))This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.Splitting and summarizing in a single chain For convenience, we can wrap both the text splitting of our long document and summarizing in a single AnalyzeDocumentsChain.from langchain.chains import AnalyzeDocumentChainsummarize_document_chain = AnalyzeDocumentChain(    combine_docs_chain=chain, text_splitter=text_splitter)summarize_document_chain.run(docs[0].page_content)ValueError: `run` not supported when there is not exactly one output key. Got [\\'output_text\\', \\'intermediate_steps\\'].PreviousExtractionNextTaggingUse caseOverviewQuickstartOption 1. StuffGo deeperOption 2. Map-ReduceGo deeperOption 3. RefineSplitting and summarizing in a single chainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'description': 'Open In Colab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \\uf8ffü¶úÔ∏è\\uf8ffüîó Langchain'})]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the vectordb\n",
    "query = \"RAG Techniques\"\n",
    "retrieved_docs = retrieverdb.get_relevant_documents(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run at the end of the session\n",
    "# For cleanup - to avoid replicatin . MUst explore embeddings redundant filter https://python.langchain.com/docs/integrations/retrievers/merger_retriever\n",
    "\n",
    "vectordb.delete_collection()\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=model, temperature=0.7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"\"\"\n",
    "    ---\n",
    "    created: date of note creation\n",
    "    aliases: learning note\n",
    "    tags: leave blank\n",
    "    Topic: title as provided by the user\n",
    "    Source: list of sources of the document - can be multiple\n",
    "    Author: list of authors of the document - can be multiple\n",
    "    Related Note: leave blank\n",
    "    Further Ref: leave blank\n",
    "    Code Ref: leave blank\n",
    "    Objective: leave blank\n",
    "    Application: leave blank\n",
    "    ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the Example Prompt\n",
    "style = \"\"\"  \n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    ## CODE EXAMPLES\n",
    "    - Extract code examples in here\n",
    "    - Make sure they are in triple backticks ```python``` format\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the example file\n",
    "with open('/home/dpvj/git/assistant/studynote/example/Langchain - RAG Techniques.md', 'r') as f:\n",
    "    example = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        \n",
    "        First, you populate the metadata based on the user document and the information below:\n",
    "        - keep the format as is and provide only the information that is available\n",
    "        - fill in after the colon :\n",
    "        - if you don't have a value or specified to do, leave it blank, but keep the fields\n",
    "        - output the metadata as a list of strings        \n",
    "        - the metatdata is contained between the dotted lines immediately preceding this instruction\n",
    "        \n",
    "        ----------------------------------------------\n",
    "        {metadata} \n",
    "        ----------------------------------------------\n",
    "        \n",
    "        Second, use the information provided in the style to summarize the note folllowing the guidelines below:\n",
    "        - keep within the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        - ensure that the style is the same as provided between the dotted lines immediately preceding this instruction.\n",
    "        \n",
    "        ----------------------------------------------\n",
    "        {style}\n",
    "        ----------------------------------------------\n",
    "        \n",
    "        Thirdly, you are also provided an example note for reference. Your note should follow the same structure as the example note.\n",
    "        This example contains both the metadata and the style. You are to separate the metadata from the style and use them accordingly.\n",
    "        \n",
    "        ----------------------------------------------\n",
    "        {example}\n",
    "        ----------------------------------------------\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Prompt Engineering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", user_message)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"input\") | retrieverdb,\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "        \"style\": itemgetter(\"style\"),\n",
    "        \"example\": itemgetter(\"example\"),\n",
    "        \"metadata\": itemgetter(\"metadata\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": title, \"style\": style, \"example\": example, \"metadata\": metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'----------------------------------------------\\ncreated: \\ntags: \\nTopic: Prompt Engineering\\nSource: mmBo8nlu2j0, https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser, https://python.langchain.com/docs/get_started/quickstart\\nAuthor: LangChain\\nRelated Note: \\nFurther Ref: \\nCode Ref: \\nObjective: \\nApplication: \\n----------------------------------------------\\n\\n# TITLE: \\n- Prompt Engineering\\n\\n## ABSTRACT:\\nThis document discusses the process of prompt engineering, focusing on the setup and deployment of a LangServe project through a virtual environment. The main components include setting up L serve for easier deployment, creating a new app, and configuring Lang Smith for debugging. It also touches on prompt templates, chat models, and output parsers in the context of retrieval augmented generation.\\n\\n## KEY POINTS:\\n- **LangServe Setup:** Utilizes a fresh virtual environment to set up L serve for easy deployment of a new app.\\n- **Lang Smith Configuration:** Involves setting up Lang Smith for debugging and easy prompt engineering.\\n- **Prompt Templates:** Used to structure inputs to the language model for better responses.\\n- **Retrieval Augmented Generation (RAG):** Combines prompt templates, LLMs, and output parsers to create a simple chain for user input and model response.\\n\\n## CONTEXT:\\n- **Bootstrap with Lang Serve:** The document details the process of setting up a LangServe project for easy deployment, leveraging the benefits of L serve for efficient deployment of a new app.\\n- **Lang Smith for Debugging:** Highlights the importance of configuring Lang Smith for debugging and prompt engineering, offering insights into the debugging process.\\n- **Prompt Template Creation:** Discusses the creation of prompt templates to structure inputs for the language model, emphasizing the importance of prompt engineering for better responses.\\n- **RAG Chain Setup:** Provides a detailed overview of setting up a simple chain for user input and model response using retrieval augmented generation techniques.\\n\\n## REFLECTIONS\\n1. How does prompt engineering contribute to the effectiveness of retrieval augmented generation?\\n2. What are the key benefits of using Lang Serve for app deployment?\\n3. Can you identify scenarios where prompt templates play a crucial role in language model responses?\\n4. Reflect on the significance of Lang Smith in the context of debugging and prompt engineering.\\n5. How does retrieval augmented generation streamline the process of user input and model response?\\n\\n## CODE EXAMPLES\\n```python\\n# Example of implementing RAG techniques in Python\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\n# Define the retrieval and language model components\\nretriever = vectorstore.as_retriever()\\ntemplate = \"Answer the question based only on the following context:{context}Question: {question}\"\\nmodel = ChatOpenAI()\\n\\n# Create the RAG pipeline\\nchain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser())\\nchain.invoke(\"where did harrison work?\")\\n# Example of refining RAG output using Python\\nrefine_template = (\\n    \"Your job is to produce a final summary\\\\\\\\n\"\\n    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\\\\\n\"\\n    \"We have the opportunity to refine the existing summary\"\\n    \"(only if needed) with some more context below.\\\\\\\\n\"\\n    \"------------\\\\\\\\n\"\\n    \"{text}\\\\\\\\n\"\\n    \"------------\\\\\\\\n\"\\n    \"Given the new context, refine the original summary in Italian\"\\n    \"If the context isn\\'t useful, return the original summary.\"\\n)\\nrefine_prompt = PromptTemplate.from_template(refine_template)\\nchain = load_summarize_chain(\\n    llm=llm, \\n    chain_type=\"refine\", \\n    question_prompt=prompt, \\n    refine_prompt=refine_prompt, \\n    return_intermediate_steps=True, \\n    input_key=\"input_documents\", \\n    output_key=\"output_text\"\\n)\\nresult = chain({\"input_documents\": split_docs}, return_only_outputs=True)\\nprint(result[\"output_text\"])\\n```\\n\\nThe document delves into prompt engineering and the setup of a LangServe project, emphasizing the role of Lang Smith in debugging and prompt templates to structure inputs for the language model. It provides insights into retrieval augmented generation techniques and offers practical examples of RAG implementation in Python.\\n'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "----------------------------------------------\n",
       "created: \n",
       "tags: \n",
       "Topic: Prompt Engineering\n",
       "Source: mmBo8nlu2j0, https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser, https://python.langchain.com/docs/get_started/quickstart\n",
       "Author: LangChain\n",
       "Related Note: \n",
       "Further Ref: \n",
       "Code Ref: \n",
       "Objective: \n",
       "Application: \n",
       "----------------------------------------------\n",
       "\n",
       "# TITLE: \n",
       "- Prompt Engineering\n",
       "\n",
       "## ABSTRACT:\n",
       "This document discusses the process of prompt engineering, focusing on the setup and deployment of a LangServe project through a virtual environment. The main components include setting up L serve for easier deployment, creating a new app, and configuring Lang Smith for debugging. It also touches on prompt templates, chat models, and output parsers in the context of retrieval augmented generation.\n",
       "\n",
       "## KEY POINTS:\n",
       "- **LangServe Setup:** Utilizes a fresh virtual environment to set up L serve for easy deployment of a new app.\n",
       "- **Lang Smith Configuration:** Involves setting up Lang Smith for debugging and easy prompt engineering.\n",
       "- **Prompt Templates:** Used to structure inputs to the language model for better responses.\n",
       "- **Retrieval Augmented Generation (RAG):** Combines prompt templates, LLMs, and output parsers to create a simple chain for user input and model response.\n",
       "\n",
       "## CONTEXT:\n",
       "- **Bootstrap with Lang Serve:** The document details the process of setting up a LangServe project for easy deployment, leveraging the benefits of L serve for efficient deployment of a new app.\n",
       "- **Lang Smith for Debugging:** Highlights the importance of configuring Lang Smith for debugging and prompt engineering, offering insights into the debugging process.\n",
       "- **Prompt Template Creation:** Discusses the creation of prompt templates to structure inputs for the language model, emphasizing the importance of prompt engineering for better responses.\n",
       "- **RAG Chain Setup:** Provides a detailed overview of setting up a simple chain for user input and model response using retrieval augmented generation techniques.\n",
       "\n",
       "## REFLECTIONS\n",
       "1. How does prompt engineering contribute to the effectiveness of retrieval augmented generation?\n",
       "2. What are the key benefits of using Lang Serve for app deployment?\n",
       "3. Can you identify scenarios where prompt templates play a crucial role in language model responses?\n",
       "4. Reflect on the significance of Lang Smith in the context of debugging and prompt engineering.\n",
       "5. How does retrieval augmented generation streamline the process of user input and model response?\n",
       "\n",
       "## CODE EXAMPLES\n",
       "```python\n",
       "# Example of implementing RAG techniques in Python\n",
       "from langchain_core.output_parsers import StrOutputParser\n",
       "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
       "\n",
       "# Define the retrieval and language model components\n",
       "retriever = vectorstore.as_retriever()\n",
       "template = \"Answer the question based only on the following context:{context}Question: {question}\"\n",
       "model = ChatOpenAI()\n",
       "\n",
       "# Create the RAG pipeline\n",
       "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser())\n",
       "chain.invoke(\"where did harrison work?\")\n",
       "# Example of refining RAG output using Python\n",
       "refine_template = (\n",
       "    \"Your job is to produce a final summary\\\\n\"\n",
       "    \"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"\n",
       "    \"We have the opportunity to refine the existing summary\"\n",
       "    \"(only if needed) with some more context below.\\\\n\"\n",
       "    \"------------\\\\n\"\n",
       "    \"{text}\\\\n\"\n",
       "    \"------------\\\\n\"\n",
       "    \"Given the new context, refine the original summary in Italian\"\n",
       "    \"If the context isn't useful, return the original summary.\"\n",
       ")\n",
       "refine_prompt = PromptTemplate.from_template(refine_template)\n",
       "chain = load_summarize_chain(\n",
       "    llm=llm, \n",
       "    chain_type=\"refine\", \n",
       "    question_prompt=prompt, \n",
       "    refine_prompt=refine_prompt, \n",
       "    return_intermediate_steps=True, \n",
       "    input_key=\"input_documents\", \n",
       "    output_key=\"output_text\"\n",
       ")\n",
       "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
       "print(result[\"output_text\"])\n",
       "```\n",
       "\n",
       "The document delves into prompt engineering and the setup of a LangServe project, emphasizing the role of Lang Smith in debugging and prompt templates to structure inputs for the language model. It provides insights into retrieval augmented generation techniques and offers practical examples of RAG implementation in Python.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Query Experiments - No Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the splitting of the transcripts by chunks according to tokens\n",
    "It seems to be better to split by tokens - where we are getting exact as opposed to the Characters (via RecursiveCharacterTextSplitter) because we are achieving more accurate results based on input limits - which is our primary concern. By defining encoding name and model_name, we can get exact num_tokens as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(page_content=transcript, metadata={'source': f\"https://www.youtube.com/watch?v={video_id}\"})\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "# chunks = splitter.split_documents([document])\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=15000, chunk_overlap=100, encoding_name=\"cl100k_base\", model_name=model)\n",
    "\n",
    "texts = text_splitter.split_text(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(texts[0], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_i = \"\"\"\n",
    "    You are a note taking assistant for a courses. \n",
    "    \n",
    "    Given the Document, write a note based on the following format and instructions defined in point format below:\n",
    "    \n",
    "    # TITLE: \n",
    "    - One line catchy title to capture the essence of the document.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_outputs = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    user_message = f\"\"\"Document: {texts[i]}\"\"\"\n",
    "    prompt = [\n",
    "            {'role': 'system', 'content': system_message_i},\n",
    "            {'role': 'user', 'content': user_message}\n",
    "        ]\n",
    "\n",
    "    response = get_completion(prompt, model=model)\n",
    "    \n",
    "    list_of_outputs.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_c = \"\"\"\n",
    "    You are a note taking assistant for a courses who summarizes several notes. Each note is separated by \"***\" sign\n",
    "    \n",
    "    Given these combined notes, compile a new note with the same headers but combining the points under each header. \n",
    "    Make sure there is no duplication of points.    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_note = \"***\"\n",
    "\n",
    "for i in range(len(list_of_outputs)):\n",
    "    combined_note += list_of_outputs[i] + \"***\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*** on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.***the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result***some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\nIn addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\nRemember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!***torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.***# TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.***model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check***we've replicated the same model using nn.sequential which is a simpler way to define a neural network in PyTorch. We've used nn.sequential to define the layers of our model and set the in and out features accordingly. This allows us to create a neural network with the same architecture as before, but with less code.\\n\\nIn the next video, we'll move on to defining a loss function and an optimizer for our model. We'll also explore how to train and evaluate our model using the training and test data sets we created earlier.***hyperparameters that we've changed so far and then we're going to change the number of epochs so we're going to go from 100 to a thousand so we're going to train for longer so let's write some code to do that so we're going to create a new model so we're going to call this circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1 and we're going to subclass nn.module so we're going to create a new class called circle model v1***of combining linear and non-linear functions remains the same. So let's write some code to replicate these non-linear activation functions. We'll start by creating a simple non-linear function using Python. We'll create a function that applies the rectified linear unit (ReLU) function to an input. The ReLU function returns the maximum of 0 or the input value. Here's how we can write this function:\\n\\n```python\\ndef relu(x):\\n    return max(0, x)\\n```\\n\\nThis simple function demonstrates the essence of the ReLU activation function. It takes an input and returns the maximum of 0 or the input value. This is a basic representation of a non-linear activation function.\\n\\nNext, let's replicate the sigmoid function. The sigmoid function is another common non-linear activation function used in neural networks. It takes an input and returns a value between 0 and 1, which is often interpreted as a probability. Here's how we can write this function:\\n\\n```python\\nimport math\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\n```\\n\\nThese simple functions demonstrate the basic principles of non-linear activation functions. They take an input and apply a non-linear transformation to produce an output.\\n\\nFinally, let's replicate the ReLU and sigmoid functions using PyTorch. PyTorch provides built-in non-linear activation functions that we can use in our neural network models. Here's how we can use the ReLU and sigmoid functions in PyTorch:\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\n# Using ReLU in PyTorch\\nrelu = nn.ReLU()\\n\\n# Using sigmoid in PyTorch\\nsigmoid = nn.Sigmoid()\\n```\\n\\nThese PyTorch functions provide the same non-linear transformations as the custom functions we wrote earlier. They can be easily integrated into our neural network models to introduce non-linearity.\\n\\nBy replicating these non-linear activation functions, we gain a deeper understanding of how they work and how they contribute to the power of neural networks in capturing complex patterns in data.\\n\\nIn conclusion, non-linear activation functions play a crucial role in enabling neural networks to model complex relationships in data. By replicating and understanding these functions, we can better appreciate their impact on the performance of neural network models.***solutions in the solutions folder but i'd highly recommend trying to do it on your own first and then checking the solutions if you get stuck and then if you'd like some extra curriculum i'd recommend checking out the torch metrics module for 10 minutes and then also the article beyond accuracy precision and recall by Will Koisrin for when to use precision and recall and then finally the classification report in scikit-learn so that's a fair bit of stuff to practice and i'd highly recommend going through it and then once you've done that you can come back and we can go through the solutions together and see how you went but i think that's a fair bit of stuff to practice and i'll leave that with you and i'll see you in the next video.***number 0 and we have 32 samples of 28 by 28 pixels and then we have a tensor of 32 labels so we have 32 labels for each of our 32 samples and then we can visualize a single image from the batch so we can go plt.imshow and we can pass in train features batch at index 0 so we're going to visualize the first image from the batch and then we can go plt.title and we can pass in train labels batch at index 0 so we're going to visualize the label of the first image from the batch and then we can go plt.show and we can see what this looks like so we've got a single image from the batch and we've got the label of that image and then we can do the same for the test data loader and visualize a single image from the test data loader and its label and that's how we can visualize a batch of images and their labels from the data loader. So we've covered a lot in this video. We've prepared our data into mini batches using the data loader and we've visualized a batch of images from the data loader. In the next video, we'll start building our convolutional neural network model to classify these images. I'll see you there!***'re right. We can create a function for our training loop and testing loop to make our code more modular and reusable. This will allow us to easily train and evaluate different models without having to rewrite the same code multiple times. Let's start by creating a function for the training loop.\\n\\n```python\\ndef train_model(model, train_loader, criterion, optimizer, device):\\n    model.train()  # Set the model to training mode\\n    train_loss = 0.0  # Initialize the training loss\\n\\n    for inputs, labels in train_loader:  # Loop through the training batches\\n        inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n        optimizer.zero_grad()  # Zero the gradients\\n        outputs = model(inputs)  # Forward pass\\n        loss = criterion(outputs, labels)  # Calculate the loss\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update the model parameters\\n\\n        train_loss += loss.item() * inputs.size(0)  # Accumulate the training loss\\n\\n    train_loss = train_loss / len(train_loader.dataset)  # Calculate the average training loss\\n\\n    return train_loss\\n```\\n\\nIn this function, we set the model to training mode, loop through the training batches, move the inputs and labels to the device, perform the forward pass, calculate the loss, perform backpropagation, update the model parameters, and accumulate the training loss. Finally, we calculate the average training loss and return it.\\n\\nNext, let's create a function for the testing loop.\\n\\n```python\\ndef test_model(model, test_loader, criterion, device):\\n    model.eval()  # Set the model to evaluation mode\\n    test_loss = 0.0  # Initialize the test loss\\n    correct = 0  # Initialize the number of correct predictions\\n\\n    with torch.no_grad():  # Disable gradient calculation\\n        for inputs, labels in test_loader:  # Loop through the testing batches\\n            inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to the device\\n\\n            outputs = model(inputs)  # Forward pass\\n            test_loss += criterion(outputs, labels).item() * inputs.size(0)  # Accumulate the test loss\\n            _, predicted = outputs.max(1)  # Get the predicted labels\\n            correct += predicted.eq(labels).sum().item()  # Count the number of correct predictions\\n\\n    test_loss = test_loss / len(test_loader.dataset)  # Calculate the average test loss\\n    test_accuracy = correct / len(test_loader.dataset)  # Calculate the test accuracy\\n\\n    return test_loss, test_accuracy\\n```\\n\\nIn this function, we set the model to evaluation mode, loop through the testing batches, move the inputs and labels to the device, perform the forward pass, accumulate the test loss, count the number of correct predictions, and calculate the average test loss and test accuracy. Finally, we return the test loss and test accuracy.\\n\\nThese functions will allow us to train and evaluate our models in a more organized and reusable manner. Now, we can use these functions to train and evaluate our model v1. Let's give it a go in the next video.***and so on and so forth so you can play around with these values and see how the output shape changes as you pass data through a max pool layer and a convolutional layer and you can also change the kernel size and see how that affects the output shape as well. This is a great way to understand how these layers manipulate the shape of the data and how they can be used to extract features from the input.\\n\\nIn the next video, we'll continue to explore the other layers of the convolutional neural network and see how they affect the data. Keep experimenting and have fun with it!***Now that we have created the directory to save our model, we can proceed with saving the model itself. We will use the `torch.save` function to save the model to a file. Let's continue with the code.\\n\\n```python\\n# Save the model\\ntorch.save(model2.state_dict(), model_path / 'fashion_mnist_model.pth')\\n```\\n\\nIn this code, we are using the `torch.save` function to save the state dictionary of our model to a file called 'fashion_mnist_model.pth' in the 'models' directory that we created earlier.\\n\\nNow that we have saved our model, we can proceed with loading it back into memory. Let's continue with the code.\\n\\n```python\\n# Load the model\\nmodel = YourModelClass(*args, **kwargs)  # Instantiate your model\\nmodel.load_state_dict(torch.load(model_path / 'fashion_mnist_model.pth'))\\nmodel.eval()  # Set the model to evaluation mode\\n```\\n\\nIn this code, we are instantiating a new instance of our model class and then loading the state dictionary from the saved file back into the model. We then set the model to evaluation mode using `model.eval()`.\\n\\nBy following these steps, we have successfully saved our best performing model to a file and then loaded it back into memory. This allows us to use the model for making predictions or further training in the future.\\n\\nIf you have any questions or need further assistance, feel free to ask!***the transformed image so we're going to create a new variable called transformed and we're going to pass in our data transform and then we're going to pass in f so we're going to transform our original image and then we're going to plot it so we're going to go ax 1 dot m show and we're going to pass in the transformed image and then we're going to set the title so set title and we're going to set this to be the transformed image and then we're going to turn off the axes for the second plot so we're going to go axis and we're going to set that to false and then we're going to show the plot so plt dot show and then we're going to return the random image paths so that's our function there so let's see if this works so let's test it out so we're going to go visualize transformed images and we're going to pass in our image path list and we're going to pass in our data transform and we're going to pass in the number of images to transform so let's say we want to transform three images at a time and then we're going to set the random seed so we're going to set the seed to 42 and let's see what happens so we've got a pizza image here and then we've got the transformed image so we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image and then we've got the original and then we've got the transformed image so we've got a way to visualize what our transformed images look like and this is a very important step when working with any kind of data set is to visualize what your transformed data looks like so that you can see if it's going to be suitable for your model and so that's how we can visualize our transformed images and in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video let's start working towards turning all of our images into tensors and then we can start building our data set and data loader so i'll see you in the next video***# TITLE: \\n- Custom Data Loading: Replicating Image Folder Functionality\\n\\n## ABSTRACT: \\n- The document outlines the process of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It demonstrates the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set.\\n\\n## KEY POINTS:\\n- Subclassing torch.utils.data.Dataset to create a custom data set.\\n- Initializing the custom data set with a target directory and a transform.\\n- Creating attributes such as paths, transform, classes, and class to idx.\\n- Writing a function to load images and overriding the len and get item methods.\\n- Creating a function to display random images from the custom data set.\\n\\n## CONTEXT \\n- The document provides a detailed walkthrough of creating a custom data set in PyTorch to replicate the functionality of the original image folder data loader class. \\n- It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. \\n- The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader.\\n\\n## REFLECTIONS\\n- How can the custom data set be further extended to handle additional data formats or preprocessing steps?\\n- What are the potential challenges of creating a custom data set, and how can they be addressed?\\n- The importance of visualizing data to ensure the correctness of the custom data loading process.\\n\\nRecap: The document provides a comprehensive guide to creating a custom data set in PyTorch, replicating the functionality of the original image folder data loader class. It covers the steps to subclass the torch.utils.data.Dataset, initialize the custom data set, and create a function to display random images from the data set. The process involves setting up the target directory, transforming the data, creating attributes, loading images, and overriding methods to ensure compatibility with the torch.utils.data.DataLoader. The importance of visualizing data to ensure the correctness of the custom data loading process is emphasized.***we don't use random seeds in practice but for educational purposes, we are using them to exemplify how we can get similar numbers on our page. However, ideally, regardless of the random seed used, our model's performance should be similar. \\n\\nWe are going to train for five epochs and create an instance of the `tiny vgg` model. The input shape for the model is three, as we are dealing with color images. The `hidden_units` is set to 10, in line with the CNN explainer website, and the `output_shape` is the number of classes in our training data set. \\n\\nFor the loss function, we are using the `nn.CrossEntropyLoss` as we are dealing with multi-class classification. As for the optimizer, we are trying the `Adam` optimizer with a learning rate of 0.001. The default learning rate for Adam is 1e-3.\\n\\nNow, let's move on to training and evaluating the model using the functions we have created in the previous sections.***we can make a prediction on our custom image. Let's see if we can get our model to predict on this image in the next video. I'll see you there!***# TITLE: \\n- Predicting on Custom Data with PyTorch: Overcoming Challenges and Building a Function\\n\\n## ABSTRACT: \\n- The document covers the process of making predictions on custom images using PyTorch, highlighting the challenges and steps involved in preparing the custom data for model prediction. It emphasizes the critical points of ensuring the data type, shape, and device compatibility with the model. The document also introduces the concept of functionizing the prediction process for ease of use and scalability.\\n\\n## KEY POINTS:\\n- Preparing Custom Data:\\n    - Ensuring the custom image is in the same data type, shape, and device as the model.\\n    - Adding a batch dimension to the image to align with the model's expectations.\\n- PyTorch Built-in Functions:\\n    - Exploring PyTorch's built-in functions for handling various data types, including images, text, and audio.\\n    - Custom data set classes can be written by subclassing torch.utils.data.dataset for specific requirements.\\n- Balancing Overfitting and Underfitting:\\n    - Understanding the balance between overfitting and underfitting in machine learning models.\\n- Predicting on Custom Data:\\n    - Highlighting the importance of addressing wrong data types, shapes, and devices when making predictions on custom data.\\n- Exercises and Extra Curriculum:\\n    - Encouraging practice through exercises and providing resources for further learning and exploration.\\n\\n## CONTEXT\\n- The document provides a detailed walkthrough of the process of preparing and making predictions on custom images using PyTorch. It covers the challenges faced, such as ensuring data compatibility with the model, and introduces the concept of functionizing the prediction process for scalability. The document emphasizes the importance of aligning the custom data with the model's expectations in terms of data type, shape, and device. It also encourages further learning through exercises and extra curriculum resources.\\n\\n- The process involves loading the custom image, transforming it to match the model's requirements, ensuring it is on the correct device, and adding a batch dimension. The document also discusses the balance between overfitting and underfitting in machine learning models and provides insights into PyTorch's built-in functions for handling diverse data types.\\n\\n- The critical takeaway is the significance of addressing wrong data types, shapes, and devices when making predictions on custom data. The document also encourages practical application through exercises and provides additional resources for further learning.\\n\\n## REFLECTIONS\\n- How can the challenges faced in preparing custom data for model prediction be mitigated effectively?\\n- What are the potential implications of incorrect data types, shapes, and devices when making predictions on custom data?\\n- The importance of functionizing the prediction process for scalability and ease of use is evident. How can this approach be further optimized for real-world applications?\\n\\n- The document provides a comprehensive understanding of the complexities involved in preparing and making predictions on custom data using PyTorch. It highlights the critical considerations and challenges, emphasizing the need for meticulous data preparation and alignment with the model's expectations. The concept of functionizing the prediction process adds a layer of scalability and practicality to the workflow, paving the way for further exploration and application in real-world scenarios.***\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(combined_note, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"Document: {combined_note}\"\"\"\n",
    "prompt = [\n",
    "        {'role': 'system', 'content': system_message_c},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "response = get_completion(prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Combined Note:\\n\\n## on the gpu but for now this is just a brief introduction to google colab and how we're going to be writing code throughout the course so that's it for this video we've covered a lot of the fundamentals we've covered how to approach the course we've covered the resources for the course and now we've got into writing some code so i'll see you in the next video where we're going to start diving into the pytorch fundamentals and writing some actual machine learning code.\\n\\n## the resulting shape of the matrix multiplication is going to be tensor a dot matmul tensor b dot t dot shape and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result of the matrix multiplication so let's see what happens here oh we've got a little bit of a typo here tensor a dot matmul tensor b dot t dot shape tensor a dot matmul tensor b dot t and then we'll print out the result\\n\\n## some random tensors and manipulate them using the concepts we've covered such as reshaping, indexing, and moving them to the GPU. Exercise number three is to experiment with reproducibility by setting random seeds and running experiments to see if you can reproduce the same results. And finally, exercise number four is to explore the concept of device agnostic code by setting up a simple model and moving it between the CPU and GPU.\\n\\n## In addition to these exercises, I'd recommend you to explore the extra curriculum resources I've provided, such as the PyTorch documentation, the PyTorch website, and the PyTorch forums. These resources will help you deepen your understanding of the concepts we've covered and provide you with more hands-on practice.\\n\\n## Remember, the best way to solidify your understanding of these fundamentals is to practice and experiment with them. So take your time to work through these exercises and resources, and don't hesitate to reach out if you have any questions or need further guidance. Good luck, and happy learning!\\n\\n## torch.inference mode to turn off gradient tracking and make our predictions faster. This is important because during inference, we don't need to keep track of gradients as we do during training. We also discussed the importance of starting with random values in our model parameters and the goal of adjusting these values to better represent the ideal values through gradient descent and backpropagation. We also encountered a common indentation error in Google Colab and learned how to troubleshoot it. Finally, we visualized our model's predictions and observed that they were quite far from the ideal values, highlighting the need to improve our model's parameters. We also discussed the benefits of using torch.inference mode over torch.no_grad for making predictions.\\n\\n## # TITLE: \\n- Optimizing Model Parameters with PyTorch: A Journey through Training and Testing\\n\\n## ABSTRACT: \\n- This document provides a comprehensive overview of the training and testing process for a machine learning model using PyTorch. It covers the concepts of forward pass, loss calculation, backpropagation, gradient descent, and testing. The document also includes a practical demonstration of training a model for 100 epochs and evaluating the test predictions.\\n\\n## KEY POINTS:\\n- Training Loop: \\n  - The training loop involves steps such as forward pass, loss calculation, zeroing the optimizer gradients, backpropagation, and optimizer step.\\n- Testing Loop: \\n  - The testing loop includes setting the model to evaluation mode, turning off gradient tracking, performing the forward pass, and calculating the test loss.\\n- Inference Mode: \\n  - Inference mode is used to turn off gradient tracking and other settings not needed for evaluation, making the code run faster during testing.\\n\\n## CONTEXT\\n- The document provides a detailed explanation of the training and testing process for a machine learning model using PyTorch. It covers the essential steps involved in training a model, including the forward pass, loss calculation, backpropagation, and gradient descent. The practical demonstration of training a model for 100 epochs and evaluating the test predictions offers a hands-on understanding of the concepts discussed.\\n\\n- The training loop and testing loop are explained in detail, highlighting the significance of each step in the process. The use of inference mode for testing and the impact of setting the model to evaluation mode are emphasized to ensure efficient evaluation of the model's performance.\\n\\n- The document also encourages the reader to experiment with training the model for longer epochs and evaluating the impact on the model's predictions. It emphasizes the iterative nature of model training and testing, encouraging continuous experimentation and improvement.\\n\\n## REFLECTIONS\\n- How can the training process be further optimized to improve the model's predictions?\\n- What are the potential challenges in training a model for a larger number of epochs, and how can they be addressed?\\n- The practical demonstration of training a model for 100 epochs and evaluating the test predictions provides valuable insights into the impact of extended training on model performance.\\n\\nRecap: The document provides a comprehensive understanding of the training and testing process for a machine learning model using PyTorch, with a focus on practical demonstration and hands-on learning. It encourages experimentation and continuous improvement in model training and testing.\\n\\n## model one to see if it's got the same parameters as what we've got here and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the same parameters as what we had before and then we can check the loaded model predictions visually and see if they match up with the original data so let's see if this works and then we'll go from there so we've got the\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith Setup [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langsmith does not work - put in placeholder - need to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv('LANGSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__47bc0afe60cd4471a05f4f1578aac790\"\n",
    "!export LANGCHAIN_PROJECT=\"pt-large-date-95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Embeddings & Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with FAISS\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vector = FAISS.from_documents(docs, embeddings)\n",
    "# retriever = vector.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can provide tools and frameworks for implementing automated testing of software applications. This can help in quickly and efficiently running tests and identifying any issues or bugs in the code.\\n\\n2. Test Case Management: Langsmith can offer solutions for managing and organizing test cases, making it easier for QA teams to create, execute, and track test cases for different scenarios.\\n\\n3. Performance Testing: Langsmith can assist in conducting performance testing to evaluate the speed, responsiveness, and stability of software applications under various conditions.\\n\\n4. Test Data Management: Langsmith can provide tools for managing test data, ensuring that the right data is available for testing different aspects of the software.\\n\\n5. Continuous Integration and Continuous Testing: Langsmith can support the implementation of continuous integration and continuous testing processes, enabling developers to quickly identify and fix issues in the code.\\n\\nOverall, Langsmith can help in streamlining the testing process, improving the efficiency and effectiveness of testing efforts, and ensuring the quality and reliability of software applications.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame the LLM object\n",
    "llm = ChatOpenAI(model=model, verbose=True)\n",
    "llm.invoke(\"how can langsmith help with testing?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "    # TITLE: \n",
    "    - As per user provided Title.\n",
    "    \n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate 3 to 5 questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "        You are a world class note taking assistant.\n",
    "        You summarize notes into concise manner summarizing the most relevant information.\n",
    "        Follow the information in given in the text, do not make things up - unless you are asked for examples.\n",
    "        Follow the guidelines provided in the style for reference on how to summarize the note.\n",
    "\n",
    "        <style>\n",
    "        {style}\n",
    "        </style>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"user\", \n",
    "        \"\"\"Use the following document to summarize into a note under the specified title\n",
    "        the note must follow a specified styleand  specified style:\n",
    "        \n",
    "        <document>\n",
    "        {context}\n",
    "        </document>\n",
    "        \n",
    "        title: {input}\"\"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Prompt Engineering\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response answer: <style>\n",
      "        \n",
      "    # TITLE: \n",
      "    - Prompt Engineering\n",
      "    \n",
      "    ## ABSTRACT: \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models, including instructions, few-shot examples, and specific context and questions. LangChain provides tooling for creating and working with prompt templates, aiming to create model agnostic templates for easy reuse across different language models.\n",
      "    \n",
      "    ## KEY POINTS:\n",
      "    - Prompt templates: Predefined recipes for generating prompts for language models.\n",
      "    - LangChain tooling: Provides tools for creating and working with prompt templates.\n",
      "    - Model agnostic templates: Designed for easy reuse across different language models.\n",
      "    - Prompt structure: Language models expect the prompt to be either a string or a list of chat messages.\n",
      "    \n",
      "    ## CONTEXT \n",
      "    - Prompt templates are predefined recipes for generating prompts for language models. These templates include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. The goal is to create model agnostic templates to make it easy to reuse existing templates across different language models. Language models expect the prompt to either be a string or a list of chat messages.\n",
      "    - The document discusses the process of prompt engineering and the use of LangChain CLI to set up a virtual environment and create an application called \"open AI prompter.\" The author mentions the use of LangChain CLI to bootstrap a Lang serve project and the importance of using LangSmith for debugging during prompt engineering. The author also highlights the iterative nature of prompt engineering and the ease of creating and exporting a chain from a notebook. The document further details the deployment of the prompter using Lang serve and the process of sharing it with others using LangSmith.\n",
      "    \n",
      "    ## REFLECTIONS\n",
      "    - How can prompt engineering contribute to the efficiency of language models?\n",
      "    - What are the advantages of using model agnostic prompt templates?\n",
      "    - How does the use of LangChain CLI simplify the process of creating prompt templates?\n",
      "    \n",
      "    * The document provides a comprehensive overview of prompt engineering and the tools provided by LangChain for creating and working with prompt templates. It emphasizes the importance of model agnostic templates and the iterative nature of prompt engineering. Additionally, it highlights the deployment process using Lang serve and the sharing capabilities of LangSmith. Overall, it offers valuable insights into the practical application of prompt engineering in language model development.\n",
      "    \n",
      "        </style>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response answer: {response['answer']}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f2aac22c790>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n        You are a world class note taking assistant.\\n        You summarize notes into concise manner summarizing the most relevant information.\\n        Follow the information in given in the text, do not make things up - unless you are asked for examples.\\n        Follow the guidelines provided in the style for reference on how to summarize the note.\\n\\n        <style>\\n        \\n    # TITLE: \\n    - As per user provided Title.\\n    \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - Elaborate on the points that are mentioned in the Key Points without repeating word for word\\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate 3 to 5 questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n    \\n        </style>\\n    ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Use the following document to summarize into a note under the specified title\\n        the note must follow a specified styleand  specified style:\\n        \\n        <document>\\n        {context}\\n        </document>\\n        \\n        title: {input}'))])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f2aac22cca0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f2aac22f070>, openai_api_key='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', openai_proxy='')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(retriever, document_chain) \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChatPromptTemplate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m]),\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "chain = create_retrieval_chain(retriever, document_chain) | output_parser\n",
    "chain.invoke({\"input\": \"ChatPromptTemplate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperSeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing by providing a comprehensive and efficient testing framework. Here are some ways in which Langsmith can help with testing:\\n\\n1. Test Automation: Langsmith enables the automation of tests by providing a testing framework that allows developers to write tests in a clear and concise manner. This framework supports various testing methodologies, such as unit testing, integration testing, and end-to-end testing.\\n\\n2. Test Coverage: Langsmith helps in measuring the coverage of tests by providing tools and features to track which parts of the codebase have been tested. This ensures that all critical areas of the application are thoroughly tested, reducing the chances of undiscovered bugs.\\n\\n3. Test Orchestration: Langsmith allows developers to easily manage and run tests across different environments and configurations. It provides a way to define test suites, group related tests, and execute them in parallel or sequentially, improving the testing process's efficiency.\\n\\n4. Test Reporting: Langsmith generates detailed and comprehensive test reports, highlighting the test results, including pass/fail status, execution time, and any errors or exceptions encountered. These reports help in identifying and debugging issues quickly.\\n\\n5. Mocking and Stubbing: Langsmith provides mechanisms for creating mock objects or stubs, allowing developers to isolate components for testing. This is particularly useful when testing code that relies on external dependencies, as it enables developers to simulate their behavior, leading to more reliable and independent tests.\\n\\n6. Performance Testing: Langsmith offers tools and utilities for performance testing, enabling developers to evaluate the system's performance under different load conditions. This helps identify potential bottlenecks and optimize the application for better scalability and responsiveness.\\n\\n7. Continuous Integration/Continuous Delivery (CI/CD) Integration: Langsmith seamlessly integrates with CI/CD pipelines, allowing tests to be automatically triggered on code changes or deployments. This ensures that tests are run consistently and continuously, providing immediate feedback on the code's quality.\\n\\nOverall, Langsmith enhances the testing process by providing a robust framework, tools, and utilities that facilitate test automation, coverage analysis, test orchestration, reporting, mocking, and performance testing. These capabilities help ensure the reliability, stability, and quality of the software being developed.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can greatly assist with testing in several ways:\\n\\n1. Test Case Generation: Langsmith can automatically generate test cases based on the specifications and requirements of your software. It uses natural language processing techniques to understand the desired behavior of the system and generate test cases that cover various scenarios and edge cases. This can save significant time and effort in test case creation.\\n\\n2. Test Data Generation: Langsmith can also generate realistic and diverse test data for your software. It understands the data requirements and constraints of your system and creates test data that covers different data types, ranges, and combinations. This can help in ensuring thorough testing and identifying potential issues related to data handling.\\n\\n3. Test Automation: Langsmith can generate test scripts or code snippets in various programming languages to automate the execution of test cases. It can integrate with popular testing frameworks and tools to streamline the automation process. This can greatly accelerate the testing process and improve test coverage.\\n\\n4. Test Documentation: Langsmith can assist in creating comprehensive test documentation. It can generate test plans, test scripts, test reports, and other relevant documentation based on the identified test cases. This ensures that the testing process is well-documented and easily understandable for all stakeholders.\\n\\n5. Test Oracles: Langsmith can help in defining oracles for automated testing. An oracle is a mechanism to determine whether the actual behavior of the system matches the expected behavior. Langsmith can understand the expected outcomes of various functionalities and generate oracles to validate the test results automatically.\\n\\nOverall, Langsmith can enhance the efficiency and effectiveness of testing by automating test case generation, test data generation, test script creation, and test documentation. It can also improve the quality of testing by ensuring comprehensive coverage and accurate oracles.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f8e84927340>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8e6bf61d80>, model_name='gpt-3.5-turbo-1106', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \\n\\ntext: ```{text}```\\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"You are a study note taking assistant for courses.\n",
    "\n",
    "Given the text delimeted by tripple backticks, extract information into a study note following the style that is {style}. \n",
    "\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_style = \"\"\"\n",
    "    ## ABSTRACT: \n",
    "    - summarize the main concepts covered in the document.\n",
    "    - emphasize critical points or key takeaways.\n",
    "    - Use bold or italic text to highlight these.\n",
    "\n",
    "    ## KEY POINTS:\n",
    "    - Include important terms and their meanings.\n",
    "    - Break the topic into smaller sections.\n",
    "    - Each section should focus on a specific aspect of the topic.\n",
    "    - Use bullet points or numbered lists for clarity.\n",
    "    \n",
    "    ## CONTEXT \n",
    "    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\n",
    "    - Provide examples to illustrate how concepts are applied.\n",
    "    - You should strive to write the context as long as you can using all relevant and necessary information provided.\n",
    "    - You must write the context in bullet form.\n",
    "    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "    \n",
    "    ## REFLECTIONS\n",
    "    - Formulate questions that test understanding of the topic.\n",
    "    - Include space for reflections or personal notes.\n",
    "    - Recap the most important points\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_text = document.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\n\\nGiven the text delimeted by tripple backticks, extract information into a study note following the style that is \\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ntext: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\\n\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studynote_messages = prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)\n",
    "studynote_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_response = chat(studynote_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ABSTRACT:\n",
       "The document discusses the Lang chain expression language, which allows for writing minimalist code to build chains within line chain. It emphasizes the advanced features of parallel execution, async, and streaming using the expression language. The document also explores the syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## KEY POINTS:\n",
       "- Lang chain expression language for building minimalist code to create chains within line chain.\n",
       "- Advanced features include parallel execution, async, and streaming.\n",
       "- Syntax and functionality of the pipe operator and the runnable lambdas.\n",
       "\n",
       "## CONTEXT:\n",
       "- The Lang chain expression language allows for writing minimalist code to build chains within line chain, making it easier to use advanced features like parallel execution, async, and streaming.\n",
       "- The pipe operator is used to string components together, making the code simpler and more flexible.\n",
       "- The runnable lambdas are used to wrap functions and create custom operations within the expression language.\n",
       "- The document provides examples of using the expression language to retrieve information in parallel and modify the output using runnable lambdas.\n",
       "- The expression language has pros such as minimalist code and advanced features, but also cons like increased abstraction and non-standard syntax.\n",
       "\n",
       "## REFLECTIONS:\n",
       "- How does the Lang chain expression language compare to other methods of building chains within line chain?\n",
       "- What are the potential use cases for the runnable lambdas within the expression language?\n",
       "- The expression language offers a minimalist approach and advanced features, but also introduces increased abstraction and non-standard syntax."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(studynote_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_schema = ResponseSchema(name=\"python_code\",\n",
    "                            description=\"Parse any code within the text and write the code in string with backticks. \\\n",
    "                                If there was no code found, then output as -1.\")\n",
    "webpage_schema = ResponseSchema(name=\"webpage_link\",\n",
    "                                    description=\"Was there any webpage links recommended. \\\n",
    "                                    If this information is not found, output -1.\")\n",
    "reading_schema = ResponseSchema(name=\"further_reading\",\n",
    "                                    description=\"Extract any recommendations on further research or reading on the subject. \\\n",
    "                                    Output them as a comma separated Python list. If none is recommended, output -1.\")\n",
    "\n",
    "response_schemas = [python_schema, \n",
    "                    webpage_schema,\n",
    "                    reading_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\\n\\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\\n\\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\\n}\\n```'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
      "\n",
      "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
      "\n",
      "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "\n",
      "text: today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"python_code\": string  // Parse any code within the text and write the code in string with backticks.                                 If there was no code found, then output as -1.\n",
      "\t\"webpage_link\": string  // Was there any webpage links recommended.                                     If this information is not found, output -1.\n",
      "\t\"further_reading\": string  // Extract any recommendations on further research or reading on the subject.                                     Output them as a comma separated Python list. If none is recommended, output -1.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studynote_text = document.page_content\n",
    "\n",
    "template_string2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "python_code: Was there a code on the text write the code in string with backticks. If there was no code text, then output as -1.\n",
    "\n",
    "webpage_link: Was there any webpage links recommended. If this information is not found, output -1.\n",
    "\n",
    "further_reading: Extract any recommendations on further research or reading on the subject Output them as a comma separated Python list. If none is recommended, output -1.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "dict_prompt = ChatPromptTemplate.from_template(template=template_string2)\n",
    "\n",
    "dict_messages = dict_prompt.format_messages(text=studynote_text, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "print(dict_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_response = chat(messages)\n",
    "\n",
    "dict_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_code': -1,\n",
       " 'webpage_link': -1,\n",
       " 'further_reading': 'Lang chain expression language, Line chain products, Line chain abstraction'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "#extract dict key gift\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# Vector Data Memory\n",
    "# Entity Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": studynote_messages}, {\"output\": studynote_response})\n",
    "memory.save_context({\"input\": dict_messages}, {\"output\": dict_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\\nAI: ```json\\n{\\n\\t\"python_code\": -1,\\n\\t\"webpage_link\": -1,\\n\\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\\n}\\n```'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human provides a detailed set of instructions for extracting information into a study note following a specific style. The text provided includes a discussion about the Lang chain expression language, its syntax, and how it works. The AI demonstrates how to use the expression language to run retrievers in parallel and modify the output using runnable lambdas. The AI also discusses the pros and cons of the expression language and concludes that it is worth learning and experimenting with. The human then requests the output to be formatted as a markdown code snippet following a specific schema.\n",
      "AI: ```json\n",
      "{\n",
      "\t\"python_code\": -1,\n",
      "\t\"webpage_link\": -1,\n",
      "\t\"further_reading\": \"Lang chain expression language, Line chain products, Line chain abstraction\"\n",
      "}\n",
      "```\n",
      "Human: Hi, what is langchain about?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lang chain is a powerful expression language that allows for running retrievers in parallel and modifying the output using runnable lambdas. It has a specific syntax and is worth learning and experimenting with. It is used for line chain products and line chain abstraction.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is langchain about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)  \n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lang Chain Expression Language company'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = studynote_messages\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 1: input= text and output= summary\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Come up with a summary for the following text:\"\n",
    "    \"\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= search queries\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate some additional web search queries based on the summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"search_queries\"\n",
    "                    )\n",
    "\n",
    "# chain 2: input= summary and output= topic\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a one line topic based summary:\"\n",
    "    \"\\n\\n{summary}\"\n",
    ")\n",
    "\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, \n",
    "                     output_key=\"topic\"\n",
    "                    )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"search_queries\",\"topic\"],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye\",\n",
       " 'summary': 'The text discusses the Lang chain expression language, which allows for minimalist code to build chains within Lang chain. It explores the advanced features of parallel execution, async, and streaming using the expression language. The text explains the syntax and shows examples of how to use the expression language with a practical use case. It also covers the pros and cons of using the expression language and provides a detailed explanation of how it works. Overall, the text provides a comprehensive overview of the Lang chain expression language and its potential benefits and drawbacks.',\n",
       " 'search_queries': '1. How to use Lang chain expression language for parallel execution?\\n2. What are the practical use cases for Lang chain expression language?\\n3. Pros and cons of using Lang chain expression language for async execution\\n4. What are the advanced features of Lang chain expression language?\\n5. Examples of minimalist code using Lang chain expression language\\n6. How does streaming work in Lang chain expression language?\\n7. Comparison of Lang chain expression language with other expression languages\\n8. Is Lang chain expression language suitable for large-scale applications?\\n9. Tips for optimizing code using Lang chain expression language\\n10. How to get started with Lang chain expression language programming?',\n",
       " 'topic': 'An in-depth exploration of the Lang chain expression language and its advanced features, syntax, practical use cases, and pros and cons.'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = document.page_content\n",
    "overall_chain(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over a larger document - Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working right now - need to revisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/home/dpvj/SecondBrain/templates/Meeting Note Template.md\")\n",
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/pydantic/_migration.py:276: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wnat are the key points of the document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- [ ]  Follow-up meetings scheduled\\n- [ ]  Next steps and responsibilities assigned\\n\\n## Decisions\\n_List any decisions made during the meeting_\\n\\n## Next Steps\\n_Outline the next steps and responsibilities_\\n\\n## Follow-up\\n_Summarize any follow-up actions required_\\n\\n## Additional Notes\\n_Any additional notes or information related to the meeting_\\n\\n## Meeting Adjourned\\n_Time the meeting was adjourned_\\n\\n## Next Meeting\\n_Date, time, and agenda for the next meeting_'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.call_as_llm(f\"{qdocs} Question: Please list all the headers of the documents\") \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_template =  \"\"\"You are a study note taking assistant for courses.\n",
    "You are expected to take notees for a course.\n",
    "\n",
    "Given the course delimeted by tripple backticks, extract information into a study note following the format: \n",
    "{format}. \n",
    "\n",
    "course: ```{text}```\"\"\"\n",
    "\n",
    "\n",
    "meetingnote_template = \"\"\"You are a meeting note assistant. \\\n",
    "You are assighed to take notes for a meeting. \\\n",
    "You are expected to take notes in the following format:\n",
    "{format}.\n",
    "\n",
    "Here is a transcript:\n",
    "transcript: '''{text}'''\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "studynote_format = studynote_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetingnote_format = \"\"\"\n",
    "## Agenda\n",
    "_Summarize the agenda of the meeting_\n",
    "_Ensure key stakeholders are participating & Leading_\n",
    "\n",
    "## Goals\n",
    "_What do we want to achieve from this meeting_\n",
    "_Align with why the meeting was called in the first place_\n",
    "\n",
    "## Discussion notes\n",
    "_Write the notes that are key to the goals & objectives, note who has said it_\n",
    "\n",
    "## Action items\n",
    "_Summarize the action items_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"studynote\", \n",
    "        \"description\": \"Good for taking notes for a course\", \n",
    "        \"prompt_template\": studynote_template,\n",
    "        \"prompt_style\": studynote_format\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"meetingnote\",  # Changed from \"math\" to \"meetingnote\"\n",
    "        \"description\": \"Good for taking notes for a meeting\", \n",
    "        \"prompt_template\": meetingnote_template,\n",
    "        \"prompt_style\": meetingnote_format\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format_messages(\n",
    "                    style=studynote_style,\n",
    "                    text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a study note taking assistant for courses.\\nYou are expected to take notees for a course.\\n\\nGiven the course delimeted by tripple backticks, extract information into a study note following the format: \\n\\n    ## ABSTRACT: \\n    - summarize the main concepts covered in the document.\\n    - emphasize critical points or key takeaways.\\n    - Use bold or italic text to highlight these.\\n\\n    ## KEY POINTS:\\n    - Include important terms and their meanings.\\n    - Break the topic into smaller sections.\\n    - Each section should focus on a specific aspect of the topic.\\n    - Use bullet points or numbered lists for clarity.\\n    \\n    ## CONTEXT \\n    - The context should focus on the details of the document, should be well structured, informative, in depth, with facts and numbers if available and a minimum of 200 words.\\n    - Provide examples to illustrate how concepts are applied.\\n    - You should strive to write the context as long as you can using all relevant and necessary information provided.\\n    - You must write the context in bullet form.\\n    - You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\n    \\n    ## REFLECTIONS\\n    - Formulate questions that test understanding of the topic.\\n    - Include space for reflections or personal notes.\\n    - Recap the most important points\\n. \\n\\ncourse: ```today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see that we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of nice it's very clean and the out of the box support for different features like streaming and the parallel execution that we saw but there are also some cons and you know there's plenty of people that are less fond of the expression language as you know it's a big change it's to be expected now the things that people point to when they're like this doesn't make sense is that it makes things more abstract L chain is a you know abstractions already so we're kind of adding another abstraction to the abstractions and that the syntax is it's definitely not common syntax of python and that kind of goes against the Zen of python which is that kind of shouldn't make special cases for things and of course it's a new syntax it's especially when you first look at it I think once you've explored it a little bit it makes sense but when you first get started with it it's it's definitely confusing so in my honest opinion I think both of those viewpoints are entirely valid there's pros and cons for for sure but I like it I I think it it's definitely worth learning and experimenting with and it can definitely speed things up when particularly when you're prototyping and maybe in production code you know it's going to depend on what you're wanting to what you're wanting to do there anyway that's it for this video I hope all this has been useful in understanding the expression language so thank you very much for watching and I will see you again in the next one bye```\")]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains['studynote'].prompt.format_messages(format=studynote_style, text=studynote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpvj/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:97\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:511\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    512\u001b[0m         _output_key\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    517\u001b[0m         _output_key\n\u001b[1;32m    518\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:92\u001b[0m, in \u001b[0;36mMultiRouteChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     91\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m---> 92\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouter_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mdestination) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(route\u001b[38;5;241m.\u001b[39mnext_inputs), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m route\u001b[38;5;241m.\u001b[39mdestination:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/base.py:40\u001b[0m, in \u001b[0;36mRouterChain.route\u001b[0;34m(self, inputs, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Route:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Route inputs to a destination chain.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        a Route object\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Route(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:316\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    317\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    318\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    319\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    303\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    304\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    305\u001b[0m     inputs,\n\u001b[1;32m    306\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 310\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:61\u001b[0m, in \u001b[0;36mLLMRouterChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     57\u001b[0m _run_manager \u001b[38;5;241m=\u001b[39m run_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManagerForChainRun\u001b[38;5;241m.\u001b[39mget_noop_manager()\n\u001b[1;32m     58\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m     60\u001b[0m     Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/llm.py:322\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/langchain/chains/router/llm_router.py:114\u001b[0m, in \u001b[0;36mRouterOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m raised following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"destination\": \"DEFAULT\",\n    \"next_inputs\": \"today we're going to be talking about Lang chain expression language which is a pretty interesting idea that essentially allows us to write very minimalist code to build chains within line chain and for sure I think we'll see from this video we can use a lot of L chains more advanced features like parallel execution async and streaming very easily using the expression language rather than just the more typical approach to build Lang chain chains and in my opinion it's worth trying just for that I think we'll see that just using this you can build stuff very quickly that's not to say it doesn't have its cons but we'll dive into those later so let's just begin with what this expression language actually is so there's a page here in the line train dos talking about this expression language right so it's LC for short and yeah they just explain a few things you know we streaming acing parel execution so on and so on right but let's just jump into this notebook and we'll see more of how this actually works so there will be a link to this notebook as they usually is at the top of the video right now and I've WR all this in collab so you can do the same it's pretty straightforward we have a few prerequisites we're going to be using line chain of course we're going to be using anthropic the new Claude 2.1 model for our llm we're going to be using cave the embeddings and we're going to be using a dock array just so I can give you an example of parallel retrieval later on which is super interesting now the main things I think we would want to use the Expression language for is these three items here so we have super fast development of chains we have those Advanced features streaming acing parallel execution just work out of the box with these super fast and easy to set up and there's also easy integration with the other Lang chain products so Lang Smith and Lang serve if you are using those now let's take a look at what it actually looks like so to get started with this we're going to need a anthropic API key and you can get that by going to console anthropic tocom you'd come into here hopefully you have an account already and you can click get API keys and you're just going to get your API keys from there if you don't have an anthropic account I think there's still a like a very minor weight list so one I just recommend you sign up and you you'll get access pretty soon but so that you're not waiting you can also just use open AI so you would just swap chat anthropic here with chat openai and swap anthropic API key for openai API key and if you do do that you will also want to drop just drop these two arguments it'll make things easier so looking at this let's see we'll put our API key in here and once we have that we now have these three components we have a prompt a Model A chat model and a output passer okay now in typical L chain we would chain these together using the llm chain okay so you can see llm chain your prompt the L and the output passer okay what I'm going to do is take this prompt where're asking to give me a small report about a particular topic okay so the the input to that is going to be topic and you can see that here so we have topic artificial intelligence and it's obviously just going to Output a small report on that okay so let's run that and see what we get so it's running uh we create our chain running chain. run and we'll just print that output and we'll get this small like rort thing on on AI okay so all looks pretty good now how would we do that with the expression language well we use this this pipe operator and I'm going to go into detail as to how this actually functions because I think that's understanding how this pipe operator functions allows us to just understand what is actually happening here okay so that we can actually understand this abstraction rather than just blindly using it so we string things together right so we have our prompt followed by the model followed by output parer and rather than putting them into an llm chain or some other chain we just string them together with this pipe operator so I mean it's like for sure if I look at this it's kind of it's simpler than this right if you compare those two it's I would say also more flexible because we can just string things together but it's you know I think it's it's not so pythonic as to what we're used to whether or not that is a good or bad thing I'm undecided on like I really I like the minimalist approach here it looks great but it it's maybe hard to understand like if you if you don't understand the syntax and you on python very well this is going to be pretty confusing anyway let's run that so we create our chain using this new this expression language syntax and then we just rather than running run we run invoke and we pass a dictionary of input variables into there so we run this and yeah it's going to do the exact same thing we or very similar output to what we saw before okay so it gives us little report again okay looks cool so these two things this and this doing the exact same thing just different syntax now I think when you see that syntax of the pipe operator for the first time at least for me I was quite confused and I think most people would be confused the way that it works is pretty simple at least the idea behind how it works can be explained very easily what we see on the left of each pipe operator the output from that gets passed to what is on the right of the pipe operator okay and then the output from this is passed into this so it's it's literally piping things from the left of the pipe operators all the way through to the right of the pipe operators that's that's all it's really doing now how that pipe operator actually works is more not necessarily complicated it's probably a little bit hacky in my opinion but it's it's kind of interesting so this pipe operator when we apply it to an object in Python what it actually looks for within that object is this or method here right so if I come down to here we have this kind of confusing class called runnable but let's break it down a little bit okay so I'm going to do class and we're going to call it what still going to call it runable now when we initialize this class we run I'll see the init method here and within that we're going to pass a function right because the way that we're going to implement this is we're going to give a function into this class and we're going to use this class to transform this function into something that we can use this pipe operator on so we want to save that function within our runable class or object and then the next thing you see this is the part that makes the the pipe operator work okay so when a pipe operator is applied to an object it's going to look for the objects all method now the or method that needs to contain another function that we call other here now the way that you can think of this the funk and the other arguments here is that funk is kind of what is on the left of our pipe and other is what is on the right of our pipe okay so what we do is we create this chain function here which is going to consume a set arguments and keyword arguments so we can call it chain Funk as we do there our arguments and we have our keyword arguments now the reason that we set up with args and keyword arguments like this is because we don't know the names of the parameters that I going to be input into our function right so by doing this we can you know those parameter names can vary we can have more or less and this chain function will be able to handle those so we would do return other so our basically this function here that consumes the output from our function okay and again that function is going to take those ARs and keyword arguments okay so from that we would then return the the runnable here so this is going to be our like runnable version of that chain function so basically by doing that we're putting the uh this ability to run chains within each one of the functions that we pass through this actual chain okay so we can do multiple of these so we could have you know other two other three so on and so on now the final thing that we need to have here is a method that allows us to call and and begin this chain now I'm going to implement it with this we will see that line chain actually uses I think they use invoke so rather than call they would have invoke here and that starts to ch but I'm I'm just going to do call because I think it's simpler so that is our runnable function we can run that and I also have it here maybe I'll just run this one and what we want to do is use this Runner board to kind of wrap around different functions that we would like to run with this pipe operator approach to do that we're going to Define two very simple functions here one is add five one is multiply by two okay so let's run those and I'm going to wrap those with this runnable object that we've created and then using this approach right so we have uh we have the chain we're going to do add five and then rather than using the PIP operator I'm going to use the the all method directly and then within that all method I'm going to pass our multiply by two runnable okay so we have those and then we can just call our train so three to it and we get the value 16 which is that's correct so we do 3 + 5 take both those gives us eight and multiply those by two okay so it's correct it's run in the correct order now we can use this syntax or now that we use this or method we can also use the syntax that we see here with the pipe operator so let's try that okay you can you see we we now have this so yeah that's that's pretty interesting so we can you know we can build our own pipe operator functions using using this and this is what line chain is doing okay so when we see this line chain expression language this is what we're actually looking at which is an interesting way of putting things together now that's how it works let's have a look at how we actually use the Expression language itself so we saw already we can use the or operators or the pipe operators now let's put it together in an actual use case so I'm going to be using the coher embedding model you know if you you can also use open a eyes embedding model it's up to you but to get that API key I don't think there's a weight list for coh here so you can you should be able to jump straight into it you can go to dashboard. here.com you'd go to API keys and from the API Keys page you can you can create either a trial key or production key and you just use that so I'm going to add mine in here and I'm going to be using the cair embedding model so the the newest one from there which is very high performance embedding model I'm going to be using that to create two kind of like document stores that we have here okay so we have you know they're very small it's just for an example we have one where we have half the information in Vector sore document sore a and half the information in saw or do saw B you'll see why soon but for now what we're going to do is just use the first one okay so we're going to use a right so it contains information about me when my birthday is the one contains the year of my birthday so let's try putting information into the Vex store or retrieving information my vase store and then feeding that alongside the original query into a chain using the expression language now when we do this there's one important thing that we need to be aware of which is when we use this syntax just using this syntax and nothing else we we have like one input and one output to each of these items right each of these components so how you know how does that work when we have you know we have a context that we need to use here and also a question that we need to feed into our prompt and the way that we do that is by using this runnable parallel object so I've imported those here we have runable parallel and runable pass through the runable parallel which we have here first it allows us to run multiple chains or components in parallel and also extract multiple values from them right so here we're going to run retriever a and then for this question we're using this runnable pass through item what runnable pass through does is whatever was input into the retrieval or the runable parallel object it's just going to return that okay so it's literally a pass through for values that you pass into here so let's run all of that okay so we have our retriever a here that we're using we have our prompt template so on and so on right we have our retrieval that happens first so we have a query when was when was I born we're going to invoke that and this value is being passed into our retriever it's doing a search getting the context it's also being passed through here and going straight through to our prompt okay so then our prompt gets formatted with the question we have when was James born with the context we have the record we will have the records from here okay so V saw a so my birthday the actual date now what we will get here is unfortunately I do not have enough context to definitively State when James was born and it tells me what it found it found this little bit of information so it knows that my birthday is z but it does not specify the year that I was born okay so it can't actually fully answer the question but we can see that this chain is working it's going to do retrieval comparing soon our prompt model Alpha Passa whatever else it's going through everything now the cool thing with runnable parallel you might have guessed with what we have here is that it can run many things in parallel not just a retriever and you know passing through a question we can actually run multiple Retrievers in parallel or we can run multiple different components in parallel at the same time and this is one of the things that is very cool about the expression language is that it you know we we set these things up in parallel and like runnable parallel here is just going to do them in parallel right it's going to run those in parallel we don't have to deal with you know building or writing any of that code ourselves which is I think pretty cool so let's come down to here what I'm going to do is now that we're going to be retrieving information from two places I'm going to create a context a and a context B we're going to run that or we're going to initialize the The Prompt then our runnable parallel now we need to modify a little bit we need to add so we have retriever a we're now mapping that to context a and we have retriever B which we're going to map over to context B and then as before we have our question which is the runable pass through now the chain itself is exactly the same we still just have one like retrieval component there now because you know both our retrievals are being run in parallel within that abstraction so we're going to run that and now I'm going to say the same the same question when was I born okay so now it it knows based on the context provider James was born in 1994 okay stated in the second document with the page content James born in 1994 and maybe if I want to kind of say okay give me the date as well i' say um what date exactly which spawn and we actually get this which is odd because so it it says unfortunately the given context does not provide definitive information to answer the question what dat exactly was James born but then then it actually it gives us here so we have I don't know that there's a little bit of a lack of reasoning ability with Claude in this case clearly so my birthday is 7th December and I was born in 1994 I don't know why it's kind of surprising to me that I didn't get that but interesting but at least we can see that our chain is working correctly we can see that it's pulling in information from both our retrievers there which is cool and we're almost done with what I think are the essentials of the expression language there's just one more thing that I think is super important and it's basically line chains abstraction of doing what I showed you earlier where we created our own sort of runnable class and fed functions into it to create these you know things that we can run with the pipe operator so to do that in line chain they have these runable lambdas okay and this is why earlier on I called that class A runnable because here they they call them runnable lambdas so we have our our add five and our multiply by two I'm going to just come up here and show you what we we had earlier so yeah we have these two functions let's take those okay we can see runable it's what we were doing before so that we could use this let's do it again here all right so we have our add five and I'll multiply by two let's run this this time we're doing runnables but we're just doing them through Line train so our train is going to be at five multiply by two as we did before and as I mentioned you know line chain we have to use infol rather than just calling the object directly so we run that and yes as before we get 16 so yeah we can wrap our own functions using Lang chains runable Lambda here now when would we use that I mean there there are definitely different scenarios why we might want to use that but let me just show you something here which you know kind of bothers me a little bit and it's a good example where we might want to use this either use this or we'd probably want to adjust the output parer as well so we have let's run both of these what we see when we run this is one there's some leading white space here that we could do removing but it also starts each answer with this here's a short fact about artificial intelligence and then we have two double new line characters maybe I don't want that and I just want it to get straight to the fact so what I can do is use this runnable Lambda abstraction to to do that right so I'm going to define a function which is going to look within this string for a double new line within the string if that is in there we're going to split by double new lines and we're going to take everything that occurs after the double new lines now in the case that maybe there are multiple double new lines we're taking everything you know one from one to the end of the list that we would get from this and then we're joining everything back here okay so we're basically just dropping that first one the first part here so let's run that I'm going to wrap that within a runable line and then I'm going to put all those things together and I'm going to add the get fact runable to the end of my chain now let's invoke again and see what we get okay so there's no weird sarting text here and yeah we see with both of those it know it works so our a little runnable Lambda here works well okay so that is really everything I wanted to cover with the expression language you know I think there's there's other things that we can talk about and more to cover but this is I think pretty much everything you need to really get started with it and just understand what this abstraction is actually doing which like I said at the start it's important to understand because then at least we know what we're doing rather than just kind of you putting in these pipe operators and kind of thinking they should work when maybe we're doing something that doesn't make sense so I hope this has been useful for understanding the expression language you know there's pros and there's cons to using this now on the pros obviously there's the Min andless style of the code which is kind of\n raised following error:\nGot invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Web Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrap_text(url: str):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract all the text from the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>begin this chain now I'm going to</td>\n",
       "      <td>612.959</td>\n",
       "      <td>6.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>implement it with this we will see that</td>\n",
       "      <td>615.880</td>\n",
       "      <td>6.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>line chain actually uses I think they</td>\n",
       "      <td>619.720</td>\n",
       "      <td>3.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>use</td>\n",
       "      <td>622.160</td>\n",
       "      <td>3.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>invoke so rather than call they would</td>\n",
       "      <td>623.360</td>\n",
       "      <td>5.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>what we we had earlier so yeah we have</td>\n",
       "      <td>1212.159</td>\n",
       "      <td>5.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>these two functions let's take those</td>\n",
       "      <td>1214.400</td>\n",
       "      <td>4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>okay we can see runable it's what we</td>\n",
       "      <td>1217.320</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>were doing before so that we could use</td>\n",
       "      <td>1219.240</td>\n",
       "      <td>5.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>this let's do it again</td>\n",
       "      <td>1221.280</td>\n",
       "      <td>5.879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text     start  duration\n",
       "226        begin this chain now I'm going to   612.959     6.761\n",
       "227  implement it with this we will see that   615.880     6.280\n",
       "228    line chain actually uses I think they   619.720     3.640\n",
       "229                                      use   622.160     3.480\n",
       "230    invoke so rather than call they would   623.360     5.919\n",
       "..                                       ...       ...       ...\n",
       "449   what we we had earlier so yeah we have  1212.159     5.161\n",
       "450     these two functions let's take those  1214.400     4.840\n",
       "451     okay we can see runable it's what we  1217.320     3.960\n",
       "452   were doing before so that we could use  1219.240     5.319\n",
       "453                   this let's do it again  1221.280     5.879\n",
       "\n",
       "[228 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = YouTubeTranscriptApi.get_transcripts([video_id], languages=['en'])\n",
    "ls = list(transcripts[0].values())[0]\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(ls)\n",
    "\n",
    "filtered_df = df[(df['start'] >= 611) & (df['start'] <= 1224)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\n",
      "\n",
      "Key Links:\n",
      "Code from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
      "LangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\n",
      "GPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "soup = BeautifulSoup(requests.get('https://www.youtube.com/watch?v=DjuXACWYkkU').content)\n",
    "pattern = re.compile('(?<=shortDescription\":\").*(?=\",\"isCrawlable)')\n",
    "description = pattern.findall(str(soup))[0].replace('\\\\n','\\n')\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'AIzaSyDYyXnayylCG2L1ToqrZykiVA--QxZ7-3Y'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=video_id\n",
    ")\n",
    "response = request.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#videoListResponse',\n",
       " 'etag': 'VB1pOZ9dsyaJCmlUHZOx-KOEwbk',\n",
       " 'items': [{'kind': 'youtube#video',\n",
       "   'etag': 'mFwyljJthlBCHJcUCN2cbl3du5U',\n",
       "   'id': 'DjuXACWYkkU',\n",
       "   'snippet': {'publishedAt': '2023-11-16T14:35:01Z',\n",
       "    'channelId': 'UCC-lyoTfSrcJzA1ab3APAgw',\n",
       "    'title': 'Building a Research Assistant from Scratch',\n",
       "    'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher',\n",
       "    'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/default.jpg',\n",
       "      'width': 120,\n",
       "      'height': 90},\n",
       "     'medium': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/mqdefault.jpg',\n",
       "      'width': 320,\n",
       "      'height': 180},\n",
       "     'high': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/hqdefault.jpg',\n",
       "      'width': 480,\n",
       "      'height': 360},\n",
       "     'standard': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/sddefault.jpg',\n",
       "      'width': 640,\n",
       "      'height': 480},\n",
       "     'maxres': {'url': 'https://i.ytimg.com/vi/DjuXACWYkkU/maxresdefault.jpg',\n",
       "      'width': 1280,\n",
       "      'height': 720}},\n",
       "    'channelTitle': 'LangChain',\n",
       "    'categoryId': '22',\n",
       "    'liveBroadcastContent': 'none',\n",
       "    'localized': {'title': 'Building a Research Assistant from Scratch',\n",
       "     'description': 'In this video, we will walk through the steps of building a research assistant from scratch with LangChain and LangSmith. We will cover prompting strategies, how to parallelize steps, and how to customize it to do research over any corpora of data.\\n\\nKey Links:\\nCode from video: https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\\nLangChain Template for Research Assistant: https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant\\nGPT-Researcher Repo: https://github.com/assafelovic/gpt-researcher'}},\n",
       "   'contentDetails': {'duration': 'PT43M40S',\n",
       "    'dimension': '2d',\n",
       "    'definition': 'hd',\n",
       "    'caption': 'false',\n",
       "    'licensedContent': False,\n",
       "    'contentRating': {},\n",
       "    'projection': 'rectangular'},\n",
       "   'statistics': {'viewCount': '9045',\n",
       "    'likeCount': '307',\n",
       "    'favoriteCount': '0',\n",
       "    'commentCount': '27'}}],\n",
       " 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_title = response['items'][0]['snippet']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_date = response['items'][0]['snippet']['publishedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = response['items'][0]['statistics']['viewCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a note taking assistant for a courses. \\n    Given the following document, write key points.\\n    If the document is not relevant, write \"not relevant\".\\n    '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8TClnBZvafYSdsb9WiFpkNfbp1GOt', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1701971291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
