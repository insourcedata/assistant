[Document(page_content="Tor augmented generation or rag has  become a little bit of a overloaded term  it promises quite a lot but when we  actually start implementing it  especially when we're new to doing this  stuff the results are sometimes amazing  but more often than not kind of not as  good as what we were expecting and that  is because rag as with most tools is  very easy to get started with but then  it's very hard to actually get good  implementing the truth is that there is  a lot more to rag than just putting  documents into a vector database and  then retrieving documents from that  Vector database and putting them into an  llm in order to make the most out of rag  you you have to do a lot of other things  as well so that's why we're starting  this series on how to do rag better in  this first video we're going to be  looking at how to do reranking which is  probably the easiest and fast way to  make a rag pipeline better now I'm going  to be talking throughout this entire  series within the context of rag and LMS  but in reality this can be applied to  retrieval as a whole if you have a  semantic Search application or maybe  even recommendation systems you can  actually apply not all but a lot of what  we're going to be talking about  throughout the series including  reranking which we'll go through today  so before jumping into the solution of  reranking I'm talk a little bit about  the problem that we face with just  retrieval as a whole and then specific  two LMS so to begin with retrieval to  ensure fast search times we use  something called Vector search that is  we transform our text into vectors place  them all into a vector space and then  compare their proximity to what we call  a query Vector which is just a vector  version of some sort of query and see  which ones are the closest together and  we return them now for Vector search of  work we we need vectors which are  essentially just compressed  representations of semantic meaning  behind that text because we're  compressing that information to a single  Vector we will naturally lose some  information but that is the cost of  vector search and for the most part it's  you know it's definitely worth paying  Vector search can give us very good  results but what I tend to find with  Vector search and rag with llms is that  okay I get some good results at the top  but there's actually another result in  let's say position 17 for example that  actually provides some very relevant  context for the question that I have  asked so in this example let's say let's  say this is position 17 down here we  have that relevant item but what we  would typically do when we're doing rag  with llms is we're returning like the  top three items so we're missing out on  these other relevant records down here  so you know what can we do the I mean  the simplest is simply to just return  everything and send all of these into  our llm right so over here we have our  llm now that's okay but llms have  limited context windows so we're going  to end up like filling that context  window very quickly if we just start  returning everything so we want to  return all of this so we want to return  a lot of Records so that we have high  retrieval home but then we want to limit  the number of Records we actually send  to our llm and that's where reranking  would come in so by adding a ranker we  can still use all of those records right  we still get to return all of these from  our retrieval component but then the  records that we actually sent to our LM  are just these here right these top  three and the rerer has gone ahead and  handled the reordering of our records to  get the most relevant items at the top  so we can then send all of that to our  llm now the question here is is a ranker  really going to help us here can we not  just use a better retrieval  model and yes we can use a better  retrieval model and that's something  we'll be talking about in a future video  but there is a very good reason as to  why a ranker can generally perform  better than a encoder model or retrieval  model so let's talk about that very  quickly this is what an encoder model is  doing so this is encoder SL retriever so  this is like your order  002 okay now what it's doing is we have  a Transformer model okay so and these  are the same Transformer model the  reason that I've got two of them on the  screen right now is because you use your  first iteration or inference step of the  transform model to create your embedding  for document  a right and from that you get your  vector a so that is the compressed  information that we can then take across  to our Vector database which would kind  of be like this point here all right  that's in our in our Vector space and  then in another inference step we're  going to do the same for document B we  get Vector B and there we go we we have  that in our Vector search and we can  then compare the proximity of those two  records to get the similarity all right  the metric that we'd be using here like  the the computation would be either dot  product or or cosine in the case of  02 now you have to consider that the  computational complexity of something  like cosine similarity is much simpler  than one of these Transformer inference  steps right so the reason that we use  this encoder architecture is that we can  do all of the Transformer inferences at  the start right when we're building our  index that takes a long time because  Transformers are big heavy things they  take a lot of computation whereas the  cosine similarity Step at the end which  we can run at you know the time when our  user is making a query is very fast so  it's kind of like we're doing the heavy  part of the computation to compare  documents at the very start of building  the index and that means we can do very  quick simple computations at user query  time and that is different to what we do  reranking so here this Transformer is  our  ranker and at query time right so let's  say document a here maybe that's our  query and document B is you know one of  documents in the database where saying  to the Transformer okay how similar are  these two  items so to compare the similarity in  this case we are running an entire  Transformer inference step so this  because we're doing everything in a  single Transformer step we're not losing  as much information as we are with this  one where we're compressing everything  into vectors that means that  theoretically we lose less information  so we can get a more accurate similarity  score here but at the same time it's way  slower so it's kind of like a you know  one on one side  you have fast and you know relatively  accurate and then on this side you have  slow but super accurate so the idea with  the sort of reranking approach to  retrieval is that we use our retrieval  and codep to basically filter down the  total number of documents to just you  know in this example let's say there's  like 25 documents there 25 documents is  not too much so feeding them into our  ranker is actually going to be very fast  whereas if we fed all documents into our  ranker we'd be  waiting I don't know like a really long  time which we don't want to do so  instead we filter down the encoder feed  them into the ranker and then we'll get  like three amazing results super quickly  so that is how the reranking approach  Works let's see how we'd actually  Implement that uh in Python okay so  we're going to be working through this  notebook  here we need hooking face data sets  that's going to be where we where we get  our data set from open AI for creating  our embeddings uh pine cone for soaring  those embeddings and C here uh for our  ranker we're going to start by  downloading our data set which is this  AI archive it's pre-run so I've already  chunked into like tokens of 300 I think  something like that and and it's  basically just a data set of archive  papers you can kind of see a few of them  here that are related to llms  essentially I gathered it by taking some  recent papers that are well known like  llama 2 paper gp4 paper gptq and and so  on and just extracting that extracting  what that was referencing and extracting  those papers and kind of just going in a  loop like through that so yeah we have a  fair few records in there it's not huge  but it's you know not small either so  41.5 th000 trunks but each chunk is you  know roughly this  size okay so I'm just going to reformat  the data into the format we need this is  basically like pine cone format you have  ID text which we're going to convert  into embeddings and metadata we're not  going to use metadata in this example  but it can be useful and maybe it's  something that will we'll look at in a  future video in this series as well so  we need to Define our embedding function  so we need to Define that encoder model  that we're going to be using for that  I'm going to be using opening eye it's  just it's easy 02 fairly good  performance although there are better  models and that's something we will also  be talking about in the future so I'm  going to just run that and I will need  to enter my openai API key to get that  you need to head on over to  platform.  open.com and get your API key I'm going  to enter mine in here and yeah so with  that we should be able to like  initialize our embedding model which we  are doing here I'm not going to go  through like all these functions because  I've done it like a million times  before I think people are probably  getting bored of um that part of these  videos so I'm just going to run through  those it's really very  quickly I'm going to get my pine cone  credentials again app. Pine cone. for  those and I will run  that enter my API key first and then I  want my Pyon environment which I find  next to my API key in the console so  mine was this yours will probably be  like gcp stter or something along those  lines Okay cool so here I'm going to  create an index if it doesn't", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 1422, 'publish_date': '2023-10-18 00:00:00', 'source': 'Uh9bYiVrW_s', 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'title': 'RAG But Better: Rerankers with Cohere AI', 'view_count': 34369}),
 Document(page_content="here it's generally recommended  actually where is it I'm here to provide  guidance and support not personal  training sessions with the augmentation  why of course we offer these premium  training sessions at just $700 per hour  which is what I told it to to say now  that's an example of what semantic  router can do it's just one example  there are many different things that you  can do with this what I've just shown  you there is using these routes to  remind the agent of particular  information or to you know call a  function but we can also use it to  protect against certain queries we can  use it to basically do function calling  without the super slow agent processing  time that function calling requires and  we can also use this and this is one of  the things I use it for a lot as another  approach to rag you know in the past  I've spoken about there's naive rag  which way you're pering search every  query you have the agent based rag which  is slower but it can usually do a bit  more it's more powerful but then we also  have this which is kind of like the  semantic router rag or semantic rag but  it takes both it can be very powerful  like your agent but it can also be very  fast like your naive rag so it really  gets the best of both and it's generally  my preferred way of doing it so that is  the semantic router as I said now I and  my team have been implementing this  across many projects so we you know  we've been implementing it seeing what  works seeing what doesn't work and  fine-tuning it based on that and I think  what we have here is the first version  okay 100% this is still very early  version but it works incredibly well  it's truly getting us that final 20% of  the AI behaviors that we need in order  to make our agents something that we can  actually go ahead and use in production  which is very cool to see and we want  other people to be able to use this as  well which is why you're seeing this  right now I personally I'm very excited  about releasing this so I hope that this  is exciting for at least a few of you I  hope some of you get to use it and you  know please let me know what you think  if you're interested in contributing  it's all open source so you can and I'll  be doing a few more videos for sure on  how we use this how to make the most of  the semantic router and especially the  other features that I haven't spoken  about yet such as Dynamic routing the  hybrid layer  those are all very exciting things and  we have many more exciting things coming  as well so I hope all of this has been  exciting and interesting but for now I  will leave it there so thank you very  much for watching and I will see you  again in the next one  bye", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 874, 'publish_date': '2024-01-02 00:00:00', 'source': 'ro312jDqAh0', 'thumbnail_url': 'https://i.ytimg.com/vi/ro312jDqAh0/hq720.jpg?v=65914ff4', 'title': 'NEW AI Framework - Steerable Chatbots with Semantic Router', 'view_count': 21867}),
 Document(page_content='RAG |   Langchain        Skip to main content  LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper\'s guideTemplatesCookbooksTutorialsYouTube videos LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookPrompt + LLMRAGMultiple chainsQuerying a SQL DBAgentsCode writingRouting by semantic similarityAdding memoryAdding moderationManaging prompt sizeUsing toolsLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageCookbookRAGOn this pageRAGLet s look at adding in a retrieval step to a prompt and LLM, which adds up to a  retrieval-augmented generation  chain%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    ["harrison worked at kensho"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = """Answer the question based only on the following context:{context}Question: {question}"""prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {"context": retriever, "question": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())chain.invoke("where did harrison work?")\'Harrison worked at Kensho.\'template = """Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}"""prompt = ChatPromptTemplate.from_template(template)chain = (    {        "context": itemgetter("question") | retriever,        "question": itemgetter("question"),        "language": itemgetter("language"),    }    | prompt    | model    | StrOutputParser())chain.invoke({"question": "where did harrison work", "language": "italian"})\'Harrison ha lavorato a Kensho.\'Conversational Retrieval Chain We can easily add in conversation history. This primarily means adding in chat_message_historyfrom langchain.schema import format_documentfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_stringfrom langchain_core.runnables import RunnableParallelfrom langchain.prompts.prompt import PromptTemplate_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.Chat History:{chat_history}Follow Up Input: {question}Standalone question:"""CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)template = """Answer the question based only on the following context:{context}Question: {question}"""ANSWER_PROMPT = ChatPromptTemplate.from_template(template)DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template="{page_content}")def _combine_documents(    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator="\\n\\n"):    doc_strings = [format_document(doc, document_prompt) for doc in docs]    return document_separator.join(doc_strings)_inputs = RunnableParallel(    standalone_question=RunnablePassthrough.assign(        chat_history=lambda x: get_buffer_string(x["chat_history"])    )    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),)_context = {    "context": itemgetter("standalone_question") | retriever | _combine_documents,    "question": lambda x: x["standalone_question"],}conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()conversational_qa_chain.invoke(    {        "question": "where did harrison work?",        "chat_history": [],    })AIMessage(content=\'Harrison was employed at Kensho.\')conversational_qa_chain.invoke(    {        "question": "where did he work?",        "chat_history": [            HumanMessage(content="Who wrote this notebook?"),            AIMessage(content="Harrison"),        ],    })AIMessage(content=\'Harrison worked at Kensho.\')With Memory and returning source documents This shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.from operator import itemgetterfrom langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(    return_messages=True, output_key="answer", input_key="question")# First we add a step to load memory# This adds a "memory" key to the input objectloaded_memory = RunnablePassthrough.assign(    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("history"),)# Now we calculate the standalone questionstandalone_question = {    "standalone_question": {        "question": lambda x: x["question"],        "chat_history": lambda x: get_buffer_string(x["chat_history"]),    }    | CONDENSE_QUESTION_PROMPT    | ChatOpenAI(temperature=0)    | StrOutputParser(),}# Now we retrieve the documentsretrieved_documents = {    "docs": itemgetter("standalone_question") | retriever,    "question": lambda x: x["standalone_question"],}# Now we construct the inputs for the final promptfinal_inputs = {    "context": lambda x: _combine_documents(x["docs"]),    "question": itemgetter("question"),}# And finally, we do the part that returns the answersanswer = {    "answer": final_inputs | ANSWER_PROMPT | ChatOpenAI(),    "docs": itemgetter("docs"),}# And now we put it all together!final_chain = loaded_memory | standalone_question | retrieved_documents | answerinputs = {"question": "where did harrison work?"}result = final_chain.invoke(inputs)result{\'answer\': AIMessage(content=\'Harrison was employed at Kensho.\'), \'docs\': [Document(page_content=\'harrison worked at kensho\')]}# Note that the memory does not save automatically# This will be improved in the future# For now you need to save it yourselfmemory.save_context(inputs, {"answer": result["answer"].content})memory.load_memory_variables({}){\'history\': [HumanMessage(content=\'where did harrison work?\'),  AIMessage(content=\'Harrison was employed at Kensho.\')]}inputs = {"question": "but where did he really work?"}result = final_chain.invoke(inputs)result{\'answer\': AIMessage(content=\'Harrison actually worked at Kensho.\'), \'docs\': [Document(page_content=\'harrison worked at kensho\')]}PreviousPrompt + LLMNextMultiple chainsConversational Retrieval ChainWith Memory and returning source documentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | \uf8ffü¶úÔ∏è\uf8ffüîó Langchain'}),
 Document(page_content="of the more  recent and better encoder models so you  could actually degrade performance if  you if you do that so you always want to  make sure that you're using kind of like  state-of-the-art rankers alongside State  of-the-art encoders and you should see  an impact kind of similar to what we saw  here with the RL HF question but anyway  as I mentioned this is like the first  method I would use when trying to  optimize an existing retrieval Pipeline  and as you can see super easy to  implement it's you know you don't really  need to modify other parts of the  pipeline you just need to put this into  the middle so I'll leave it there for  now I hope this walk through has been  useful and interesting thank you very  much for watching and I will see you  again in the next one  bye", metadata={'author': 'James Briggs', 'description': 'Unknown', 'length': 1422, 'publish_date': '2023-10-18 00:00:00', 'source': 'Uh9bYiVrW_s', 'thumbnail_url': 'https://i.ytimg.com/vi/Uh9bYiVrW_s/hq720.jpg', 'title': 'RAG But Better: Rerankers with Cohere AI', 'view_count': 34369}),
 Document(page_content='of the capabilities and potential applications of such agents.7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents.8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources.Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges.Go deeper CustomizationAs shown above, you can customize the LLMs and prompts for map and reduce stages.Real-world use-caseSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!  The blog post and associated repo also introduce clustering as a means of summarization.This opens up a third path beyond the stuff or map-reduce approaches that is worth considering.Option 3. Refine Refine is similar to map-reduce:The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.This can be easily run with the chain_type="refine" specified.chain = load_summarize_chain(llm, chain_type="refine")chain.run(split_docs)\'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\'It s also possible to supply a prompt and return intermediate steps.prompt_template = """Write a concise summary of the following:{text}CONCISE SUMMARY:"""prompt = PromptTemplate.from_template(prompt_template)refine_template = (    "Your job is to produce a final summary\\n"    "We have provided an existing summary up to a certain point: {existing_answer}\\n"    "We have the opportunity to refine the existing summary"    "(only if needed) with some more context below.\\n"    "------------\\n"    "{text}\\n"    "------------\\n"    "Given the new context, refine the original summary in Italian"    "If the context isn\'t useful, return the original summary.")refine_prompt = PromptTemplate.from_template(refine_template)chain = load_summarize_chain(    llm=llm,    chain_type="refine",    question_prompt=prompt,    refine_prompt=refine_prompt,    return_intermediate_steps=True,    input_key="input_documents",    output_key="output_text",)result = chain({"input_documents": split_docs}, return_only_outputs=True)print(result["output_text"])Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit  di contesto che limita l\'inclusione di informazioni storiche dettagliate e la difficolt  di pianificazione a lungo termine e decomposizione delle attivit . Inoltre, l\'affidabilit  dell\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti   incerta, poich  i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci , il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\'ambito degli agenti autonomi.print("\\n\\n".join(result["intermediate_steps"][:3]))This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.Splitting and summarizing in a single chain For convenience, we can wrap both the text splitting of our long document and summarizing in a single AnalyzeDocumentsChain.from langchain.chains import AnalyzeDocumentChainsummarize_document_chain = AnalyzeDocumentChain(    combine_docs_chain=chain, text_splitter=text_splitter)summarize_document_chain.run(docs[0].page_content)ValueError: `run` not supported when there is not exactly one output key. Got [\'output_text\', \'intermediate_steps\'].PreviousExtractionNextTaggingUse caseOverviewQuickstartOption 1. StuffGo deeperOption 2. Map-ReduceGo deeperOption 3. RefineSplitting and summarizing in a single chainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright   2024 LangChain, Inc.', metadata={'description': 'Open In Colab', 'language': 'en', 'source': 'https://python.langchain.com/docs/use_cases/summarization', 'title': 'Summarization | \uf8ffü¶úÔ∏è\uf8ffüîó Langchain'})]