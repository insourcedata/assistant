---
created: 2024-01-26
aliases:
  - learning note
tags:
  - LEARN/llm
Topic: Prompt Engineering
Source:
  - https://platform.openai.com/docs/guides/prompt-engineering/strategy-give-models-time-to-think
Author:
  - OpenAI
Related Note:
  - "[[OpenAI - PromptEngineering]]"
Further Ref:
  - "[[Prompt Engineering - Resource Summary]]"
  - "[[Prompt Engineering - Freecodecamp]]"
Code Ref: 
Objective: To Improve Prompt Engineering for Existing Models
Application:
  - "[[StudyAssistant - Architecture.canvas|StudyAssistant - Architecture]]"
---
## ABSTRACT:

Prompt engineering involves strategies and tactics to enhance the performance of large language models like GPT-4. This guide focuses on maximizing the effectiveness of these models by using various methods such as clear instructions, providing reference text, and breaking down complex tasks. Emphasis is placed on understanding and applying different strategies to get the most out of the model's capabilities, especially in complex scenarios.

## KEY POINTS:

- **Clear Instructions:** Vital for obtaining precise and relevant responses from the model.
- **Reference Texts:** Using trusted information as a reference can reduce inaccuracies.
- **Task Simplification:** Decomposing complex tasks into simpler subtasks can improve accuracy.
- **Model Thinking Time:** Allowing the model time to "think" through a problem can yield more accurate solutions.
- **External Tools:** Enhancing the model's capabilities through additional tools or functions.
- **Systematic Testing:** Regularly testing changes to ensure they improve model performance.

## CONTEXT:

- Clear Instructions:
    - Ensures the model understands the user's intent.
    - Specific examples and detailed queries lead to more relevant and accurate responses.
- Reference Texts:
    - Providing reference material helps the model answer with fewer fabrications.
    - Can be used to guide the model's responses and improve reliability.
- Task Simplification:
    - Breaking down complex tasks into smaller, manageable parts.
    - This method mirrors software engineering practices and reduces error rates.
- Model Thinking Time:
    - Similar to human problem-solving, the model benefits from time to process and "think" through issues.
    - Techniques like asking for a chain of thought can lead to more reliable answers.
- External Tools:
    - Using other tools, like text retrieval systems or code execution engines, can compensate for model limitations.
    - These tools can provide up-to-date information or perform accurate calculations.
- Systematic Testing:
    - Regular testing with a comprehensive suite of examples.
    - Necessary to confirm overall performance improvement.

## REFLECTIONS:

1. How can providing clear instructions improve the model's response?
2. In what ways can reference texts be utilized to enhance the model's accuracy?
3. Describe the benefits of simplifying complex tasks into subtasks.
4. How does giving the model time to "think" affect its problem-solving abilities?
5. What role do external tools play in prompt engineering?
6. Why is systematic testing important in the context of prompt engineering?


## CODE EXAMPLES:

```python

# Example code snippets would be placed here if applicable.

```


(Note: The original document did not contain specific code examples related to prompt engineering.)


